<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.98711">
Joint Incremental Disfluency Detection and Dependency Parsing
</title>
<author confidence="0.997929">
Matthew Honnibal
</author>
<affiliation confidence="0.997205">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.529398">
Sydney, Australia
</address>
<email confidence="0.997414">
matthew.honnibal@mq.edu.edu.au
</email>
<author confidence="0.996789">
Mark Johnson
</author>
<affiliation confidence="0.997174">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.529485">
Sydney, Australia
</address>
<email confidence="0.997587">
mark.johnson@mq.edu.edu.au
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936692307692">
We present an incremental dependency
parsing model that jointly performs disflu-
ency detection. The model handles speech
repairs using a novel non-monotonic tran-
sition system, and includes several novel
classes of features. For comparison,
we evaluated two pipeline systems, us-
ing state-of-the-art disfluency detectors.
The joint model performed better on both
tasks, with a parse accuracy of 90.5% and
84.0% accuracy at disfluency detection.
The model runs in expected linear time,
and processes over 550 tokens a second.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901105263158">
Most unscripted speech contains filled pauses
(ums and uhs), and errors that are usually edited
on-the-fly by the speaker. Disfluency detection is
the task of detecting these infelicities in spoken
language transcripts. The task has some imme-
diate value, as disfluencies have been shown to
make speech recognition output much more dif-
ficult to read (Jones et al., 2003), but has also
been motivated as a module in a natural language
understanding pipeline, because disfluencies have
proven problematic for PCFG parsing models.
Instead of a pipeline approach, we build on re-
cent work in transition-based dependency parsing,
to perform the two tasks jointly. There have been
two small studies of dependency parsing on un-
scripted speech, both using entirely greedy pars-
ing strategies, without a direct comparison against
a pipeline architecture (Jorgensen, 2007; Rasooli
and Tetreault, 2013). We go substantially beyond
these pilot studies, and present a system that com-
pares favourably to a pipeline consisting of state-
of-the-art components. Our parser largely follows
the design of Zhang and Clark (2011). We use a
structured averaged perceptron model with beam-
search decoding (Collins, 2002). Our feature set
is based on Zhang and Clark (2011), and our
transition-system is based on the arc-eager system
of Nivre (2003).
We extend the transition system with a novel
non-monotonic transition, Edit. It allows sen-
tences like ‘Pass the pepper uh salt’ to be parsed
incrementally, without the need to guess early
that pepper is disfluent. This is achieved by re-
processing the leftward children of the word Edit
marks as disfluent. For instance, if the parser at-
taches the to pepper, but subsequently marks pep-
per as disfluent, the will be returned to the stack.
We also exploit the ease with which the model can
incorporate arbitrary features, and design a set of
features that capture the ‘rough copy’ structure of
some speech repairs, which motivated the Johnson
and Charniak (2004) noisy channel model.
Our main comparison is against two pipeline
systems, which use the two current state-of-the-
art disfluency detection systems as pre-processors
to our parser, minus the custom disfluency fea-
tures and transition. The joint model compared
favourably to the pipeline parsers at both tasks,
with an unlabelled attachment score of 90.5%, and
84.0% accuracy at detecting speech repairs. An ef-
ficient implementation is available under an open-
source license.1 The future prospects of the sys-
tem are also quite promising. Because the parser
is incremental, it should be well suited to un-
segmented text such as the output of a speech-
recognition system. We consider our main con-
tributions to be:
</bodyText>
<listItem confidence="0.901461">
• a novel non-monotonic transition system, for
speech repairs and restarts,
</listItem>
<footnote confidence="0.977953">
1http://github.com/syllog1sm/redshift
</footnote>
<page confidence="0.91506">
131
</page>
<note confidence="0.3579025">
Transactions of the Association for Computational Linguistics, 2 (2014) 131–142. Action Editor: Joakim Nivre.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics.
</note>
<figure confidence="0.845809">
A flight to um
����
FP
</figure>
<figureCaption confidence="0.999756">
Figure 1: A sentence with disfluencies annotated in
</figureCaption>
<bodyText confidence="0.728006">
the style of Shriberg (1994) and the Switchboard cor-
pus. FP=Filled Pause, RM=Reparandum, IM=Interregnum,
RP=Repair. We follow previous work in evaluating the sys-
tem on the accuracy with which it identifies speech-repairs,
marked reparandum above.
</bodyText>
<listItem confidence="0.9961834">
• several novel feature classes,
• direct comparison against the two best disflu-
ency pre-processors, and
• state-of-the-art accuracy for both speech
parsing and disfluency detection.
</listItem>
<sectionHeader confidence="0.927103" genericHeader="introduction">
2 Switchboard Disfluency Annotations
</sectionHeader>
<bodyText confidence="0.999968952380953">
The Switchboard portion of the Penn Treebank
(Marcus et al., 1993) consists of telephone conver-
sations between strangers about an assigned topic.
Two annotation layers are provided: one for syn-
tactic bracketing (MRG files), and one for disflu-
encies (DPS files). The disfluency layer marks el-
ements with little or no syntactic function, such as
filled pauses and discourse markers, and annotates
speech repairs using the Shriberg (1994) system
of reparandum/interregnum/repair. An example is
shown in Figure 1.
In the syntactic annotation, edited words are
covered by a special node labelled EDITED. The
idea is to mark text which, if excised, would re-
sult in a grammatical sentence. The MRG files do
not mark other types of disfluencies. We follow
the evaluation defined by Charniak and Johnson
(2001), which evaluates the accuracy of identify-
ing speech repairs and restarts. This definition of
the task is the standard in recent work. The reason
for this is that filled pauses can be detected using
a simple rule-based approach, and parentheticals
have less impact on readability and down-stream
processing accuracy.
The MRG and DPS layers have high but im-
perfect agreement over what tokens they mark as
speech repairs: of the text annotated with both lay-
ers, 33,720 tokens are marked as disfluent in at
least one layer, 32,310 are only marked as disflu-
ent by the DPS files, and 32,742 are only marked
as disfluent by the MRG layer.
The Switchboard annotation project was not
fully completed. Because disfluency annotation is
cheaper to produce, many of the DPS training files
do not have matching MRG files. Only 619,236
of the 1,482,845 tokens in the DPS disfluency-
detection training data have gold-standard syntac-
tic parses. Our system requires the more expen-
sive syntactic annotation, but we find that it out-
performs the previous state-of-the-art (Qian and
Liu, 2013), despite training on less than half the
data.
</bodyText>
<subsectionHeader confidence="0.856058">
2.1 Dependency Conversion
</subsectionHeader>
<bodyText confidence="0.999972813953489">
As is standard in statistical dependency parsing
of English, we acquire our gold-standard depen-
dencies from phrase-structure trees. We used the
2013-04-05 version of the Stanford dependency
converter (de Marneffe et al., 2006). As is standard
for English dependency parsing, we use the Ba-
sic Dependencies scheme, which produces strictly
projective representations.
At first we feared that the filled pauses, disfluen-
cies and meta-data tokens in the Switchboard cor-
pus might disrupt the conversion process, by mak-
ing it more difficult for the converter to recognise
the underlying production rules.
To test this, we performed a small experiment.
We prepared two versions of the corpus: one
where EDITED nodes, filled pauses and meta-data
were removed before the trees were transformed
by the Stanford converter, and one where the dis-
fluency removal was performed after the depen-
dency conversion. The resulting corpora were
largely identical: 99.54% of unlabelled and 98.7%
of labelled dependencies were the same. The fact
that the Stanford converter is quite robust to dis-
fluencies was useful for our baseline joint model,
which is trained on dependency trees that also in-
cluded governors for disfluent words.
We follow previous work on disfluency detec-
tion by lower-casing the text and removing punc-
tuation and partial words (words tagged XX and
words ending in ‘-’). We also remove one-token
sentences, as their syntactic analyses are trivial.
We found that two additional simple pre-processes
improved our results: discarding all ‘um’ and ‘uh’
tokens; and merging ‘you know’ and ‘i mean’ into
single tokens.
These pre-processes can be completed on the in-
put string without losing information: none of the
‘um’ or ‘uh’ tokens are semantically significant,
and the bigrams you know and i mean have a de-
pendency between the two tokens over 99.9% of
the times they occur in the treebank, with you and
I never having any children. This makes it easy
to unmerge the tokens deterministically after pars-
</bodyText>
<table confidence="0.650102">
� Boston � �� � � Denver Tuesday
Y I mean Y
RM IM RP
</table>
<page confidence="0.995909">
132
</page>
<bodyText confidence="0.995445">
ing: all incoming and outgoing arcs will point to
know or mean. The same pre-processing was per-
formed for all our parsing systems.
</bodyText>
<sectionHeader confidence="0.958265" genericHeader="method">
3 Transition-based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999918652173913">
A transition-based parser predicts the syntactic
structure of a sentence incrementally, by making
a sequence of classification decisions. We follow
the architecture of Zhang and Clark (2011), who
use beam-search for decoding, and a structured av-
eraged perceptron for training. Despite its simplic-
ity, this type of parser has produced highly com-
petitive results on the Wall Street Journal: with the
extended feature set described by Zhang and Nivre
(2011), it achieves 93.5% unlabelled accuracy on
Stanford basic dependencies (de Marneffe et al.,
2006). Converting the constituency trees produced
by the Charniak and Johnson (2005) reranking
parser results in similar accuracy.
Briefly, the transition-based parser consists of a
configuration (or ‘state’) which is sequentially ma-
nipulated by a set of possible transitions. For us, a
state is a 4-tuple c = (v, β, A, D), where v and β
are disjoint sets of word indices termed the stack
and buffer respectively, A is the set of dependency
arcs, and D is the set of word indices marked dis-
fluent. There are no arcs to or from members of D,
so the dependencies and disfluencies can be imple-
mented as a single vector (in our parser, a token is
marked as disfluent by setting it as its own head).
We use the arc-eager transition system (Nivre,
2003, 2008), which consists of four parsing ac-
tions: Shift, Left-Arc, Right-Arc and Reduce. We
denote the stack with its topmost element to the
right, and the buffer with its first element to the
left. A vertical bar is used to indicate concate-
nation to the stack or buffer, e.g. v|i indicates a
stack with the topmost element i and remaining
elements v. A dependency from a governor i to
a child j is denoted i → j. The four arc-eager
transitions are shown in Figure 2.
The Shift action moves the first item of the
buffer onto the stack. The Right-Arc does the
same, but also adds an arc, so that the top two
items on the stack are connected. The Reduce
move and the Left-Arc both pop the stack, but the
Left-Arc first adds an arc from the first word of
the buffer to the word on top of the stack. Con-
straints on the Reduce and Left-Arc moves ensure
that every word is assigned exactly one head in
the final configuration. We follow the suggestion
</bodyText>
<equation confidence="0.9606573">
(o-, i|0, A, D) ` (o-|i, 0, A, D) S
(o-|i, j|0, A, D) ` (o-, j|0, A ∪ {j → i}, D) L
Only if i does not have an incoming arc.
(o-|i, j|0, A, D) ` (o-|i|j, 0, A ∪ {i → j}, D) R
(o-|i, 0, A, D) ` (o-, 0, A, D) D
Only if i has an incoming arc.
(o-|i, j|0, A, D) ` (o-|[x1, xn], j|0, A&apos;, D&apos;) E
Where
A&apos; = A \ {x → y or y → x : ∀x ∈ [i, j), ∀y ∈ N}
D&apos; = D ∪ [i, j)
</equation>
<bodyText confidence="0.443707">
x1...xn are the former left children of i
</bodyText>
<figureCaption confidence="0.975296">
Figure 2: Our parser’s transition system. The first four
transitions are the standard arc-eager system; the fifth is our
novel Edit transition.
</figureCaption>
<bodyText confidence="0.998765">
of Ballesteros and Nivre (2013) and add a dummy
token that governs root dependencies to the end of
the sentence. Parsing terminates when this token
is at the start of the buffer, and the stack is empty.
Disfluencies are added to D via the Edit transition,
E, which we now define.
</bodyText>
<sectionHeader confidence="0.99674" genericHeader="method">
4 A Non-Monotonic Edit Transition
</sectionHeader>
<bodyText confidence="0.999933419354839">
One of the reasons disfluent sentences are hard to
parse is that there often appear to be syntactic re-
lationships between words in the reparandum and
the fluent sentence. When these relations are con-
sidered in addition to the dependencies between
fluent words, the resulting structure is not neces-
sarily a projective tree.
Figure 3 shows a simple example, where the re-
pair square replaces the reparandum rectangle. An
incremental parser could easily become ‘garden-
pathed’ and attach the repair square to the preced-
ing words, constructing the dependencies shown
dotted in Figure 3. Rather than attempting to de-
vise an incremental model that avoids construct-
ing such dependencies, we allow the parser to con-
struct these dependencies and later delete them if
the governor or child are marked disfluent.
Psycholinguistic models of human sentence
processing have long posited repair mechanisms
(Frazier and Rayner, 1982). Recently, Honnibal
et al. (2013) showed that a limited amount of ‘non-
monotonic’ behaviour can improve an incremen-
tal parser’s accuracy. We here introduce a non-
monotonic transition, Edit, for speech repairs.
The Edit transition marks the word i on top
of the stack v|i as disfluent, along with its right-
ward descendents — i.e., all words in the sequence
i...j − 1, where j is the word at the start of the
buffer. It then restores the words both preceding
and formerly governed by i to the stack.
In other words, the word on top of the stack and
</bodyText>
<page confidence="0.966341">
133
</page>
<figure confidence="0.998312666666667">
1. S His company went broke i mean went bankrupt
Pass me the red rectangle uh I mean square
2. L His company went broke i mean went bankrupt
</figure>
<figureCaption confidence="0.998776">
Figure 3: Example where apparent dependencies between
</figureCaption>
<bodyText confidence="0.90771296">
the reparandum and the fluent sentence complicate parsing.
The dotted edges are difficult for an incremental parser to
avoid, but cannot be part of the final parse if it is to be a
projective tree. Our solution is to make the transition system
non-monotonic: the parser is able to delete edges.
its rightward descendents are all marked as dis-
fluent, and the stack is popped. We then restore
its leftward children to the stack, and all depen-
dencies to and from words marked disfluent are
deleted. The transition is non-monotonic in the
sense that it can delete dependencies created by
a previous transition, and replace tokens onto the
stack that had been popped.
Why revisit the leftward children, but not the
right? We are concerned about dependencies
which might be mirrored between the reparandum
and the repair. The rightward subtree of the disflu-
ency might well be incorrect, but if it is, it would
still be incorrect if the word on top of the stack
were actually fluent. We therefore regard these
as parsing errors that we will train our model to
avoid. In contrast, avoiding the Left-Arc transi-
tions would require the parser to predict that the
head is disfluent when it has not necessarily seen
any evidence indicating that.
</bodyText>
<subsectionHeader confidence="0.996049">
4.1 Worked Example
</subsectionHeader>
<bodyText confidence="0.999975">
Figure 4 shows a gold-standard derivation for
a disfluent sentence from the development data.
Line 1 shows the state resulting from the initial
Shift action. In the next three states, His is Left-
Arced to company, which is then Shifted onto the
stack, and Left-Arced to went in Line 4.
The dependency between went and company is
not part of the gold-standard, because went is dis-
fluent. The correct governor of company is the sec-
ond went in the sentence. The Left-Arc move in
Line 4 can still be considered correct, however, be-
cause the gold-standard analysis is still derivable
from the resulting configuration, via the Edit tran-
sition. Another non-gold dependency is created in
Line 6, between broke and went, before broke is
Reduced from the stack in Line 7.
Lines 9 and 10 show the states before and after
the Edit transition. The word on top of the stack in
Line 9, went, has one leftward child, and one right-
</bodyText>
<figure confidence="0.857713666666667">
3. S His company went broke i mean went bankrupt
4. L His company went broke i mean went bankrupt
5. D His company went broke i mean went bankrupt
6. S His company went broke i mean went bankrupt
7. L His company went broke i mean went bankrupt
8. E His company 3uenr broke�� i mean went bankrupt
9. L His company svenr .broke i mean went bankrupt
10. S His company 3uenC broke�� i mean went bankrupt
bankrupt
</figure>
<figureCaption confidence="0.9735848">
Figure 4: A gold-standard transition sequence using our
EDIT transition. Each line specifies an action and shows the
state resulting from it. Words on the stack are circled, and
the arrow indicates the start of the buffer. Disfluent words are
struck-through.
</figureCaption>
<bodyText confidence="0.999655071428571">
ward child. After the Edit transition is applied,
went and its rightward child broke are both marked
disfluent, and company is returned to the stack. All
of the previous dependencies to and from went and
broke are deleted.
Parsing then proceeds as normal, with the cor-
rect governor of company being assigned by the
Left-Arc in Line 11, and bankrupt being Right-
Arced to went in Line 12. To conserve space, we
have omitted the dummy ROOT token, which is
placed at the end of the sentence, following the
suggestion of Ballesteros and Nivre (2013). The
final action will be a Left-Arc from the ROOT to-
ken to went.
</bodyText>
<subsectionHeader confidence="0.973554">
4.2 Dynamic Oracle Training Algorithm
</subsectionHeader>
<bodyText confidence="0.995273666666667">
Our non-monotonic transition system introduces
substantial spurious ambiguity: the gold-standard
parse can be derived via many different transition
</bodyText>
<figure confidence="0.998903">
11. S His company went broke i mean went bankrupt
12. R His company went broke i mean went bankrupt
13. D His company went .broW i mean went bankrupt
12. R His company svenr .broke i mean went
</figure>
<page confidence="0.991736">
134
</page>
<bodyText confidence="0.999989954545455">
sequences. Recent work has shown that this can
be advantageous (Sartorio et al., 2013; Honnibal
et al., 2013; Goldberg and Nivre, 2012), because
difficult decisions can sometimes be delayed until
more information is available.
Line 5 of Figure 4 shows a state that introduces
spurious ambiguity. From this configuration, there
are multiple actions that could be considered ‘cor-
rect’, in the sense that the gold-standard analysis
can be derived from them. The Edit transition is
correct because went is disfluent, but the Left-Arc
and even the Right-Arc are also correct, in that
there are continuations from them that lead to the
gold-standard analysis.
We regard all transition sequences that can re-
sult in the correct analysis as equally valid, and
want to avoid stipulating one of them during train-
ing. We achieve this by following Goldberg and
Nivre (2012) in using a dynamic oracle to create
partially labelled training data.2 A dynamic oracle
is a function that determines the cost of applying
an action to a state, in terms of gold-standard arcs
that are newly unreachable.
We follow Collins (2002) in training an aver-
aged perceptron model to predict transition se-
quences, rather than individual transitions. This
type of model is often referred to as a struc-
tured perceptron, or sometimes a global percep-
tron. During training, if the model does not pre-
dict the correct sequence, an update is performed,
based on the gold-standard sequence and part of
the sequence predicted by the current weights.
Only part of the sequence is used to calculate the
weight update, in order to account for search er-
rors. We use the maximum violation strategy de-
scribed by Huang et al. (2012) to select the subse-
quence to update from.
To train our model using the dynamic oracle,
we use the latent-variable structured perceptron al-
gorithm described by Sun et al. (2009). Beam-
search is performed to find the highest-scoring
gold-standard sequence, as well as the highest-
scoring prediction. We use the same beam-width
for both search procedures.
</bodyText>
<subsectionHeader confidence="0.999554">
4.3 Path Length Normalisation
</subsectionHeader>
<bodyText confidence="0.99964455">
One problem introduced by the Edit transition is
that the number of actions applied to a sentence is
2 The training data is partially labelled in the sense that in-
stances can have multiple true labels. Equivalently, one might
say that the transitions are latent variables, which generate the
dependencies.
no longer constant — it is no longer guaranteed to
be 2n − 1, for a sentence of length n. When the
Edit transition is applied to a word with leftward
children, those children are returned to the stack,
and processed again. This has little to no impact
on the algorithm’s empirical efficiency, although
worst-case complexity is no longer linear, but it
does pose a problem for decoding.
The perceptron model tends to assign large pos-
itive scores to its top prediction. We thus ob-
served a problem when comparing paths of differ-
ent lengths, at the end of the sentence. Paths that
included Edit transitions were longer, so the sum
of their scores tended to be higher.
The same problem has been observed during
incremental PCFG parsing, by Zhu et al. (2013).
They introduce an additional transition, IDLE, to
ensure that paths are the same length. So long as
one candidate in the beam is still being processed,
all other candidates apply the IDLE transition.
We adopt a simpler solution. We normalise the
figure-of-merit for a candidate state, which is used
to rank it in the beam, by the length of its transition
history. The new figure-of-merit is the arithmetic
mean of the candidate’s transition scores, where
previously the figure-of-merit was the sum of the
candidate’s transition scores.
Interestingly, Zhu et al. (2013) report that they
tried exactly this, and that it was less effective than
their solution. We found that the features associ-
ated with the IDLE transition were uninformative
(the state is at termination, so the stack and buffer
are empty), and had nothing to do with how many
edit transitions were earlier applied.
</bodyText>
<sectionHeader confidence="0.988873" genericHeader="method">
5 Features for the Joint Parser
</sectionHeader>
<bodyText confidence="0.999874266666667">
Our baseline parser uses the feature set described
by Zhang and Nivre (2011). The feature set con-
tains 73 templates that mostly refer to the prop-
erties of 12 context tokens: the top of the stack
(S0), its two leftmost and rightmost children (S0L,
S0L2, S0R, S0R2), its parent and grand-parent
(S0h, S0h2), the first word of the buffer and its two
leftmost children (N0, N0L, N0LL), and the next
two words of the buffer (N1, N2).
Atomic features consist of the word, part-of-
speech tag, or dependency label for these tokens;
and multiple feature atoms are often combined for
feature templates. There are also features for the
string-distance between S0 and N0, and the left
and right valencies (total number of children) of
</bodyText>
<page confidence="0.995262">
135
</page>
<bodyText confidence="0.999913">
S0 and N0, as well as the set of their children’s de-
pendency labels. We restrict these to the first and
last 2 children for implementation efficiency, as
we found this had no effect on accuracy. Numeric
features (for distance and valency) are binned with
the function Ax : min(x, 5). There is only one bi-
lexical feature template, which pairs the words of
S0 and N0. There are also ten tri-tag templates.
Our feature set includes additional dependency
label features not used by Zhang and Nivre (2011),
as we found that disfluency detection errors often
resulted in ungrammatical dependency label com-
binations. The additional templates combine the
POS tag of S0 with two or three dependency la-
bels from its left and right subtrees. Details can be
found in the supplementary material.
</bodyText>
<subsectionHeader confidence="0.990039">
5.1 Brown Cluster Features
</subsectionHeader>
<bodyText confidence="0.99982409375">
The Brown clustering algorithm (Brown et al.,
1992) is a well known source of semi-supervised
features. The clustering algorithm is run over
a large sample of unlabelled data, to generate a
type-to-cluster map. This mapping is then used to
generate features that sometimes generalise better
than lexical features, and are helpful for out-of-
vocabulary words (Turian et al., 2010).
Koo and Collins (2010) found that Brown clus-
ter features greatly improved the performance of a
graph-based dependency parser. On our transition-
based parser, Brown cluster features bring a small
but statistically significant improvement on the
WSJ task (0.1-0.3% UAS). Other developers of
transition-based parsers seem to have found sim-
ilar results (personal communication). Since a
Brown cluster mapping computed by Liang (2005)
is easily available,3 the features are simple to im-
plement and cheap to compute.
Our templates follow Koo and Collins (2010)
in including features that refer to cluster prefix
strings, as well as the full clusters. We adapt their
templates to transition-based parsing by replacing
‘head’ with ‘item on top of the stack’ and ‘child’
with ‘first word of the buffer’. The exact templates
can be found in the supplementary material.
The Brown cluster features are used in our
‘baseline’ parser, and in the parsers we use as part
of our pipeline systems. They improved develop-
ment set accuracy by 0.4%. We experimented with
the other feature sets in these parsers, but found
that they did not improve accuracy on fluent text.
</bodyText>
<footnote confidence="0.83828">
3http://www.metaoptimize.com/projects/wordreps
</footnote>
<subsectionHeader confidence="0.985197">
5.2 Rough Copy Features
</subsectionHeader>
<bodyText confidence="0.999879888888889">
Johnson and Charniak (2004) point out that in
speech repairs, the repair is often a ‘rough copy’
of the reparandum. The simplest case of this is
where the repair is a single word repetition. It is
common for the repair to differ from the reparan-
dum by insertion, deletion or substitution of one
or more words.
To capture this regularity, we first extend the
feature-set with three new context tokens:4
</bodyText>
<listItem confidence="0.9999">
1. S0re: The rightmost edge of S0 descendants;
2. S0le: The leftmost edge of S0 descendants;
3. N0le: The leftmost edge of N0 descendants.
</listItem>
<bodyText confidence="0.999581857142857">
If a word has no leftward children, it will be
its own left-edge, and similarly it will be its own
rightward edge if it has no rightward children.
Note that the token S0re is necessarily immedi-
ately before N0le, unless some of the tokens be-
tween them are disfluent. We use the S0le and N0le
to compute the following rough-copy features:
</bodyText>
<listItem confidence="0.9731215">
1. How long is the prefix word match between
S0le...S0 and N0le...N0?
</listItem>
<bodyText confidence="0.834632333333333">
If the parser were analysing the red the blue
square, with red on the stack and square at
N0, its value would be 1.
</bodyText>
<listItem confidence="0.995932">
2. How long is the prefix POS tag match be-
tween S0le...S0 and N0le...N0?
3. Do the words in S0le...S0 and N0le...N0
match exactly?
4. Do the POS tags in S0le...S0 and N0le...N0
match exactly?
</listItem>
<bodyText confidence="0.9889114">
If the parser were analysing the red square
the blue rectangle, with square on the stack
and rectangle at N0, its value would be true.
The prefix-length features are binned using the
function Ax : min(x, 5).
</bodyText>
<subsectionHeader confidence="0.994696">
5.3 Match Features
</subsectionHeader>
<bodyText confidence="0.999977">
This class of features ask which pairs of the con-
text tokens match, in word or POS tag. The con-
text tokens in the Zhang and Nivre (2011) fea-
ture set are the top of the stack (S0), its head and
</bodyText>
<footnote confidence="0.9375572">
4As is common in this type of parser, our implementation
has a number of vectors for properties that are defined before
parsing, such as word forms, POS tags, Brown clusters, etc. A
context token is an index into these vectors, allowing features
considering these properties to be computed.
</footnote>
<page confidence="0.996559">
136
</page>
<bodyText confidence="0.940724">
grandparent (S0h, S0h2), its two left- and right-
most children (S0L, S0L2, S0R, S0R2), the first
three words of the buffer (N0, N1, N2), and the
two leftmost children of N0 (N0L, N0LL). We ex-
tend this set with the S0l,, S0T, and N0l, tokens
described above, and also the first left and right
child of S0 and N0 (S0L0, S0R0, N0L0).
All up, there are 18 context tokens, so
�18
) = 153 token pairs. For each pair of these
2
tokens, we add two binary features, indicating
whether the two tokens match in word form or POS
tag. We also have two further classes of features:
if the words do match, a feature is added indicat-
ing the word form; if the tags match, a feature is
added indicating the tag. These finer grained ver-
sions help the model adjust for the fact that some
words can be duplicated in grammatical sentences
(e.g. ‘that that’), while most rare words cannot.
</bodyText>
<subsectionHeader confidence="0.938957">
5.4 Edited Neighbour Features
</subsectionHeader>
<bodyText confidence="0.999920909090909">
Disfluencies are usually string contiguous, even if
they do not form a single constituent. In these situ-
ations, our model has to make multiple transitions
to mark a single disfluency. For instance, if an ut-
terance begins and the and a, the stack will contain
two entries, for and and the, and two Edit transi-
tions will be required.
To mitigate this disadvantage of our model, we
add four binary features. Two fire when the word
or pair of words immediately preceding N0 have
been marked disfluent; the other two fire when the
word or pair of words immediately following S0
have been marked disfluent. These features pro-
vide an additional string-based view that the parser
would otherwise be missing. Speakers tend be
disfluent in bursts: if the previous word is dis-
fluent, the next word is more likely to be disflu-
ent. These four features are therefore all associ-
ated with positive weights for the Edit transition.
Without these features, we would miss an aspect of
disfluency processing that sequence models natu-
rally capture.
</bodyText>
<sectionHeader confidence="0.994514" genericHeader="method">
6 Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.999978647058824">
We adopt the standard strategy of using a POS
tagger as a pre-process before parsing. Most
transition-based parsers use a structured averaged
perceptron model with beam-search for tagging,
as this model achieves competitive accuracy and
matches the standard dependency parsing archi-
tecture. Our tagger also uses this architecture.
We performed some additional feature engi-
neering for the tagger, in order to improve its accu-
racy given the lack of case distinctions and punc-
tuation in the data. Our additional features use two
sources of unsupervised information. First, we
follow the suggestion of Manning (2011) by us-
ing Brown cluster features to improve the tagger’s
accuracy on unknown words. Second, we com-
pensate for the lack of case distinctions by includ-
ing features that ask what percentage of the time
a word form was seen title-cased, upper-cased and
lower-cased in the Google Web1T corpus.
Where most previous work uses cross-fold
training for the tagger, to ensure that the parser
is trained on tags that reflect run-time accuracies,
we do online training of the tagger alongside the
parser, using the current tagger model to produce
tags during parser training. This had no impact on
parse accuracy, and made it slightly easier to de-
velop our tagger alongside the parser.
The tagger achieved 96.5% accuracy on the de-
velopment data, but when we ran our final test
experiments, we found its accuracy dropped to
96.0%, indicating some over-fitting during our
feature engineering. On the development data, our
parser accuracy improves by about 1% when gold-
standard tags are used.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999987909090909">
We use the Switchboard portion of the Penn Tree-
bank (Marcus et al., 1993), as described in Sec-
tion 2, to train our joint models and evaluate them
on dependency parsing and disfluency detection.
The pre-processing and dependency conversion
are described in Section 2.1. We use the stan-
dard train/dev/test split from Charniak and John-
son (2001): Sections 2 and 3 for training, and Sec-
tion 4 divided into three held-out sections, the first
of which is used for final evaluation.
Our parser evaluation uses the SPARSEVAL
(Roark et al., 2006) metric. However, we wanted
to use the Stanford dependency converter, for the
reasons described in Section 2.1, so we used our
own implementation. Because we do not need to
deal with recognition errors, we do not need to
report our parsing results using P/R/F-measures.
Instead, we report an unlabelled accuracy score,
which refers to the percentage of fluent words
whose governors were assigned correctly. Note
that words marked as disfluent cannot have any in-
coming or out-going dependencies, so if a word is
</bodyText>
<page confidence="0.995833">
137
</page>
<bodyText confidence="0.993939557692308">
incorrectly marked as disfluent, all of its depen-
dencies will be incorrect.
We follow Johnson and Charniak (2004) and
others in restricting our disfluency evaluation to
speech repairs, which we identify as words that
have a node labelled EDITED as an ancestor. Un-
like most other disfluency detection research, we
train only on the MRG files, giving us 619,236
words of training data instead of the 1,482,845
used by the pipeline systems. It may be possible
to improve our system’s disfluency detection by
leveraging the additional data that does not have
syntactic annotation in some way.
All parsing models were trained for 15 itera-
tions. We found that optimising the number of
iterations on a development set led to small im-
provements that did not transfer to a second devel-
opment set (part of Section 4, which Charniak and
Johnson (2001) reserved for ‘future use’).
We test for statistical significance in our results
by training 20 models for each experimental con-
figuration, using different random seeds. The ran-
dom seeds control how the sentences are shuf-
fled during training, which the perceptron model
is quite sensitive to. We use the Wilcoxon rank-
sums non-parametric test. The standard deviation
in UAS for a sample was typically around 0.05%,
and 0.5% for disfluency F-measure.
All of our models use beam-search decoding,
with a beam width of 32. We found that a beam
width of 64 brought a very small accuracy im-
provement (about 0.1%), at the cost of 50% slower
run-time. Wider beams than this brought no ac-
curacy improvement. Accuracy seems to plateau
with slightly narrower beams than on newswire
text. This is probably due to the shorter sentences
in Switchboard.
The baseline and pipeline systems are config-
ured in the same way, except that the baseline
parser is modified slightly to allow it to predict
disfluencies, using a special dependency label,
ERASED. All descendants of a word attached to its
head by this label are marked as disfluent. Both the
baseline and pipeline/oracle parsers use the same
feature set: the Zhang and Nivre (2011) features,
plus our Brown cluster features.
The baseline system is a standard arc-eager
transition-based parser with a structured averaged
perceptron model and beam-search decoding. The
model is trained in the standard way, with a ‘static’
oracle and maximum-violation update, following
(Huang et al., 2012).
</bodyText>
<subsectionHeader confidence="0.998995">
7.1 Comparison with Pipeline Approaches
</subsectionHeader>
<bodyText confidence="0.99993465">
The accuracy of incremental dependency parsers
is well established on the Wall Street Journal, but
there are no dependency parsing results in the lit-
erature that make it easy to put our joint model’s
parsing accuracy into context. We therefore com-
pare our joint model to two pipeline systems,
which consist of a disfluency detector, followed by
our dependency parser. We also evaluate parse ac-
curacies after oracle pre-processing, to gauge the
net effect of disfluencies on our parser’s accuracy.
The dependency parser for the pipeline systems
was trained on text with all disfluencies removed,
following Charniak and Johnson (2001). The two
disfluency detection systems we used were the
Qian and Liu (2013) sequence-tagging model, and
a version of the Johnson and Charniak (2004)
noisy channel model, using the Charniak (2001)
syntactic language model and the reranking fea-
tures of Zwarts and Johnson (2011). They are the
two best published disfluency detection systems.
</bodyText>
<sectionHeader confidence="0.999589" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999937653846154">
Table 1 shows the development set accuracies for
our joint parser. Both the disfluency features and
the Edit transition make statistically significant
improvements, in both disfluency F-measure, un-
labelled attachment score (UAS), and labelled at-
tachment score (LAS).
The Oracle pipeline system, which uses the
gold-standard to clean disfluencies prior to pars-
ing, shows the total impact of speech-errors on the
parser. The baseline parser, which uses the Zhang
and Nivre (2011) feature set plus the Brown clus-
ter features, scores 1.8% UAS lower than the ora-
cle.
When we add the features described in Sec-
tions 5.2, 5.3 and 5.4, the gap is reduced to 1.2%
(+Features). Finally, the improved transition sys-
tem reduces the gap further still, to 0.8% UAS
(+Edit transition). We also tested these features
in the Oracle parser, but found they were ineffec-
tive on fluent text.
The w/s column shows the tokens analysed per
second for each system, including disfluencies,
with a single thread on a 2.4GHz Intel Xeon. The
additional features reduce efficiency, but the non-
monotonic Edit transition does not. The system is
easily efficient enough for real-time use.
</bodyText>
<page confidence="0.993505">
138
</page>
<table confidence="0.9998742">
P R F UAS LAS w/s
Baseline joint 79.4 70.1 74.5 89.9 86.9 711
+Features 86.0 77.2 81.3 90.5 87.5 539
+Edit transition 92.2 80.2 85.8 90.9 87.9 555
Oracle pipeline 100 100 100 91.7 88.6 782
</table>
<tableCaption confidence="0.5951482">
Table 1: Development results for the joint models. For the
baseline model, disfluencies reduce parse accuracy by 1.7%
Unlabelled Attachment Score (UAS). Our features and Edit
transition reduce the gap to 0.7%, and improve disfluency de-
tection by 11.3% F-measure.
</tableCaption>
<table confidence="0.999846">
Disfl. F UAS
Johnson et al pipeline 82.1 90.3
Qian and Liu pipeline 83.9 90.1
Baseline joint parser 73.9 89.4
Final joint parser 84.1 90.5
</table>
<tableCaption confidence="0.934006">
Table 2: Test-set parse and disfluency accuracies. The joint
parser is improved by the features and Edit transition, and is
better than pre-processing the text with state-of-the-art disflu-
ency detectors.
</tableCaption>
<bodyText confidence="0.997305163265306">
Table 2 shows the final evaluation. Our main
comparison is with the two pipeline systems, de-
scribed in Section 7.1. The Johnson and Char-
niak (2004) system was 1.8% less accurate at dis-
fluency detection than the other disfluency detec-
tor we evaluated, the state-of-the-art Qian and Liu
(2013) system. However, when we evaluated the
two systems as pre-processors before our parser,
we found that the Johnson et al pipeline achieved
0.2% better unlabelled attachment score than the
Qian and Liu pipeline. We attribute this to the
use of the Charniak and Johnson (2001) syntac-
tic language model in the Johnson et al pipeline,
which would help the system produce more syn-
tactically consistent output.
Our joint model achieved an unlabelled at-
tachment score of 90.5%, out-performing both
pipeline systems. The Baseline joint parser,
which did not include the Edit transition or disflu-
ency features, scores 1.1% below the Final joint
parser. All of the parse accuracy differences were
found to be statistically significant (p &lt; 0.001).
The Edit transition and disfluency features to-
gether brought a 10.1% improvement in disfluency
F-measure, which was also found to be statisti-
cally significant. The final joint parser achieved
0.2% higher disfluency detection accuracy than
the previous state-of-the-art, the Qian and Liu
(2013) system,5 despite having approximately half
as much training data (we require syntactic anno-
5 Our scores refer to an updated version of the system that
corrects minor pre-processing problems. We thank Qian Xian
for making his code available.
tation, for which there is less data).
Our significance testing regime involved using
20 different random seeds when training each of
our models, which the perceptron algorithm is sen-
sitive to. This could not be applied to the other two
disfluency detectors, so we cannot test those dif-
ferences for significance. However, we note that
the 20 samples for our disfluency detector ranged
in accuracy from 83.3-84.6, so we doubt that 0.2%
mean improvement over the Qian and Liu (2013)
result is meaningful.
Although we did not systematically optimise
on the development set, our test scores are lower
than our development accuracies. Much of the
over-fitting seems to be in the POS tagger, which
dropped in accuracy by 0.5%.
</bodyText>
<subsectionHeader confidence="0.653831">
9 Analysis of Edit Behaviour
</subsectionHeader>
<bodyText confidence="0.999983242424243">
In order to understand how the parser applies
the Edit transition, we collected some additional
statistics over the development data. The parser
predicted 2,558 Edit transitions, which together
marked 2,706 words disfluent (2,495 correctly).
The Edit transition can mark multiple words dis-
fluent when S0 has one or more rightward descen-
dants. It turns out this case is uncommon; the
parser largely assigns disfluency labels word-by-
word, only sometimes marking words with right-
ward descendents as disfluent.
Of the 2,558 Edit transitions, there were 682
cases were at least one leftward child was returned
to the stack, and the total number of leftward chil-
dren returned was 1,132. The most common type
of construction that caused the parser to return
words to the stack were disfluent predicates, which
often have subjects and discourse conjunctions as
leftward children. An example of a disfluent pred-
icate with a fluent subject is shown in Figure 4.
There were only 48 cases of the same word be-
ing returned to the stack twice. The possibility of
words being returned to the stack multiple times
is what gives our system worse than linear worst-
case complexity. In the worst case, the ith word
of a sentence of length n could be returned to the
stack n − (i + 1) times. Empirically, the Edit tran-
sition made no difference to run-time.
Once a word has been returned to the stack by
the Edit transition, how does the parser end up
analysing it? If it turned out that almost all of
the former leftward children of disfluent words are
subsequently marked as disfluent, there would be
</bodyText>
<page confidence="0.99664">
139
</page>
<bodyText confidence="0.999908526315789">
little point in returning them to the stack — we
could just mark them as disfluent in the original
Edit transition. On the other hand, if they are al-
most all marked as fluent, perhaps they can just be
attached as children to the first word of the buffer.
In fact the two cases are almost equally com-
mon. Of the 1,132 words returned to the stack,
547 were subsequently marked disfluent, and 584
were not. The parser was also quite accurate in
its decisions over these tokens. Of the 547 tokens
marked disfluent, 500 were correct — similar to
the overall development set precision, 92.2%.
Accuracy over the words returned to the stack
might be improved in future by features referring
to their former heads. For instance, in He went
broke uh became bankrupt, we do not currently
have features that record the deleted dependency
became he and went. We thank one of the anony-
mous reviewers for this suggestion.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="related work">
10 Related Work
</sectionHeader>
<bodyText confidence="0.999983578947368">
The most similar system to ours was published
very recently. Rasooli and Tetreault (2013) de-
scribe a joint model of dependency parsing and
disfluency detection. They introduce a second
classification step, where they first decide whether
to apply a disfluency transition, or a regular pars-
ing move. Disfluency transitions operate either
over a sequence of words before the start of the
buffer, or a sequence of words from the start of
the buffer forward. Instead of the dynamic oracle
training method that we employ, they use a two-
stage bootstrap-style process.
Direct comparison between our model and
theirs is difficult, as they use the Penn2MALT
scheme, and their parser uses greedy decoding,
where we use beam search. They also use gold-
standard part-of-speech tags, which would im-
prove our scores by around 1%. The use of
beam-search may explain much of our perfor-
mance advantage: they report an unlabelled at-
tachment score of 88.6, and a disfluency detec-
tion F-measure of 81.4%. Our training algorithm
would be applicable to a beam-search version of
their parser, as their transition-system also intro-
duces substantial spurious ambiguity, and some
non-monotonic behaviour.
A hybrid transition system would also be possi-
ble, as the two types of Edit transition seem to be
complementary. The Rasooli and Tetreault system
offers a token-based view of disfluencies, which
is useful for examples such as, and the and the,
which would require two applications of our tran-
sition. On the other hand, our Edit transition may
have the advantage for more syntactically compli-
cated examples, particularly for disfluent verbs.
The importance of syntactic features for disflu-
ency detection was demonstrated by Johnson and
Charniak (2004). Despite this, most subsequent
work has used sequence models, rather than syn-
tactic parsers. The other disfluency system that
we compare our model to, developed by Qian and
Liu (2013), uses a cascade of Maximum Margin
Markov Models to perform disfluency detection
with minimal syntactic information.
One motivation for sequential approaches is that
most applications of these models will be over un-
segmented text, as segmenting unpunctuated text
is a difficult task that benefits from syntactic fea-
tures (Zhang et al., 2013).
We consider the most promising aspect of our
system to be that it is naturally incremental, so it
should be straightforward to extend the system to
operate on unsegmented text in subsequent work.
Due to its use of syntactic features, from the joint
model, the system is substantially more accurate
than the previous state-of-the-art in incremental
disfluency detection, 77% (Zwarts et al., 2010).
</bodyText>
<sectionHeader confidence="0.982391" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.9999215">
We have presented an efficient and accurate joint
model of dependency parsing and disfluency de-
tection. The model out-performs pipeline ap-
proaches using state-of-the-art disfluency detec-
tors, and is highly efficient, processing over 550
tokens a second. Because the system is incremen-
tal, it should be straight-forward to apply it to un-
segmented text. The success of an incremental,
non-monotonic parser at disfluent speech parsing
may also be of some psycholinguistic interest.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999695833333333">
The authors would like to thank the anony-
mous reviewers for their valuable comments.
This research was supported under the Aus-
tralian Research Council’s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593).
</bodyText>
<sectionHeader confidence="0.980524" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.946972666666667">
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
</bodyText>
<page confidence="0.996131">
140
</page>
<reference confidence="0.983668737373737">
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467–
479.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of 39th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 124–131. Associa-
tion for Computational Linguistics, Toulouse,
France.
Eugene Charniak and Mark Johnson. 2001. Edit
detection and parsing for transcribed speech. In
Proceedings of the 2nd Meeting of the North
American Chapter of the Association for Com-
putational Linguistics, pages 118–126. The As-
sociation for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 173–180. As-
sociation for Computational Linguistics, Ann
Arbor, Michigan.
Michael Collins. 2002. Discriminative training
methods for hidden Markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1–8. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalu-
ation (LREC).
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178–210.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Matthew Honnibal, Yoav Goldberg, and Mark
Johnson. 2013. A non-monotonic arc-eager
transition system for dependency parsing. In
Proceedings of the Seventeenth Conference on
Computational Natural Language Learning,
pages 163–172. Association for Computational
Linguistics, Sofia, Bulgaria.
Liang Huang, Suphan Fayong, and Yang Guo.
2012. Structured perceptron with inexact
search. In Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 142–
151. Association for Computational Linguis-
tics, Montr´eal, Canada.
Mark Johnson and Eugene Charniak. 2004. A
TAG-based noisy channel model of speech re-
pairs. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 33–39.
Douglas A. Jones, Florian Wolf, Edward Gib-
son, Elliott Williams, Evelina Fedorenko, Dou-
glas A. Reynolds, and Marc A. Zissman. 2003.
Measuring the readability of automatic speech-
to-text transcripts. In INTERSPEECH. ISCA.
Fredrik Jorgensen. 2007. The effects of disflu-
ency detection in parsing spoken language. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of
the 16th Nordic Conference of Computational
Linguistics NODALIDA-2007, pages 240–244.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1–
11.
Percy Liang. 2005. Semi-supervised learning for
natural language. Ph.D. thesis, MIT.
Christopher D. Manning. 2011. Part-of-speech
tagging from 97linguistics? In Proceedings of
the 12th international conference on Computa-
tional linguistics and intelligent text processing
- Volume Part I, CICLing’11, pages 171–189.
Springer-Verlag, Berlin, Heidelberg.
Michell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
</reference>
<page confidence="0.97984">
141
</page>
<reference confidence="0.998893247191011">
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149–160.
Joakim Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computa-
tional Linguistics, 34:513–553.
Xian Qian and Yang Liu. 2013. Disfluency detec-
tion using multi-step stacked learning. In Pro-
ceedings of the 2013 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 820–825. Association for Com-
putational Linguistics, Atlanta, Georgia.
Mohammad Sadegh Rasooli and Joel Tetreault.
2013. Joint parsing and disfluency detection in
linear time. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 124–129. Association
for Computational Linguistics, Seattle, Wash-
ington, USA.
Brian Roark, Mary Harper, Eugene Charniak,
Bonnie Dorr, Mark Johnson, Jeremy Kahn,
Yang Liu, Mary Ostendorf, John Hale, Anna
Krasnyanskaya, Matthew Lease, Izhak Shafran,
Matthew Snover, Robin Stewart, and LisaYung.
2006. Sparseval: Evaluation metrics for pars-
ing speech. In Proceedings of Language Re-
source and Evaluation Conference, pages 333–
338. European Language Resources Associa-
tion (ELRA), Genoa, Italy.
Francesco Sartorio, Giorgio Satta, and Joakim
Nivre. 2013. A transition-based dependency
parser using a dynamic parsing strategy. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
135–144. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Elizabeth Shriberg. 1994. Preliminaries to a The-
ory of Speech Disfluencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,
and Jun’ichi Tsujii. 2009. Latent variable per-
ceptron algorithm for structured classification.
In IJCAI, pages 1236–1242.
Joseph Turian, Lev-Arie Ratinov, and Yoshua
Bengio. 2010. Word representations: A simple
and general method for semi-supervised learn-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics, pages 384–394. Association for Computa-
tional Linguistics, Uppsala, Sweden.
Dongdong Zhang, Shuangzhi Wu, Nan Yang, and
Mu Li. 2013. Punctuation prediction with
transition-based parsing. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, pages 752–760. As-
sociation for Computational Linguistics, Sofia,
Bulgaria.
Yue Zhang and Stephen Clark. 2011. Syntac-
tic processing using the generalized perceptron
and beam search. Computational Linguistics,
37(1):105–151.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188–193. Association for Computational
Linguistics, Portland, USA.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min
Zhang, and Jingbo Zhu. 2013. Fast and accu-
rate shift-reduce constituent parsing. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages
434–443. Association for Computational Lin-
guistics, Sofia, Bulgaria.
Simon Zwarts and Mark Johnson. 2011. The im-
pact of language models and loss functions on
repair disfluency detection. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 703–711. Association for
Computational Linguistics, Portland, USA.
Simon Zwarts, Mark Johnson, and Robert Dale.
2010. Detecting speech repairs incrementally
using a noisy channel approach. In Proceedings
of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages
1371–1378. Coling 2010 Organizing Commit-
tee, Beijing, China.
</reference>
<page confidence="0.997731">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.435903">
<title confidence="0.999728">Joint Incremental Disfluency Detection and Dependency Parsing</title>
<author confidence="0.992081">Matthew</author>
<affiliation confidence="0.871337">Department of Macquarie</affiliation>
<address confidence="0.83302">Sydney, Australia</address>
<email confidence="0.996458">matthew.honnibal@mq.edu.edu.au</email>
<author confidence="0.991544">Mark</author>
<affiliation confidence="0.8741905">Department of Macquarie</affiliation>
<address confidence="0.839415">Sydney, Australia</address>
<email confidence="0.996398">mark.johnson@mq.edu.edu.au</email>
<abstract confidence="0.994442928571429">We present an incremental dependency parsing model that jointly performs disfluency detection. The model handles speech repairs using a novel non-monotonic transition system, and includes several novel classes of features. For we evaluated two pipeline systems, using state-of-the-art disfluency detectors. The joint model performed better on both tasks, with a parse accuracy of 90.5% and 84.0% accuracy at disfluency detection. The model runs in expected linear time, and processes over 550 tokens a second.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>479</pages>
<contexts>
<context position="22754" citStr="Brown et al., 1992" startWordPosition="3804" endWordPosition="3807"> binned with the function Ax : min(x, 5). There is only one bilexical feature template, which pairs the words of S0 and N0. There are also ten tri-tag templates. Our feature set includes additional dependency label features not used by Zhang and Nivre (2011), as we found that disfluency detection errors often resulted in ungrammatical dependency label combinations. The additional templates combine the POS tag of S0 with two or three dependency labels from its left and right subtrees. Details can be found in the supplementary material. 5.1 Brown Cluster Features The Brown clustering algorithm (Brown et al., 1992) is a well known source of semi-supervised features. The clustering algorithm is run over a large sample of unlabelled data, to generate a type-to-cluster map. This mapping is then used to generate features that sometimes generalise better than lexical features, and are helpful for out-ofvocabulary words (Turian et al., 2010). Koo and Collins (2010) found that Brown cluster features greatly improved the performance of a graph-based dependency parser. On our transitionbased parser, Brown cluster features bring a small but statistically significant improvement on the WSJ task (0.1-0.3% UAS). Oth</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467– 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>124--131</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="34115" citStr="Charniak (2001)" startWordPosition="5704" endWordPosition="5705">racy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-errors on the parser. The baseline parser, which</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 124–131. Association for Computational Linguistics, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="5181" citStr="Charniak and Johnson (2001)" startWordPosition="779" endWordPosition="782">ovided: one for syntactic bracketing (MRG files), and one for disfluencies (DPS files). The disfluency layer marks elements with little or no syntactic function, such as filled pauses and discourse markers, and annotates speech repairs using the Shriberg (1994) system of reparandum/interregnum/repair. An example is shown in Figure 1. In the syntactic annotation, edited words are covered by a special node labelled EDITED. The idea is to mark text which, if excised, would result in a grammatical sentence. The MRG files do not mark other types of disfluencies. We follow the evaluation defined by Charniak and Johnson (2001), which evaluates the accuracy of identifying speech repairs and restarts. This definition of the task is the standard in recent work. The reason for this is that filled pauses can be detected using a simple rule-based approach, and parentheticals have less impact on readability and down-stream processing accuracy. The MRG and DPS layers have high but imperfect agreement over what tokens they mark as speech repairs: of the text annotated with both layers, 33,720 tokens are marked as disfluent in at least one layer, 32,310 are only marked as disfluent by the DPS files, and 32,742 are only marke</context>
<context position="30169" citStr="Charniak and Johnson (2001)" startWordPosition="5059" endWordPosition="5063">y on the development data, but when we ran our final test experiments, we found its accuracy dropped to 96.0%, indicating some over-fitting during our feature engineering. On the development data, our parser accuracy improves by about 1% when goldstandard tags are used. 7 Experiments We use the Switchboard portion of the Penn Treebank (Marcus et al., 1993), as described in Section 2, to train our joint models and evaluate them on dependency parsing and disfluency detection. The pre-processing and dependency conversion are described in Section 2.1. We use the standard train/dev/test split from Charniak and Johnson (2001): Sections 2 and 3 for training, and Section 4 divided into three held-out sections, the first of which is used for final evaluation. Our parser evaluation uses the SPARSEVAL (Roark et al., 2006) metric. However, we wanted to use the Stanford dependency converter, for the reasons described in Section 2.1, so we used our own implementation. Because we do not need to deal with recognition errors, we do not need to report our parsing results using P/R/F-measures. Instead, we report an unlabelled accuracy score, which refers to the percentage of fluent words whose governors were assigned correctly</context>
<context position="31719" citStr="Charniak and Johnson (2001)" startWordPosition="5319" endWordPosition="5322">hat have a node labelled EDITED as an ancestor. Unlike most other disfluency detection research, we train only on the MRG files, giving us 619,236 words of training data instead of the 1,482,845 used by the pipeline systems. It may be possible to improve our system’s disfluency detection by leveraging the additional data that does not have syntactic annotation in some way. All parsing models were trained for 15 iterations. We found that optimising the number of iterations on a development set led to small improvements that did not transfer to a second development set (part of Section 4, which Charniak and Johnson (2001) reserved for ‘future use’). We test for statistical significance in our results by training 20 models for each experimental configuration, using different random seeds. The random seeds control how the sentences are shuffled during training, which the perceptron model is quite sensitive to. We use the Wilcoxon ranksums non-parametric test. The standard deviation in UAS for a sample was typically around 0.05%, and 0.5% for disfluency F-measure. All of our models use beam-search decoding, with a beam width of 32. We found that a beam width of 64 brought a very small accuracy improvement (about </context>
<context position="33920" citStr="Charniak and Johnson (2001)" startWordPosition="5671" endWordPosition="5674">ccuracy of incremental dependency parsers is well established on the Wall Street Journal, but there are no dependency parsing results in the literature that make it easy to put our joint model’s parsing accuracy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachm</context>
<context position="36789" citStr="Charniak and Johnson (2001)" startWordPosition="6140" endWordPosition="6143">ocessing the text with state-of-the-art disfluency detectors. Table 2 shows the final evaluation. Our main comparison is with the two pipeline systems, described in Section 7.1. The Johnson and Charniak (2004) system was 1.8% less accurate at disfluency detection than the other disfluency detector we evaluated, the state-of-the-art Qian and Liu (2013) system. However, when we evaluated the two systems as pre-processors before our parser, we found that the Johnson et al pipeline achieved 0.2% better unlabelled attachment score than the Qian and Liu pipeline. We attribute this to the use of the Charniak and Johnson (2001) syntactic language model in the Johnson et al pipeline, which would help the system produce more syntactically consistent output. Our joint model achieved an unlabelled attachment score of 90.5%, out-performing both pipeline systems. The Baseline joint parser, which did not include the Edit transition or disfluency features, scores 1.1% below the Final joint parser. All of the parse accuracy differences were found to be statistically significant (p &lt; 0.001). The Edit transition and disfluency features together brought a 10.1% improvement in disfluency F-measure, which was also found to be sta</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pages 118–126. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="9178" citStr="Charniak and Johnson (2005)" startWordPosition="1425" endWordPosition="1428">ransition-based parser predicts the syntactic structure of a sentence incrementally, by making a sequence of classification decisions. We follow the architecture of Zhang and Clark (2011), who use beam-search for decoding, and a structured averaged perceptron for training. Despite its simplicity, this type of parser has produced highly competitive results on the Wall Street Journal: with the extended feature set described by Zhang and Nivre (2011), it achieves 93.5% unlabelled accuracy on Stanford basic dependencies (de Marneffe et al., 2006). Converting the constituency trees produced by the Charniak and Johnson (2005) reranking parser results in similar accuracy. Briefly, the transition-based parser consists of a configuration (or ‘state’) which is sequentially manipulated by a set of possible transitions. For us, a state is a 4-tuple c = (v, β, A, D), where v and β are disjoint sets of word indices termed the stack and buffer respectively, A is the set of dependency arcs, and D is the set of word indices marked disfluent. There are no arcs to or from members of D, so the dependencies and disfluencies can be implemented as a single vector (in our parser, a token is marked as disfluent by setting it as its </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2013" citStr="Collins, 2002" startWordPosition="290" endWordPosition="291"> work in transition-based dependency parsing, to perform the two tasks jointly. There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components. Our parser largely follows the design of Zhang and Clark (2011). We use a structured averaged perceptron model with beamsearch decoding (Collins, 2002). Our feature set is based on Zhang and Clark (2011), and our transition-system is based on the arc-eager system of Nivre (2003). We extend the transition system with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be parsed incrementally, without the need to guess early that pepper is disfluent. This is achieved by reprocessing the leftward children of the word Edit marks as disfluent. For instance, if the parser attaches the to pepper, but subsequently marks pepper as disfluent, the will be returned to the stack. We also exploit the ease with whi</context>
<context position="18236" citStr="Collins (2002)" startWordPosition="3044" endWordPosition="3045">se went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequences that can result in the correct analysis as equally valid, and want to avoid stipulating one of them during training. We achieve this by following Goldberg and Nivre (2012) in using a dynamic oracle to create partially labelled training data.2 A dynamic oracle is a function that determines the cost of applying an action to a state, in terms of gold-standard arcs that are newly unreachable. We follow Collins (2002) in training an averaged perceptron model to predict transition sequences, rather than individual transitions. This type of model is often referred to as a structured perceptron, or sometimes a global perceptron. During training, if the model does not predict the correct sequence, an update is performed, based on the gold-standard sequence and part of the sequence predicted by the current weights. Only part of the sequence is used to calculate the weight update, in order to account for search errors. We use the maximum violation strategy described by Huang et al. (2012) to select the subsequen</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Keith Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.</title>
<date>1982</date>
<journal>Cognitive Psychology,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="12559" citStr="Frazier and Rayner, 1982" startWordPosition="2055" endWordPosition="2058">t necessarily a projective tree. Figure 3 shows a simple example, where the repair square replaces the reparandum rectangle. An incremental parser could easily become ‘gardenpathed’ and attach the repair square to the preceding words, constructing the dependencies shown dotted in Figure 3. Rather than attempting to devise an incremental model that avoids constructing such dependencies, we allow the parser to construct these dependencies and later delete them if the governor or child are marked disfluent. Psycholinguistic models of human sentence processing have long posited repair mechanisms (Frazier and Rayner, 1982). Recently, Honnibal et al. (2013) showed that a limited amount of ‘nonmonotonic’ behaviour can improve an incremental parser’s accuracy. We here introduce a nonmonotonic transition, Edit, for speech repairs. The Edit transition marks the word i on top of the stack v|i as disfluent, along with its rightward descendents — i.e., all words in the sequence i...j − 1, where j is the word at the start of the buffer. It then restores the words both preceding and formerly governed by i to the stack. In other words, the word on top of the stack and 133 1. S His company went broke i mean went bankrupt P</context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Lyn Frazier and Keith Rayner. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14(2):178–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for arc-eager dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (Coling 2012). Association for Computational Linguistics,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="17266" citStr="Goldberg and Nivre, 2012" startWordPosition="2884" endWordPosition="2887">ggestion of Ballesteros and Nivre (2013). The final action will be a Left-Arc from the ROOT token to went. 4.2 Dynamic Oracle Training Algorithm Our non-monotonic transition system introduces substantial spurious ambiguity: the gold-standard parse can be derived via many different transition 11. S His company went broke i mean went bankrupt 12. R His company went broke i mean went bankrupt 13. D His company went .broW i mean went bankrupt 12. R His company svenr .broke i mean went 134 sequences. Recent work has shown that this can be advantageous (Sartorio et al., 2013; Honnibal et al., 2013; Goldberg and Nivre, 2012), because difficult decisions can sometimes be delayed until more information is available. Line 5 of Figure 4 shows a state that introduces spurious ambiguity. From this configuration, there are multiple actions that could be considered ‘correct’, in the sense that the gold-standard analysis can be derived from them. The Edit transition is correct because went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequences that can result in the correct analysis as equ</context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proceedings of the 24th International Conference on Computational Linguistics (Coling 2012). Association for Computational Linguistics, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Yoav Goldberg</author>
<author>Mark Johnson</author>
</authors>
<title>A non-monotonic arc-eager transition system for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>163--172</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="12593" citStr="Honnibal et al. (2013)" startWordPosition="2060" endWordPosition="2063">re 3 shows a simple example, where the repair square replaces the reparandum rectangle. An incremental parser could easily become ‘gardenpathed’ and attach the repair square to the preceding words, constructing the dependencies shown dotted in Figure 3. Rather than attempting to devise an incremental model that avoids constructing such dependencies, we allow the parser to construct these dependencies and later delete them if the governor or child are marked disfluent. Psycholinguistic models of human sentence processing have long posited repair mechanisms (Frazier and Rayner, 1982). Recently, Honnibal et al. (2013) showed that a limited amount of ‘nonmonotonic’ behaviour can improve an incremental parser’s accuracy. We here introduce a nonmonotonic transition, Edit, for speech repairs. The Edit transition marks the word i on top of the stack v|i as disfluent, along with its rightward descendents — i.e., all words in the sequence i...j − 1, where j is the word at the start of the buffer. It then restores the words both preceding and formerly governed by i to the stack. In other words, the word on top of the stack and 133 1. S His company went broke i mean went bankrupt Pass me the red rectangle uh I mean</context>
<context position="17239" citStr="Honnibal et al., 2013" startWordPosition="2880" endWordPosition="2883">tence, following the suggestion of Ballesteros and Nivre (2013). The final action will be a Left-Arc from the ROOT token to went. 4.2 Dynamic Oracle Training Algorithm Our non-monotonic transition system introduces substantial spurious ambiguity: the gold-standard parse can be derived via many different transition 11. S His company went broke i mean went bankrupt 12. R His company went broke i mean went bankrupt 13. D His company went .broW i mean went bankrupt 12. R His company svenr .broke i mean went 134 sequences. Recent work has shown that this can be advantageous (Sartorio et al., 2013; Honnibal et al., 2013; Goldberg and Nivre, 2012), because difficult decisions can sometimes be delayed until more information is available. Line 5 of Figure 4 shows a state that introduces spurious ambiguity. From this configuration, there are multiple actions that could be considered ‘correct’, in the sense that the gold-standard analysis can be derived from them. The Edit transition is correct because went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequences that can result in </context>
</contexts>
<marker>Honnibal, Goldberg, Johnson, 2013</marker>
<rawString>Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system for dependency parsing. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163–172. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="18812" citStr="Huang et al. (2012)" startWordPosition="3141" endWordPosition="3144">newly unreachable. We follow Collins (2002) in training an averaged perceptron model to predict transition sequences, rather than individual transitions. This type of model is often referred to as a structured perceptron, or sometimes a global perceptron. During training, if the model does not predict the correct sequence, an update is performed, based on the gold-standard sequence and part of the sequence predicted by the current weights. Only part of the sequence is used to calculate the weight update, in order to account for search errors. We use the maximum violation strategy described by Huang et al. (2012) to select the subsequence to update from. To train our model using the dynamic oracle, we use the latent-variable structured perceptron algorithm described by Sun et al. (2009). Beamsearch is performed to find the highest-scoring gold-standard sequence, as well as the highestscoring prediction. We use the same beam-width for both search procedures. 4.3 Path Length Normalisation One problem introduced by the Edit transition is that the number of actions applied to a sentence is 2 The training data is partially labelled in the sense that instances can have multiple true labels. Equivalently, on</context>
<context position="33246" citStr="Huang et al., 2012" startWordPosition="5566" endWordPosition="5569">pt that the baseline parser is modified slightly to allow it to predict disfluencies, using a special dependency label, ERASED. All descendants of a word attached to its head by this label are marked as disfluent. Both the baseline and pipeline/oracle parsers use the same feature set: the Zhang and Nivre (2011) features, plus our Brown cluster features. The baseline system is a standard arc-eager transition-based parser with a structured averaged perceptron model and beam-search decoding. The model is trained in the standard way, with a ‘static’ oracle and maximum-violation update, following (Huang et al., 2012). 7.1 Comparison with Pipeline Approaches The accuracy of incremental dependency parsers is well established on the Wall Street Journal, but there are no dependency parsing results in the literature that make it easy to put our joint model’s parsing accuracy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142– 151. Association for Computational Linguistics, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAG-based noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--39</pages>
<contexts>
<context position="2802" citStr="Johnson and Charniak (2004)" startWordPosition="421" endWordPosition="424">with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be parsed incrementally, without the need to guess early that pepper is disfluent. This is achieved by reprocessing the leftward children of the word Edit marks as disfluent. For instance, if the parser attaches the to pepper, but subsequently marks pepper as disfluent, the will be returned to the stack. We also exploit the ease with which the model can incorporate arbitrary features, and design a set of features that capture the ‘rough copy’ structure of some speech repairs, which motivated the Johnson and Charniak (2004) noisy channel model. Our main comparison is against two pipeline systems, which use the two current state-of-theart disfluency detection systems as pre-processors to our parser, minus the custom disfluency features and transition. The joint model compared favourably to the pipeline parsers at both tasks, with an unlabelled attachment score of 90.5%, and 84.0% accuracy at detecting speech repairs. An efficient implementation is available under an opensource license.1 The future prospects of the system are also quite promising. Because the parser is incremental, it should be well suited to unse</context>
<context position="24331" citStr="Johnson and Charniak (2004)" startWordPosition="4049" endWordPosition="4052"> as the full clusters. We adapt their templates to transition-based parsing by replacing ‘head’ with ‘item on top of the stack’ and ‘child’ with ‘first word of the buffer’. The exact templates can be found in the supplementary material. The Brown cluster features are used in our ‘baseline’ parser, and in the parsers we use as part of our pipeline systems. They improved development set accuracy by 0.4%. We experimented with the other feature sets in these parsers, but found that they did not improve accuracy on fluent text. 3http://www.metaoptimize.com/projects/wordreps 5.2 Rough Copy Features Johnson and Charniak (2004) point out that in speech repairs, the repair is often a ‘rough copy’ of the reparandum. The simplest case of this is where the repair is a single word repetition. It is common for the repair to differ from the reparandum by insertion, deletion or substitution of one or more words. To capture this regularity, we first extend the feature-set with three new context tokens:4 1. S0re: The rightmost edge of S0 descendants; 2. S0le: The leftmost edge of S0 descendants; 3. N0le: The leftmost edge of N0 descendants. If a word has no leftward children, it will be its own left-edge, and similarly it wil</context>
<context position="30992" citStr="Johnson and Charniak (2004)" startWordPosition="5196" endWordPosition="5199">metric. However, we wanted to use the Stanford dependency converter, for the reasons described in Section 2.1, so we used our own implementation. Because we do not need to deal with recognition errors, we do not need to report our parsing results using P/R/F-measures. Instead, we report an unlabelled accuracy score, which refers to the percentage of fluent words whose governors were assigned correctly. Note that words marked as disfluent cannot have any incoming or out-going dependencies, so if a word is 137 incorrectly marked as disfluent, all of its dependencies will be incorrect. We follow Johnson and Charniak (2004) and others in restricting our disfluency evaluation to speech repairs, which we identify as words that have a node labelled EDITED as an ancestor. Unlike most other disfluency detection research, we train only on the MRG files, giving us 619,236 words of training data instead of the 1,482,845 used by the pipeline systems. It may be possible to improve our system’s disfluency detection by leveraging the additional data that does not have syntactic annotation in some way. All parsing models were trained for 15 iterations. We found that optimising the number of iterations on a development set le</context>
<context position="34068" citStr="Johnson and Charniak (2004)" startWordPosition="5695" endWordPosition="5698">ure that make it easy to put our joint model’s parsing accuracy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-e</context>
<context position="36371" citStr="Johnson and Charniak (2004)" startWordPosition="6071" endWordPosition="6075">ed Attachment Score (UAS). Our features and Edit transition reduce the gap to 0.7%, and improve disfluency detection by 11.3% F-measure. Disfl. F UAS Johnson et al pipeline 82.1 90.3 Qian and Liu pipeline 83.9 90.1 Baseline joint parser 73.9 89.4 Final joint parser 84.1 90.5 Table 2: Test-set parse and disfluency accuracies. The joint parser is improved by the features and Edit transition, and is better than pre-processing the text with state-of-the-art disfluency detectors. Table 2 shows the final evaluation. Our main comparison is with the two pipeline systems, described in Section 7.1. The Johnson and Charniak (2004) system was 1.8% less accurate at disfluency detection than the other disfluency detector we evaluated, the state-of-the-art Qian and Liu (2013) system. However, when we evaluated the two systems as pre-processors before our parser, we found that the Johnson et al pipeline achieved 0.2% better unlabelled attachment score than the Qian and Liu pipeline. We attribute this to the use of the Charniak and Johnson (2001) syntactic language model in the Johnson et al pipeline, which would help the system produce more syntactically consistent output. Our joint model achieved an unlabelled attachment s</context>
<context position="42773" citStr="Johnson and Charniak (2004)" startWordPosition="7129" endWordPosition="7132">introduces substantial spurious ambiguity, and some non-monotonic behaviour. A hybrid transition system would also be possible, as the two types of Edit transition seem to be complementary. The Rasooli and Tetreault system offers a token-based view of disfluencies, which is useful for examples such as, and the and the, which would require two applications of our transition. On the other hand, our Edit transition may have the advantage for more syntactically complicated examples, particularly for disfluent verbs. The importance of syntactic features for disfluency detection was demonstrated by Johnson and Charniak (2004). Despite this, most subsequent work has used sequence models, rather than syntactic parsers. The other disfluency system that we compare our model to, developed by Qian and Liu (2013), uses a cascade of Maximum Margin Markov Models to perform disfluency detection with minimal syntactic information. One motivation for sequential approaches is that most applications of these models will be over unsegmented text, as segmenting unpunctuated text is a difficult task that benefits from syntactic features (Zhang et al., 2013). We consider the most promising aspect of our system to be that it is natu</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAG-based noisy channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 33–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas A Jones</author>
<author>Florian Wolf</author>
<author>Edward Gibson</author>
<author>Elliott Williams</author>
<author>Evelina Fedorenko</author>
<author>Douglas A Reynolds</author>
<author>Marc A Zissman</author>
</authors>
<title>Measuring the readability of automatic speechto-text transcripts.</title>
<date>2003</date>
<booktitle>In INTERSPEECH. ISCA.</booktitle>
<contexts>
<context position="1191" citStr="Jones et al., 2003" startWordPosition="163" endWordPosition="166">f-the-art disfluency detectors. The joint model performed better on both tasks, with a parse accuracy of 90.5% and 84.0% accuracy at disfluency detection. The model runs in expected linear time, and processes over 550 tokens a second. 1 Introduction Most unscripted speech contains filled pauses (ums and uhs), and errors that are usually edited on-the-fly by the speaker. Disfluency detection is the task of detecting these infelicities in spoken language transcripts. The task has some immediate value, as disfluencies have been shown to make speech recognition output much more difficult to read (Jones et al., 2003), but has also been motivated as a module in a natural language understanding pipeline, because disfluencies have proven problematic for PCFG parsing models. Instead of a pipeline approach, we build on recent work in transition-based dependency parsing, to perform the two tasks jointly. There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compa</context>
</contexts>
<marker>Jones, Wolf, Gibson, Williams, Fedorenko, Reynolds, Zissman, 2003</marker>
<rawString>Douglas A. Jones, Florian Wolf, Edward Gibson, Elliott Williams, Evelina Fedorenko, Douglas A. Reynolds, and Marc A. Zissman. 2003. Measuring the readability of automatic speechto-text transcripts. In INTERSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrik Jorgensen</author>
</authors>
<title>The effects of disfluency detection in parsing spoken language.</title>
<date>2007</date>
<booktitle>In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek, and Mare Koit, editors, Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007,</booktitle>
<pages>240--244</pages>
<contexts>
<context position="1680" citStr="Jorgensen, 2007" startWordPosition="239" endWordPosition="240">iate value, as disfluencies have been shown to make speech recognition output much more difficult to read (Jones et al., 2003), but has also been motivated as a module in a natural language understanding pipeline, because disfluencies have proven problematic for PCFG parsing models. Instead of a pipeline approach, we build on recent work in transition-based dependency parsing, to perform the two tasks jointly. There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components. Our parser largely follows the design of Zhang and Clark (2011). We use a structured averaged perceptron model with beamsearch decoding (Collins, 2002). Our feature set is based on Zhang and Clark (2011), and our transition-system is based on the arc-eager system of Nivre (2003). We extend the transition system with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be par</context>
</contexts>
<marker>Jorgensen, 2007</marker>
<rawString>Fredrik Jorgensen. 2007. The effects of disfluency detection in parsing spoken language. In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek, and Mare Koit, editors, Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007, pages 240–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="23105" citStr="Koo and Collins (2010)" startWordPosition="3859" endWordPosition="3862">label combinations. The additional templates combine the POS tag of S0 with two or three dependency labels from its left and right subtrees. Details can be found in the supplementary material. 5.1 Brown Cluster Features The Brown clustering algorithm (Brown et al., 1992) is a well known source of semi-supervised features. The clustering algorithm is run over a large sample of unlabelled data, to generate a type-to-cluster map. This mapping is then used to generate features that sometimes generalise better than lexical features, and are helpful for out-ofvocabulary words (Turian et al., 2010). Koo and Collins (2010) found that Brown cluster features greatly improved the performance of a graph-based dependency parser. On our transitionbased parser, Brown cluster features bring a small but statistically significant improvement on the WSJ task (0.1-0.3% UAS). Other developers of transition-based parsers seem to have found similar results (personal communication). Since a Brown cluster mapping computed by Liang (2005) is easily available,3 the features are simple to implement and cheap to compute. Our templates follow Koo and Collins (2010) in including features that refer to cluster prefix strings, as well </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1– 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="23511" citStr="Liang (2005)" startWordPosition="3920" endWordPosition="3921">o-cluster map. This mapping is then used to generate features that sometimes generalise better than lexical features, and are helpful for out-ofvocabulary words (Turian et al., 2010). Koo and Collins (2010) found that Brown cluster features greatly improved the performance of a graph-based dependency parser. On our transitionbased parser, Brown cluster features bring a small but statistically significant improvement on the WSJ task (0.1-0.3% UAS). Other developers of transition-based parsers seem to have found similar results (personal communication). Since a Brown cluster mapping computed by Liang (2005) is easily available,3 the features are simple to implement and cheap to compute. Our templates follow Koo and Collins (2010) in including features that refer to cluster prefix strings, as well as the full clusters. We adapt their templates to transition-based parsing by replacing ‘head’ with ‘item on top of the stack’ and ‘child’ with ‘first word of the buffer’. The exact templates can be found in the supplementary material. The Brown cluster features are used in our ‘baseline’ parser, and in the parsers we use as part of our pipeline systems. They improved development set accuracy by 0.4%. W</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97linguistics?</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part I, CICLing’11,</booktitle>
<pages>171--189</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="28836" citStr="Manning (2011)" startWordPosition="4841" endWordPosition="4842">g We adopt the standard strategy of using a POS tagger as a pre-process before parsing. Most transition-based parsers use a structured averaged perceptron model with beam-search for tagging, as this model achieves competitive accuracy and matches the standard dependency parsing architecture. Our tagger also uses this architecture. We performed some additional feature engineering for the tagger, in order to improve its accuracy given the lack of case distinctions and punctuation in the data. Our additional features use two sources of unsupervised information. First, we follow the suggestion of Manning (2011) by using Brown cluster features to improve the tagger’s accuracy on unknown words. Second, we compensate for the lack of case distinctions by including features that ask what percentage of the time a word form was seen title-cased, upper-cased and lower-cased in the Google Web1T corpus. Where most previous work uses cross-fold training for the tagger, to ensure that the parser is trained on tags that reflect run-time accuracies, we do online training of the tagger alongside the parser, using the current tagger model to produce tags during parser training. This had no impact on parse accuracy,</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97linguistics? In Proceedings of the 12th international conference on Computational linguistics and intelligent text processing - Volume Part I, CICLing’11, pages 171–189. Springer-Verlag, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4446" citStr="Marcus et al., 1993" startWordPosition="662" endWordPosition="665">inguistics. A flight to um ���� FP Figure 1: A sentence with disfluencies annotated in the style of Shriberg (1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous work in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. • several novel feature classes, • direct comparison against the two best disfluency pre-processors, and • state-of-the-art accuracy for both speech parsing and disfluency detection. 2 Switchboard Disfluency Annotations The Switchboard portion of the Penn Treebank (Marcus et al., 1993) consists of telephone conversations between strangers about an assigned topic. Two annotation layers are provided: one for syntactic bracketing (MRG files), and one for disfluencies (DPS files). The disfluency layer marks elements with little or no syntactic function, such as filled pauses and discourse markers, and annotates speech repairs using the Shriberg (1994) system of reparandum/interregnum/repair. An example is shown in Figure 1. In the syntactic annotation, edited words are covered by a special node labelled EDITED. The idea is to mark text which, if excised, would result in a gramm</context>
<context position="29900" citStr="Marcus et al., 1993" startWordPosition="5017" endWordPosition="5020">ne training of the tagger alongside the parser, using the current tagger model to produce tags during parser training. This had no impact on parse accuracy, and made it slightly easier to develop our tagger alongside the parser. The tagger achieved 96.5% accuracy on the development data, but when we ran our final test experiments, we found its accuracy dropped to 96.0%, indicating some over-fitting during our feature engineering. On the development data, our parser accuracy improves by about 1% when goldstandard tags are used. 7 Experiments We use the Switchboard portion of the Penn Treebank (Marcus et al., 1993), as described in Section 2, to train our joint models and evaluate them on dependency parsing and disfluency detection. The pre-processing and dependency conversion are described in Section 2.1. We use the standard train/dev/test split from Charniak and Johnson (2001): Sections 2 and 3 for training, and Section 4 divided into three held-out sections, the first of which is used for final evaluation. Our parser evaluation uses the SPARSEVAL (Roark et al., 2006) metric. However, we wanted to use the Stanford dependency converter, for the reasons described in Section 2.1, so we used our own imple</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Michell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="2141" citStr="Nivre (2003)" startWordPosition="312" endWordPosition="313">rsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components. Our parser largely follows the design of Zhang and Clark (2011). We use a structured averaged perceptron model with beamsearch decoding (Collins, 2002). Our feature set is based on Zhang and Clark (2011), and our transition-system is based on the arc-eager system of Nivre (2003). We extend the transition system with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be parsed incrementally, without the need to guess early that pepper is disfluent. This is achieved by reprocessing the leftward children of the word Edit marks as disfluent. For instance, if the parser attaches the to pepper, but subsequently marks pepper as disfluent, the will be returned to the stack. We also exploit the ease with which the model can incorporate arbitrary features, and design a set of features that capture the ‘rough copy’ structure of some sp</context>
<context position="9840" citStr="Nivre, 2003" startWordPosition="1548" endWordPosition="1549">fly, the transition-based parser consists of a configuration (or ‘state’) which is sequentially manipulated by a set of possible transitions. For us, a state is a 4-tuple c = (v, β, A, D), where v and β are disjoint sets of word indices termed the stack and buffer respectively, A is the set of dependency arcs, and D is the set of word indices marked disfluent. There are no arcs to or from members of D, so the dependencies and disfluencies can be implemented as a single vector (in our parser, a token is marked as disfluent by setting it as its own head). We use the arc-eager transition system (Nivre, 2003, 2008), which consists of four parsing actions: Shift, Left-Arc, Right-Arc and Reduce. We denote the stack with its topmost element to the right, and the buffer with its first element to the left. A vertical bar is used to indicate concatenation to the stack or buffer, e.g. v|i indicates a stack with the topmost element i and remaining elements v. A dependency from a governor i to a child j is denoted i → j. The four arc-eager transitions are shown in Figure 2. The Shift action moves the first item of the buffer onto the stack. The Right-Arc does the same, but also adds an arc, so that the to</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--513</pages>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34:513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Disfluency detection using multi-step stacked learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>820--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Atlanta,</location>
<contexts>
<context position="6252" citStr="Qian and Liu, 2013" startWordPosition="956" endWordPosition="959">layers, 33,720 tokens are marked as disfluent in at least one layer, 32,310 are only marked as disfluent by the DPS files, and 32,742 are only marked as disfluent by the MRG layer. The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluencydetection training data have gold-standard syntactic parses. Our system requires the more expensive syntactic annotation, but we find that it outperforms the previous state-of-the-art (Qian and Liu, 2013), despite training on less than half the data. 2.1 Dependency Conversion As is standard in statistical dependency parsing of English, we acquire our gold-standard dependencies from phrase-structure trees. We used the 2013-04-05 version of the Stanford dependency converter (de Marneffe et al., 2006). As is standard for English dependency parsing, we use the Basic Dependencies scheme, which produces strictly projective representations. At first we feared that the filled pauses, disfluencies and meta-data tokens in the Switchboard corpus might disrupt the conversion process, by making it more dif</context>
<context position="33995" citStr="Qian and Liu (2013)" startWordPosition="5684" endWordPosition="5687">urnal, but there are no dependency parsing results in the literature that make it easy to put our joint model’s parsing accuracy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard t</context>
<context position="36515" citStr="Qian and Liu (2013)" startWordPosition="6095" endWordPosition="6098">hnson et al pipeline 82.1 90.3 Qian and Liu pipeline 83.9 90.1 Baseline joint parser 73.9 89.4 Final joint parser 84.1 90.5 Table 2: Test-set parse and disfluency accuracies. The joint parser is improved by the features and Edit transition, and is better than pre-processing the text with state-of-the-art disfluency detectors. Table 2 shows the final evaluation. Our main comparison is with the two pipeline systems, described in Section 7.1. The Johnson and Charniak (2004) system was 1.8% less accurate at disfluency detection than the other disfluency detector we evaluated, the state-of-the-art Qian and Liu (2013) system. However, when we evaluated the two systems as pre-processors before our parser, we found that the Johnson et al pipeline achieved 0.2% better unlabelled attachment score than the Qian and Liu pipeline. We attribute this to the use of the Charniak and Johnson (2001) syntactic language model in the Johnson et al pipeline, which would help the system produce more syntactically consistent output. Our joint model achieved an unlabelled attachment score of 90.5%, out-performing both pipeline systems. The Baseline joint parser, which did not include the Edit transition or disfluency features</context>
<context position="38268" citStr="Qian and Liu (2013)" startWordPosition="6374" endWordPosition="6377">s refer to an updated version of the system that corrects minor pre-processing problems. We thank Qian Xian for making his code available. tation, for which there is less data). Our significance testing regime involved using 20 different random seeds when training each of our models, which the perceptron algorithm is sensitive to. This could not be applied to the other two disfluency detectors, so we cannot test those differences for significance. However, we note that the 20 samples for our disfluency detector ranged in accuracy from 83.3-84.6, so we doubt that 0.2% mean improvement over the Qian and Liu (2013) result is meaningful. Although we did not systematically optimise on the development set, our test scores are lower than our development accuracies. Much of the over-fitting seems to be in the POS tagger, which dropped in accuracy by 0.5%. 9 Analysis of Edit Behaviour In order to understand how the parser applies the Edit transition, we collected some additional statistics over the development data. The parser predicted 2,558 Edit transitions, which together marked 2,706 words disfluent (2,495 correctly). The Edit transition can mark multiple words disfluent when S0 has one or more rightward </context>
<context position="42957" citStr="Qian and Liu (2013)" startWordPosition="7159" endWordPosition="7162">e Rasooli and Tetreault system offers a token-based view of disfluencies, which is useful for examples such as, and the and the, which would require two applications of our transition. On the other hand, our Edit transition may have the advantage for more syntactically complicated examples, particularly for disfluent verbs. The importance of syntactic features for disfluency detection was demonstrated by Johnson and Charniak (2004). Despite this, most subsequent work has used sequence models, rather than syntactic parsers. The other disfluency system that we compare our model to, developed by Qian and Liu (2013), uses a cascade of Maximum Margin Markov Models to perform disfluency detection with minimal syntactic information. One motivation for sequential approaches is that most applications of these models will be over unsegmented text, as segmenting unpunctuated text is a difficult task that benefits from syntactic features (Zhang et al., 2013). We consider the most promising aspect of our system to be that it is naturally incremental, so it should be straightforward to extend the system to operate on unsegmented text in subsequent work. Due to its use of syntactic features, from the joint model, t</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 820–825. Association for Computational Linguistics, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Joel Tetreault</author>
</authors>
<title>Joint parsing and disfluency detection in linear time.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>124--129</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1710" citStr="Rasooli and Tetreault, 2013" startWordPosition="241" endWordPosition="244">sfluencies have been shown to make speech recognition output much more difficult to read (Jones et al., 2003), but has also been motivated as a module in a natural language understanding pipeline, because disfluencies have proven problematic for PCFG parsing models. Instead of a pipeline approach, we build on recent work in transition-based dependency parsing, to perform the two tasks jointly. There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components. Our parser largely follows the design of Zhang and Clark (2011). We use a structured averaged perceptron model with beamsearch decoding (Collins, 2002). Our feature set is based on Zhang and Clark (2011), and our transition-system is based on the arc-eager system of Nivre (2003). We extend the transition system with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be parsed incrementally, without the</context>
<context position="41134" citStr="Rasooli and Tetreault (2013)" startWordPosition="6867" endWordPosition="6870">, and 584 were not. The parser was also quite accurate in its decisions over these tokens. Of the 547 tokens marked disfluent, 500 were correct — similar to the overall development set precision, 92.2%. Accuracy over the words returned to the stack might be improved in future by features referring to their former heads. For instance, in He went broke uh became bankrupt, we do not currently have features that record the deleted dependency became he and went. We thank one of the anonymous reviewers for this suggestion. 10 Related Work The most similar system to ours was published very recently. Rasooli and Tetreault (2013) describe a joint model of dependency parsing and disfluency detection. They introduce a second classification step, where they first decide whether to apply a disfluency transition, or a regular parsing move. Disfluency transitions operate either over a sequence of words before the start of the buffer, or a sequence of words from the start of the buffer forward. Instead of the dynamic oracle training method that we employ, they use a twostage bootstrap-style process. Direct comparison between our model and theirs is difficult, as they use the Penn2MALT scheme, and their parser uses greedy dec</context>
</contexts>
<marker>Rasooli, Tetreault, 2013</marker>
<rawString>Mohammad Sadegh Rasooli and Joel Tetreault. 2013. Joint parsing and disfluency detection in linear time. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 124–129. Association for Computational Linguistics, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Brian Roark</author>
<author>Mary Harper</author>
<author>Eugene Charniak</author>
<author>Bonnie Dorr</author>
<author>Mark Johnson</author>
<author>Jeremy Kahn</author>
<author>Yang Liu</author>
<author>Mary Ostendorf</author>
<author>John Hale</author>
<author>Anna Krasnyanskaya</author>
<author>Matthew Lease</author>
<author>Izhak Shafran</author>
<author>Matthew Snover</author>
<author>Robin Stewart</author>
<author>LisaYung</author>
</authors>
<title>Sparseval: Evaluation metrics for parsing speech.</title>
<date>2006</date>
<booktitle>In Proceedings of Language Resource and Evaluation Conference,</booktitle>
<pages>pages</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="30364" citStr="Roark et al., 2006" startWordPosition="5094" endWordPosition="5097">rser accuracy improves by about 1% when goldstandard tags are used. 7 Experiments We use the Switchboard portion of the Penn Treebank (Marcus et al., 1993), as described in Section 2, to train our joint models and evaluate them on dependency parsing and disfluency detection. The pre-processing and dependency conversion are described in Section 2.1. We use the standard train/dev/test split from Charniak and Johnson (2001): Sections 2 and 3 for training, and Section 4 divided into three held-out sections, the first of which is used for final evaluation. Our parser evaluation uses the SPARSEVAL (Roark et al., 2006) metric. However, we wanted to use the Stanford dependency converter, for the reasons described in Section 2.1, so we used our own implementation. Because we do not need to deal with recognition errors, we do not need to report our parsing results using P/R/F-measures. Instead, we report an unlabelled accuracy score, which refers to the percentage of fluent words whose governors were assigned correctly. Note that words marked as disfluent cannot have any incoming or out-going dependencies, so if a word is 137 incorrectly marked as disfluent, all of its dependencies will be incorrect. We follow</context>
</contexts>
<marker>Roark, Harper, Charniak, Dorr, Johnson, Kahn, Liu, Ostendorf, Hale, Krasnyanskaya, Lease, Shafran, Snover, Stewart, LisaYung, 2006</marker>
<rawString>Brian Roark, Mary Harper, Eugene Charniak, Bonnie Dorr, Mark Johnson, Jeremy Kahn, Yang Liu, Mary Ostendorf, John Hale, Anna Krasnyanskaya, Matthew Lease, Izhak Shafran, Matthew Snover, Robin Stewart, and LisaYung. 2006. Sparseval: Evaluation metrics for parsing speech. In Proceedings of Language Resource and Evaluation Conference, pages 333– 338. European Language Resources Association (ELRA), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Sartorio</author>
<author>Giorgio Satta</author>
<author>Joakim Nivre</author>
</authors>
<title>A transition-based dependency parser using a dynamic parsing strategy.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>135--144</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="17216" citStr="Sartorio et al., 2013" startWordPosition="2876" endWordPosition="2879">d at the end of the sentence, following the suggestion of Ballesteros and Nivre (2013). The final action will be a Left-Arc from the ROOT token to went. 4.2 Dynamic Oracle Training Algorithm Our non-monotonic transition system introduces substantial spurious ambiguity: the gold-standard parse can be derived via many different transition 11. S His company went broke i mean went bankrupt 12. R His company went broke i mean went bankrupt 13. D His company went .broW i mean went bankrupt 12. R His company svenr .broke i mean went 134 sequences. Recent work has shown that this can be advantageous (Sartorio et al., 2013; Honnibal et al., 2013; Goldberg and Nivre, 2012), because difficult decisions can sometimes be delayed until more information is available. Line 5 of Figure 4 shows a state that introduces spurious ambiguity. From this configuration, there are multiple actions that could be considered ‘correct’, in the sense that the gold-standard analysis can be derived from them. The Edit transition is correct because went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequen</context>
</contexts>
<marker>Sartorio, Satta, Nivre, 2013</marker>
<rawString>Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using a dynamic parsing strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 135–144. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="3941" citStr="Shriberg (1994)" startWordPosition="593" endWordPosition="594">omising. Because the parser is incremental, it should be well suited to unsegmented text such as the output of a speechrecognition system. We consider our main contributions to be: • a novel non-monotonic transition system, for speech repairs and restarts, 1http://github.com/syllog1sm/redshift 131 Transactions of the Association for Computational Linguistics, 2 (2014) 131–142. Action Editor: Joakim Nivre. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. A flight to um ���� FP Figure 1: A sentence with disfluencies annotated in the style of Shriberg (1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous work in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. • several novel feature classes, • direct comparison against the two best disfluency pre-processors, and • state-of-the-art accuracy for both speech parsing and disfluency detection. 2 Switchboard Disfluency Annotations The Switchboard portion of the Penn Treebank (Marcus et al., 1993) consists of telephone conversations between strangers about an assigned topic. Two annotation </context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disfluencies. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Latent variable perceptron algorithm for structured classification. In</title>
<date>2009</date>
<booktitle>IJCAI,</booktitle>
<pages>1236--1242</pages>
<contexts>
<context position="18989" citStr="Sun et al. (2009)" startWordPosition="3171" endWordPosition="3174">en referred to as a structured perceptron, or sometimes a global perceptron. During training, if the model does not predict the correct sequence, an update is performed, based on the gold-standard sequence and part of the sequence predicted by the current weights. Only part of the sequence is used to calculate the weight update, in order to account for search errors. We use the maximum violation strategy described by Huang et al. (2012) to select the subsequence to update from. To train our model using the dynamic oracle, we use the latent-variable structured perceptron algorithm described by Sun et al. (2009). Beamsearch is performed to find the highest-scoring gold-standard sequence, as well as the highestscoring prediction. We use the same beam-width for both search procedures. 4.3 Path Length Normalisation One problem introduced by the Edit transition is that the number of actions applied to a sentence is 2 The training data is partially labelled in the sense that instances can have multiple true labels. Equivalently, one might say that the transitions are latent variables, which generate the dependencies. no longer constant — it is no longer guaranteed to be 2n − 1, for a sentence of length n.</context>
</contexts>
<marker>Sun, Matsuzaki, Okanohara, Tsujii, 2009</marker>
<rawString>Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and Jun’ichi Tsujii. 2009. Latent variable perceptron algorithm for structured classification. In IJCAI, pages 1236–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<contexts>
<context position="23081" citStr="Turian et al., 2010" startWordPosition="3855" endWordPosition="3858">rammatical dependency label combinations. The additional templates combine the POS tag of S0 with two or three dependency labels from its left and right subtrees. Details can be found in the supplementary material. 5.1 Brown Cluster Features The Brown clustering algorithm (Brown et al., 1992) is a well known source of semi-supervised features. The clustering algorithm is run over a large sample of unlabelled data, to generate a type-to-cluster map. This mapping is then used to generate features that sometimes generalise better than lexical features, and are helpful for out-ofvocabulary words (Turian et al., 2010). Koo and Collins (2010) found that Brown cluster features greatly improved the performance of a graph-based dependency parser. On our transitionbased parser, Brown cluster features bring a small but statistically significant improvement on the WSJ task (0.1-0.3% UAS). Other developers of transition-based parsers seem to have found similar results (personal communication). Since a Brown cluster mapping computed by Liang (2005) is easily available,3 the features are simple to implement and cheap to compute. Our templates follow Koo and Collins (2010) in including features that refer to cluster </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Shuangzhi Wu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
</authors>
<title>Punctuation prediction with transition-based parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>752--760</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="43298" citStr="Zhang et al., 2013" startWordPosition="7211" endWordPosition="7214">of syntactic features for disfluency detection was demonstrated by Johnson and Charniak (2004). Despite this, most subsequent work has used sequence models, rather than syntactic parsers. The other disfluency system that we compare our model to, developed by Qian and Liu (2013), uses a cascade of Maximum Margin Markov Models to perform disfluency detection with minimal syntactic information. One motivation for sequential approaches is that most applications of these models will be over unsegmented text, as segmenting unpunctuated text is a difficult task that benefits from syntactic features (Zhang et al., 2013). We consider the most promising aspect of our system to be that it is naturally incremental, so it should be straightforward to extend the system to operate on unsegmented text in subsequent work. Due to its use of syntactic features, from the joint model, the system is substantially more accurate than the previous state-of-the-art in incremental disfluency detection, 77% (Zwarts et al., 2010). 11 Conclusion We have presented an efficient and accurate joint model of dependency parsing and disfluency detection. The model out-performs pipeline approaches using state-of-the-art disfluency detect</context>
</contexts>
<marker>Zhang, Wu, Yang, Li, 2013</marker>
<rawString>Dongdong Zhang, Shuangzhi Wu, Nan Yang, and Mu Li. 2013. Punctuation prediction with transition-based parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 752–760. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1925" citStr="Zhang and Clark (2011)" startWordPosition="275" endWordPosition="278">e proven problematic for PCFG parsing models. Instead of a pipeline approach, we build on recent work in transition-based dependency parsing, to perform the two tasks jointly. There have been two small studies of dependency parsing on unscripted speech, both using entirely greedy parsing strategies, without a direct comparison against a pipeline architecture (Jorgensen, 2007; Rasooli and Tetreault, 2013). We go substantially beyond these pilot studies, and present a system that compares favourably to a pipeline consisting of stateof-the-art components. Our parser largely follows the design of Zhang and Clark (2011). We use a structured averaged perceptron model with beamsearch decoding (Collins, 2002). Our feature set is based on Zhang and Clark (2011), and our transition-system is based on the arc-eager system of Nivre (2003). We extend the transition system with a novel non-monotonic transition, Edit. It allows sentences like ‘Pass the pepper uh salt’ to be parsed incrementally, without the need to guess early that pepper is disfluent. This is achieved by reprocessing the leftward children of the word Edit marks as disfluent. For instance, if the parser attaches the to pepper, but subsequently marks p</context>
<context position="8738" citStr="Zhang and Clark (2011)" startWordPosition="1357" endWordPosition="1360">n have a dependency between the two tokens over 99.9% of the times they occur in the treebank, with you and I never having any children. This makes it easy to unmerge the tokens deterministically after pars� Boston � �� � � Denver Tuesday Y I mean Y RM IM RP 132 ing: all incoming and outgoing arcs will point to know or mean. The same pre-processing was performed for all our parsing systems. 3 Transition-based Dependency Parsing A transition-based parser predicts the syntactic structure of a sentence incrementally, by making a sequence of classification decisions. We follow the architecture of Zhang and Clark (2011), who use beam-search for decoding, and a structured averaged perceptron for training. Despite its simplicity, this type of parser has produced highly competitive results on the Wall Street Journal: with the extended feature set described by Zhang and Nivre (2011), it achieves 93.5% unlabelled accuracy on Stanford basic dependencies (de Marneffe et al., 2006). Converting the constituency trees produced by the Charniak and Johnson (2005) reranking parser results in similar accuracy. Briefly, the transition-based parser consists of a configuration (or ‘state’) which is sequentially manipulated b</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<location>Portland, USA.</location>
<contexts>
<context position="9002" citStr="Zhang and Nivre (2011)" startWordPosition="1400" endWordPosition="1403">ng: all incoming and outgoing arcs will point to know or mean. The same pre-processing was performed for all our parsing systems. 3 Transition-based Dependency Parsing A transition-based parser predicts the syntactic structure of a sentence incrementally, by making a sequence of classification decisions. We follow the architecture of Zhang and Clark (2011), who use beam-search for decoding, and a structured averaged perceptron for training. Despite its simplicity, this type of parser has produced highly competitive results on the Wall Street Journal: with the extended feature set described by Zhang and Nivre (2011), it achieves 93.5% unlabelled accuracy on Stanford basic dependencies (de Marneffe et al., 2006). Converting the constituency trees produced by the Charniak and Johnson (2005) reranking parser results in similar accuracy. Briefly, the transition-based parser consists of a configuration (or ‘state’) which is sequentially manipulated by a set of possible transitions. For us, a state is a 4-tuple c = (v, β, A, D), where v and β are disjoint sets of word indices termed the stack and buffer respectively, A is the set of dependency arcs, and D is the set of word indices marked disfluent. There are </context>
<context position="21243" citStr="Zhang and Nivre (2011)" startWordPosition="3547" endWordPosition="3550"> new figure-of-merit is the arithmetic mean of the candidate’s transition scores, where previously the figure-of-merit was the sum of the candidate’s transition scores. Interestingly, Zhu et al. (2013) report that they tried exactly this, and that it was less effective than their solution. We found that the features associated with the IDLE transition were uninformative (the state is at termination, so the stack and buffer are empty), and had nothing to do with how many edit transitions were earlier applied. 5 Features for the Joint Parser Our baseline parser uses the feature set described by Zhang and Nivre (2011). The feature set contains 73 templates that mostly refer to the properties of 12 context tokens: the top of the stack (S0), its two leftmost and rightmost children (S0L, S0L2, S0R, S0R2), its parent and grand-parent (S0h, S0h2), the first word of the buffer and its two leftmost children (N0, N0L, N0LL), and the next two words of the buffer (N1, N2). Atomic features consist of the word, part-ofspeech tag, or dependency label for these tokens; and multiple feature atoms are often combined for feature templates. There are also features for the string-distance between S0 and N0, and the left and </context>
<context position="25923" citStr="Zhang and Nivre (2011)" startWordPosition="4339" endWordPosition="4342">are, with red on the stack and square at N0, its value would be 1. 2. How long is the prefix POS tag match between S0le...S0 and N0le...N0? 3. Do the words in S0le...S0 and N0le...N0 match exactly? 4. Do the POS tags in S0le...S0 and N0le...N0 match exactly? If the parser were analysing the red square the blue rectangle, with square on the stack and rectangle at N0, its value would be true. The prefix-length features are binned using the function Ax : min(x, 5). 5.3 Match Features This class of features ask which pairs of the context tokens match, in word or POS tag. The context tokens in the Zhang and Nivre (2011) feature set are the top of the stack (S0), its head and 4As is common in this type of parser, our implementation has a number of vectors for properties that are defined before parsing, such as word forms, POS tags, Brown clusters, etc. A context token is an index into these vectors, allowing features considering these properties to be computed. 136 grandparent (S0h, S0h2), its two left- and rightmost children (S0L, S0L2, S0R, S0R2), the first three words of the buffer (N0, N1, N2), and the two leftmost children of N0 (N0L, N0LL). We extend this set with the S0l,, S0T, and N0l, tokens describe</context>
<context position="32939" citStr="Zhang and Nivre (2011)" startWordPosition="5522" endWordPosition="5525">t 0.1%), at the cost of 50% slower run-time. Wider beams than this brought no accuracy improvement. Accuracy seems to plateau with slightly narrower beams than on newswire text. This is probably due to the shorter sentences in Switchboard. The baseline and pipeline systems are configured in the same way, except that the baseline parser is modified slightly to allow it to predict disfluencies, using a special dependency label, ERASED. All descendants of a word attached to its head by this label are marked as disfluent. Both the baseline and pipeline/oracle parsers use the same feature set: the Zhang and Nivre (2011) features, plus our Brown cluster features. The baseline system is a standard arc-eager transition-based parser with a structured averaged perceptron model and beam-search decoding. The model is trained in the standard way, with a ‘static’ oracle and maximum-violation update, following (Huang et al., 2012). 7.1 Comparison with Pipeline Approaches The accuracy of incremental dependency parsers is well established on the Wall Street Journal, but there are no dependency parsing results in the literature that make it easy to put our joint model’s parsing accuracy into context. We therefore compare</context>
<context position="34747" citStr="Zhang and Nivre (2011)" startWordPosition="5798" endWordPosition="5801"> language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-errors on the parser. The baseline parser, which uses the Zhang and Nivre (2011) feature set plus the Brown cluster features, scores 1.8% UAS lower than the oracle. When we add the features described in Sections 5.2, 5.3 and 5.4, the gap is reduced to 1.2% (+Features). Finally, the improved transition system reduces the gap further still, to 0.8% UAS (+Edit transition). We also tested these features in the Oracle parser, but found they were ineffective on fluent text. The w/s column shows the tokens analysed per second for each system, including disfluencies, with a single thread on a 2.4GHz Intel Xeon. The additional features reduce efficiency, but the nonmonotonic Edit </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transitionbased dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193. Association for Computational Linguistics, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shift-reduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>434--443</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="20252" citStr="Zhu et al. (2013)" startWordPosition="3384" endWordPosition="3387"> with leftward children, those children are returned to the stack, and processed again. This has little to no impact on the algorithm’s empirical efficiency, although worst-case complexity is no longer linear, but it does pose a problem for decoding. The perceptron model tends to assign large positive scores to its top prediction. We thus observed a problem when comparing paths of different lengths, at the end of the sentence. Paths that included Edit transitions were longer, so the sum of their scores tended to be higher. The same problem has been observed during incremental PCFG parsing, by Zhu et al. (2013). They introduce an additional transition, IDLE, to ensure that paths are the same length. So long as one candidate in the beam is still being processed, all other candidates apply the IDLE transition. We adopt a simpler solution. We normalise the figure-of-merit for a candidate state, which is used to rank it in the beam, by the length of its transition history. The new figure-of-merit is the arithmetic mean of the candidate’s transition scores, where previously the figure-of-merit was the sum of the candidate’s transition scores. Interestingly, Zhu et al. (2013) report that they tried exactl</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>703--711</pages>
<location>Portland, USA.</location>
<contexts>
<context position="34196" citStr="Zwarts and Johnson (2011)" startWordPosition="5715" endWordPosition="5718"> systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F-measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-errors on the parser. The baseline parser, which uses the Zhang and Nivre (2011) feature set plus the Brown cluster features, sco</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 703–711. Association for Computational Linguistics, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
<author>Robert Dale</author>
</authors>
<title>Detecting speech repairs incrementally using a noisy channel approach.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1371--1378</pages>
<location>Beijing, China.</location>
<contexts>
<context position="43695" citStr="Zwarts et al., 2010" startWordPosition="7274" endWordPosition="7277">ne motivation for sequential approaches is that most applications of these models will be over unsegmented text, as segmenting unpunctuated text is a difficult task that benefits from syntactic features (Zhang et al., 2013). We consider the most promising aspect of our system to be that it is naturally incremental, so it should be straightforward to extend the system to operate on unsegmented text in subsequent work. Due to its use of syntactic features, from the joint model, the system is substantially more accurate than the previous state-of-the-art in incremental disfluency detection, 77% (Zwarts et al., 2010). 11 Conclusion We have presented an efficient and accurate joint model of dependency parsing and disfluency detection. The model out-performs pipeline approaches using state-of-the-art disfluency detectors, and is highly efficient, processing over 550 tokens a second. Because the system is incremental, it should be straight-forward to apply it to unsegmented text. The success of an incremental, non-monotonic parser at disfluent speech parsing may also be of some psycholinguistic interest. Acknowledgments The authors would like to thank the anonymous reviewers for their valuable comments. This</context>
</contexts>
<marker>Zwarts, Johnson, Dale, 2010</marker>
<rawString>Simon Zwarts, Mark Johnson, and Robert Dale. 2010. Detecting speech repairs incrementally using a noisy channel approach. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371–1378. Coling 2010 Organizing Committee, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.487447">
TREETALK: Composition and Compression of Trees for Image Descriptions
</title>
<author confidence="0.799537">
Polina Kuznetsova† Vicente Ordonez‡ Tamara L. Berg‡ Yejin Choi††
</author>
<affiliation confidence="0.803839">
† Stony Brook University $ UNC Chapel Hill ††University of Washington
</affiliation>
<address confidence="0.751998">
Stony Brook, NY Chapel Hill, NC Seattle, WA
</address>
<email confidence="0.9787465">
pkuznetsova {vicente,tlberg} yejin@cs.washington.edu
@cs.stonybrook.edu @cs.unc.edu
</email>
<sectionHeader confidence="0.99664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985080952381">
We present a new tree based approach to
composing expressive image descriptions that
makes use of naturally occuring web images
with captions. We investigate two related
tasks: image caption generalization and gen-
eration, where the former is an optional sub-
task of the latter. The high-level idea of our
approach is to harvest expressive phrases (as
tree fragments) from existing image descrip-
tions, then to compose a new description by
selectively combining the extracted (and op-
tionally pruned) tree fragments. Key algo-
rithmic components are tree composition and
compression, both integrating tree structure
with sequence structure. Our proposed system
attains significantly better performance than
previous approaches for both image caption
generalization and generation. In addition,
our work is the first to show the empirical ben-
efit of automatically generalized captions for
composing natural image descriptions.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984173913044">
The web is increasingly visual, with hundreds of bil-
lions of user contributed photographs hosted online.
A substantial portion of these images have some sort
of accompanying text, ranging from keywords, to
free text on web pages, to textual descriptions di-
rectly describing depicted image content (i.e. cap-
tions). We tap into the last kind of text, using natu-
rally occuring pairs of images with natural language
descriptions to compose expressive descriptions for
query images via tree composition and compression.
Such automatic image captioning efforts could
potentially be useful for many applications: from
automatic organization of photo collections, to facil-
itating image search with complex natural language
queries, to enhancing web accessibility for the vi-
sually impaired. On the intellectual side, by learn-
ing to describe the visual world from naturally exist-
ing web data, our study extends the domains of lan-
guage grounding to the highly expressive language
that people use in their everyday online activities.
There has been a recent spike in efforts to au-
tomatically describe visual content in natural lan-
guage (Yang et al., 2011; Kulkarni et al., 2011; Li
et al., 2011; Farhadi et al., 2010; Krishnamoorthy et
al., 2013; Elliott and Keller, 2013; Yu and Siskind,
2013; Socher et al., 2014). This reflects the long
standing understanding that encoding the complex-
ities and subtleties of image content often requires
more expressive language constructs than a set of
tags. Now that visual recognition algorithms are be-
ginning to produce reliable estimates of image con-
tent (Perronnin et al., 2012; Deng et al., 2012a; Deng
et al., 2010; Krizhevsky et al., 2012), the time seems
ripe to begin exploring higher level semantic tasks.
There have been two main complementary direc-
tions explored for automatic image captioning. The
first focuses on describing exactly those items (e.g.,
objects, attributes) that are detected by vision recog-
nition, which subsequently confines what should be
described and how (Yao et al., 2010; Kulkarni et al.,
2011; Kojima et al., 2002). Approaches in this direc-
tion could be ideal for various practical applications
such as image description for the visually impaired.
However, it is not clear whether the semantic expres-
siveness of these approaches can eventually scale up
to the casual, but highly expressive language peo-
</bodyText>
<page confidence="0.989089">
351
</page>
<note confidence="0.376485">
Transactions of the Association for Computational Linguistics, 2 (2014) 351–362. Action Editor: Hal Daume III.
Submitted 2/2014; Revised 5/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
</note>
<figure confidence="0.9947026">
Target Image
Object Ac/on Stuff Scene
A cow standing in the I noticed that this funny A bird hovering in the You can see these
water cow was staring at me grass beautiful hills only in
the countryside
</figure>
<figureCaption confidence="0.999963">
Figure 1: Harvesting phrases (as tree fragments) for the target image based on (partial) visual match.
</figureCaption>
<bodyText confidence="0.999927430769231">
ple naturally use in their online activities. In Fig-
ure 1, for example, it would be hard to compose “I
noticed that this funny cow was staring at me” or
“You can see these beautiful hills only in the coun-
tryside” in a purely bottom-up manner based on the
exact content detected. The key technical bottleneck
is that the range of describable content (i.e., objects,
attributes, actions) is ultimately confined by the set
of items that can be reliably recognized by state-of-
the-art vision techniques.
The second direction, in a complementary avenue
to the first, has explored ways to make use of the
rich spectrum of visual descriptions contributed by
online citizens (Kuznetsova et al., 2012; Feng and
Lapata, 2013; Mason, 2013; Ordonez et al., 2011).
In these approaches, the set of what can be described
can be substantially larger than the set of what can be
recognized, where the former is shaped and defined
by the data, rather than by humans. This allows the
resulting descriptions to be substantially more ex-
pressive, elaborate, and interesting than what would
be possible in a purely bottom-up manner. Our work
contributes to this second line of research.
One challenge in utilizing naturally existing mul-
timodal data, however, is the noisy semantic align-
ment between images and text (Dodge et al., 2012;
Berg et al., 2010). Therefore, we also investi-
gate a related task of image caption generalization
(Kuznetsova et al., 2013), which aims to improve
the semantic image-text alignment by removing bits
of text from existing captions that are less likely to
be transferable to other images.
The high-level idea of our system is to harvest
useful bits of text (as tree fragments) from exist-
ing image descriptions using detected visual content
similarity, and then to compose a new description
by selectively combining these extracted (and op-
tionally pruned) tree fragments. This overall idea
of composition based on extracted phrases is not
new in itself (Kuznetsova et al., 2012), however, we
make several technical and empirical contributions.
First, we propose a novel stochastic tree compo-
sition algorithm based on extracted tree fragments
that integrates both tree structure and sequence co-
hesion into structural inference. Our algorithm per-
mits a substantially higher level of linguistic expres-
siveness, flexibility, and creativity than those based
on rules or templates (Kulkarni et al., 2011; Yang et
al., 2011; Mitchell et al., 2012), while also address-
ing long-distance grammatical relations in a more
principled way than those based on hand-coded con-
straints (Kuznetsova et al., 2012).
Second, we address image caption generalization
as an optional subtask of image caption generation,
and propose a tree compression algorithm that per-
forms a light-weight parsing to search for the op-
timal set of tree branches to prune. Our work is
the first to report empirical benefits of automatically
compressed captions for image captioning.
The proposed approaches attain significantly bet-
ter performance for both image caption generaliza-
tion and generation tasks over competitive baselines
and previous approaches. Our work results in an im-
proved image caption corpus with automatic gener-
alization, which is publicly available.1
</bodyText>
<sectionHeader confidence="0.898937" genericHeader="method">
2 Harvesting Tree Fragments
</sectionHeader>
<bodyText confidence="0.999940125">
Given a query image, we retrieve images that are vi-
sually similar to the query image, then extract po-
tentially useful segments (i.e., phrases) from their
corresponding image descriptions. We then com-
pose a new image description using these retrieved
text fragments (§3). Extraction of useful phrases
is guided by both visual similarity and the syn-
tactic parse of the corresponding textual descrip-
</bodyText>
<footnote confidence="0.984097">
1http://ilp-cky.appspot.com/
</footnote>
<page confidence="0.99323">
352
</page>
<bodyText confidence="0.99947797368421">
tion. This extraction strategy, originally proposed
by Kuznetsova et al. (2012), attempts to make the
best use of linguistic regularities with respect to
objects, actions, and scenes, making it possible to
obtain richer textual descriptions than what cur-
rent state-of-the-art vision techniques can provide
in isolation. In all of our experiments we use the
captioned image corpus of Ordonez et al. (2011),
first pre-processing the corpus for relevant content
by running deformable part model object detec-
tors (Felzenszwalb et al., 2010). For our study, we
run detectors for 89 object classes set a high confi-
dence threshold for detection.
As illustrated in Figure 1, for a query image de-
tection, we extract four types of phrases (as tree
fragments). First, we retrieve relevant noun phrases
from images with visually similar object detections.
We use color, texture (Leung and Malik, 1999), and
shape (Dalal and Triggs, 2005; Lowe, 2004) based
features encoded in a histogram of vector quantized
responses to measure visual similarity. Second, we
extract verb phrases for which the corresponding
noun phrase takes the subject role. Third, from
those images with “stuff” detections, e.g.“water”,
or “sky” (typically mass nouns), we extract preposi-
tional phrases based on similarity of both visual ap-
pearance and relative spatial relationships between
detected objects and “stuff”. Finally, we use global
“scene” similarity2 to extract prepositional phrases
referring to the overall scene, e.g., “at the confer-
ence,” or “in the market”.
We perform this phrase retrieval process for each
detected object in the query image and generate one
sentence for each object. All sentences are then
combined together to produce the final description.
Optionally, we apply image caption generalization
(via compression) (§4) to all captions in the corpus
prior to the phrase extraction and composition.
</bodyText>
<sectionHeader confidence="0.987236" genericHeader="method">
3 Tree Composition
</sectionHeader>
<bodyText confidence="0.9999395">
We model tree composition as constraint optimiza-
tion. The input to our algorithm is the set of re-
trieved phrases (i.e., tree fragments), as illustrated
in §2. Let P = {p0,..., pL−11 be the set of all
phrases across the four phrase types (objects, ac-
tions, stuff and scene). We assume a mapping func-
</bodyText>
<subsectionHeader confidence="0.488826">
2L2 distance between classification score vectors (Xiao et
</subsectionHeader>
<bodyText confidence="0.999646685714285">
tion pt : [0, L) -+ T, where T is the set of phrase
types, so that the phrase type of pi is pt(i). In ad-
dition, let R be the set of PCFG production rules
and NT be the set of nonterminal symbols of the
PCFG. The goal is to find and combine a good se-
quence of phrases G, |G |G |T |= N = 4, drawn
from P, into a final sentence. More concretely, we
want to select and order a subset of phrases (at most
one phrase of each phrase type) while considering
both the parse structure and n-gram cohesion across
phrasal boundaries.
Figure 2 shows a simplified example of a com-
posed sentence with its corresponding parse struc-
ture. For brevity, the figure shows only one phrase
for each phrase type, but in actuality there would be
a set of candidate phrases for each type. Figure 3
shows the CKY-style representation of the internal
mechanics of constraint optimization for the exam-
ple composition from Figure 2. Each cell ij of the
CKY matrix corresponds to Gij, a subsequence of
G starting at position i and ending at position j. If
a cell in the CKY matrix is labeled with a nontermi-
nal symbol s, it means that the corresponding tree of
Gij has s as its root.
Although we visualize the operation using a CKY-
style representation in Figure 3, note that composi-
tion requires more complex combinatorial decisions
than CKY parsing due to two additional considera-
tions. We are: (1) selecting a subset of candidate
phrases, and (2) re-ordering the selected phrases
(hence making the problem NP-hard). Therefore,
we encode our problem using Integer Linear Pro-
gramming (ILP) (Roth and tau Yih, 2004; Clarke
and Lapata, 2008) and use the CPLEX (ILOG, Inc,
2006) solver.
</bodyText>
<subsectionHeader confidence="0.99451">
3.1 ILP Variables
</subsectionHeader>
<bodyText confidence="0.941015142857143">
Variables for Sequence Structure: Variables α en-
code phrase selection and ordering:
αik = 1 iff phrase i E P is selected (1)
for position k E [0, N)
Where k is one of the N=4 positions in a sentence.3
Additionally, we define variables for each pair of ad-
jacent phrases to capture sequence cohesion:
</bodyText>
<footnote confidence="0.8572165">
3The number of positions is equal to the number of phrase
al., 2010) types, since we select at most one from each type.
</footnote>
<page confidence="0.997832">
353
</page>
<figure confidence="0.968892393258427">
1&apos;021o=b1
re nt
h
#02 = 1
y:
k=0e
02 k=0f
PP-VP
ce.
�02
1
s g-
12 e
p
cato
�� e
22
jkpq k+1)
was 1staring
Phra
qtk+1)
at f me
030
13 i
e sVP
23
PP
33
!i0 = 1
ell par
Y ftt i
m00
9010 =n1
Aone u
cow
if k
e01
YPP
g t
11 f 0 i
o
we do
in the
previous di
g s capure
f the
countryside
e whole p
f h
�ipoud
NPe = gNP ek=1 S
Figure 2: An examplerscenario of tree composition. Only Fij
the first three phrases are chosenafor the composition.
ith th rdi f h
ih h b ilied b
h dii
in- the grass
↵ij1 =p1
N2d= 1
NP8)
f 4
atio nal
NP
5).PP eus con
VP e s
PP
l
nncSfiq
eAacow in the countryside was staringgat me in theigrass
T Lf d S
i h Mi ll
n
h0 w1 2r 2oH3
+)
i=0 (k=1 j=2
e n
as
aijk = 1Ciff αik = aj(k+l)i= 1 (2)
d
s wVariables for Tree Structure: r several cnsrain o
A � Intege
Variables β encode
on, which we omit for
a en ojc
A tind i
l
!ik As m
therparsetstructure:
8βijs = 1 iff the phrase sequenceoGij (3)
</figure>
<bodyText confidence="0.962999714285714">
maps to the nonterminal symbol s EwNT
Where i E [0, N) and j ∈ [i, N) index rows and
columns ofTtrefCKY-style matrix inCFigureo3. A cor-
responding exampledtree iseihown in Figuren2,fwhere
theaphrase sequence G02 corresponds to the cell la-
beledhwith S. We also define variables to indicate
selected PCFG rules in the resulting parse:
</bodyText>
<equation confidence="0.771123235294118">
c ndie se e o
phrse) Ths, Equati te du an
orresponenc btwn p
al symbos a the tree le io (4)
βijkr = 1 iff βijh = βikp
which nase
0.2
loit Cplex (ILG Inc
nts: To ense that
n phrse ype
e tre lefs
jk =
=β(k+1)jq = s 1
hat the numbe
quire To
,
3
</equation>
<bodyText confidence="0.999814">
Where r= h pqo∈ R and k ∈o[i, j).iIndex k � 1 (10
points to the boundary of split between tworchildren
nd aseshown incFigure 2 forfthe sequence Got
</bodyText>
<equation confidence="0.545848">
t i
</equation>
<listItem confidence="0.91515175">
ngruenc Constraits: To esue
r Varables We define ariables ↵
Auxiliary Varibles: For notational c
ch CKY ell has at most one symbol n
</listItem>
<equation confidence="0.953234333333333">
word
Y weealso include:
γijk = 1 iff �s �
(msky
βijs (5)
3.
(1
hra (1 uta
�1
2s∈NT
= jE Xβ(k+l)js — 1
SENT
</equation>
<subsectionHeader confidence="0.996461">
3.2 ILPsObjective Function
</subsectionHeader>
<bodyText confidence="0.999722">
j1 Wehmodel tree composition as maximization oftthe
following objective function: h
</bodyText>
<equation confidence="0.960111133333333">
N−1
ow
F = Fi xeE
ns whe a e αik
no
al bo of
W (6)
rd l
ij out
i k=0
+EFij e ×(
sel
ij
f +u E
ij
</equation>
<figureCaption confidence="0.958958666666667">
Figure 3: CKY-stylewrepresentation of decision variables
defined ina§3.1yfor the treezexample in Fig 2. Non-
terminal symbolsain boldfacea(inoblue) and solidtarrows
</figureCaption>
<bodyText confidence="0.951927956521739">
bine the sliecteddset of phrases.eNonterm,nal symbols in
(also in blue) represent the1chosen PCFG rules to com-
smaller fontc(in red) and dotted arrow s (alsogin red) rep
not selected.
e types of weights
Fi)represents the
phrase selection score based on visual similarity, de-
scribed in §2. Fij quantifies the sequence cohe-
sion across phrase boundaries. Forothis, we use n-
gram scores (n E [2,5]) between adjacent phrases
computed using the Google Web 1-T corpus (Brants
andoFranz., 2006). Finally, Frrquantifies PCFGIrule
scores (log probabilities) estimated fromrthet1M im-
agehcaptionlcorpuse(Ordonez et al., 2011) parsedrus-
ing the Stanfordoparser (Klein
tent selectionscore,
ere h pq 2 R
whileddFij andYFracorrespond toclinguisticefluency
Disio
Th um h
nalscores capturing sequenceeandttreeistructure respec-
tively. If we set positive values for allhof these
weights, the optimization functionbwould be beased
</bodyText>
<subsectionHeader confidence="0.630207">
s
</subsectionHeader>
<bodyText confidence="0.933429956521739">
r semantic erros. Our phase
ion are tree fragments rather than individual
tord verbos production, since selcting an addi-
words. There are three pracical benefits: (1
tactc and semantic expressiveness (2) correct
an be viewe at a high lvel as
tional phrse will increase th objctive function. To
tacic and semantc expressveness, (2) correc
and (3) computational efficiency Because w
r control forcverbosity, we set scoresycorrespondeng
totlinguistic fluency,ti.e.,oFij and Fr usingnnegative
valuesa(smaller absolute values for higherafluency),
to balancel dynamics between content selection and p
tation p b id t hih l
ble to use expre
linguistic fluency.
t i
extraction proc
vilu
grmmatcal and ma
vually-guded
y
ctio process can be
</bodyText>
<subsectionHeader confidence="0.997704">
3.3 ILP Constrains
</subsectionHeader>
<bodyText confidence="0.837198">
Also, because th
</bodyText>
<subsectionHeader confidence="0.885528">
Soundness Constraints: We need constraint to
</subsectionHeader>
<bodyText confidence="0.919818">
A, bes the t pen i e
llygrounded or visuallytuated paraphrasng
ments, the ILP formution encoded in this wo
enforce consstenc between differen types of vari-
ments, the ILP formulation encoded in this w
computationally lightweght. If the unit of co
h i f t
</bodyText>
<footnote confidence="0.519748">
4All weights are normalized using z-score.
</footnote>
<bodyText confidence="0.966749666666667">
siti wa w, te L
ition was word the ILP instan
he ILP formulation encoded in
</bodyText>
<figure confidence="0.959509043478261">
βikse For
= �
e for
s∈NT
N-2
E
ducg a c
k=0
αijklsy
e a
j−1
e E
k=i
p
lid tree st
Fr × βijkr
root of t
f th
lec � the
a va
he r
r∈R
resent possibledother choices that are
</figure>
<figureCaption confidence="0.7111995">
Thissobjective is comprised of thre
(confidenceFscores): Fi, Fij, Fr a
</figureCaption>
<bodyText confidence="0.878693142857143">
onvenience, OneacanalvieweFi as aCcon
and Manning, 2003).
ables (Equations 2, 4, 5). Constraints for a product
of two variables have been discussed by Clarke and
Lapata (2008). For Equation 2, we add the follow-
ing constraints (similar constraints are also added for
Equations 4,5).
</bodyText>
<equation confidence="0.996728">
∀ijk, αijk ≤ αik (7)
αijk ≤ αj(k+1)
αijk + (1 − αik) + (1 − αj(k+1)) ≥ 1
</equation>
<bodyText confidence="0.997350166666667">
Consistency between Tree Leafs and Sequences:
The ordering of phrases implied by αijk must be
consistent with the ordering of phrases implied by
the O variables. This can be achieved by aligning the
leaf cells (i.e., Okks) in the CKY-style matrix with α
variables as follows:
</bodyText>
<equation confidence="0.9967155">
X∀ik, αik ≤ Okks (8)
sENTi
X∀k, Xαik = Okks (9)
i sENT
</equation>
<bodyText confidence="0.9995729">
Where NTi refers to the set of PCFG nonterminals
that are compatible with a phrase type pt(i) of pi.
For example, NTi = {NN,NP, ...} if pi corresponds
to an “object” (noun-phrase). Thus, Equation 8 en-
forces the correspondence between phrase types and
nonterminal symbols at the tree leafs. Equation 9
enforces the constraint that the number of selected
phrases and instantiated tree leafs must be the same.
Tree Congruence Constraints: To ensure that
each CKY cell has at most one symbol we require
</bodyText>
<equation confidence="0.9672105">
X∀ij, Oijs ≤ 1 (10)
sENT
</equation>
<bodyText confidence="0.967336272727273">
We also require that j−1 X X Oijkr (11)
∀i,j&gt;i,h, Oijh = k=i rERh
Where Rh = {r ∈ R : r = h → pq}. We enforce
these constraints only for non-leafs. This constraint
forbids instantiations where a nonterminal symbol h
is selected for cell ij without selecting a correspond-
ing PCFG rule.
We also ensure that we produce a valid tree struc-
ture. For instance, if we select 3 phrases as shown
in Figure 3, we must have the root of the tree at the
corresponding cell 02.
</bodyText>
<equation confidence="0.5210365">
X∀kE[1,N),
sENT
</equation>
<bodyText confidence="0.9992395">
We also require cells that are not selected for the
resulting parse structure to be empty:
</bodyText>
<equation confidence="0.659652">
γijk ≤ 1 (13)
</equation>
<bodyText confidence="0.9995896">
Additionally, we penalize solutions without the S
tag at the parse root as a soft-constraint.
Miscellaneous Constraints: Finally, we include
several constraints to avoid degenerate solutions or
to otherwise enhance the composed output. We: (1)
enforce that a noun-phrase is selected (to ensure se-
mantic relevance to the image content), (2) allow at
most one phrase of each type, (3) do not allow mul-
tiple phrases with identical headwords (to avoid re-
dundancy), (4) allow at most one scene phrase for
all sentences in the description. We find that han-
dling of sentence boundaries is important if the ILP
formulation is based only on sequence structure, but
with the integration of tree-based structure, we do
not need to specifically handle sentence boundaries.
</bodyText>
<subsectionHeader confidence="0.63195">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999984294117647">
An interesting aspect of description generation ex-
plored in this paper is using tree fragments as the
building blocks of composition rather than individ-
ual words. There are three practical benefits: (1)
syntactic and semantic expressiveness, (2) correct-
ness, and (3) computational efficiency. Because we
extract phrases from human written captions, we are
able to use expressive language, and less likely to
make syntactic or semantic errors. Our phrase ex-
traction process can be viewed at a high level as
visually-grounded or visually-situated paraphrasing.
Also, because the unit of operation is tree fragments,
the ILP formulation encoded in this work is com-
putationally lightweight. If the unit of composition
was words, the ILP instances would be significantly
more computationally intensive, and more likely to
suffer from grammatical and semantic errors.
</bodyText>
<sectionHeader confidence="0.996629" genericHeader="method">
4 Tree Compression
</sectionHeader>
<bodyText confidence="0.999997">
As noted by recent studies (Mason and Charniak,
2013; Kuznetsova et al., 2013; Jamieson et al.,
2010), naturally existing image captions often in-
clude contextual information that does not directly
describe visual content, which ultimately hinders
their usefulness for describing other images. There-
fore, to improve the fidelity of the generated descrip-
tions, we explore image caption generalization as an
</bodyText>
<equation confidence="0.907735111111111">
N−1X
t=k
O0ts (12)
Okks ≤
X
sENT
X
∀ij
k
</equation>
<page confidence="0.926447">
355
</page>
<figure confidence="0.896937">
A cat
</figure>
<footnote confidence="0.6850645">
strolled along the fence
and posed for this classic profile
</footnote>
<figureCaption confidence="0.926285">
Figure 4: Compressed captions (on the left) are more ap-
plicable for describing new images (on the right).
</figureCaption>
<bodyText confidence="0.999422611111111">
optional pre-processing step. Figure 4 illustrates a
concrete example of image caption generalization in
the context of image caption generation.
We cast caption generalization as sentence com-
pression. We encode the problem as tree pruning via
lightweight CKY parsing, while also incorporating
several other considerations such as leaf-level ngram
cohesion scores and visually informed content selec-
tion. Figure 5 shows an example compression, and
Figure 6 shows the corresponding CKY matrix.
At a high level, the compression operation resem-
bles bottom-up CKY parsing, but in addition to pars-
ing, we also consider deletion of parts of the trees.
When deleting parts of the original tree, we might
need to re-parse the remainder of the tree. Note that
we consider re-parsing only with respect to the orig-
inal parse tree produced by a state-of-the-art parser,
hence it is only a light-weight parsing.5
</bodyText>
<subsectionHeader confidence="0.971077">
4.1 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.999956166666667">
Input to the algorithm is a sentence, represented as a
vector x = x0...xn−1 = x[0 : n − 1], and its PCFG
parse π(x) obtained from the Stanford parser. For
simplicity of notation, we assume that both the parse
tree and the word sequence are encoded in x. Then,
the compression can be formalized as:
</bodyText>
<footnote confidence="0.842442">
5Integrating full parsing into the original sentence would be
a straightforward extension conceptually, but may not be an em-
pirically better choice when parsing for compression is based on
vanilla unlexicalized parsing.
</footnote>
<equation confidence="0.9350078">
yˆ = arg max � Oi(x, y) (14)
Y i
Where each Oi is a potential function, corresponding
to a criteria of the desired compression:
Oi(x, y) = exp(Bi - fi(x,y)) (15)
</equation>
<bodyText confidence="0.999649">
Where Bi is the weight for a particular criteria (de-
scribed in §4.2), whose scoring function is fi.
We solve the decoding problem (Equation 14) us-
ing dynamic programming. For this, we need to
solve the compression sub-problems for sequences
x[i : j], which can be viewed as branches ˆy[i, j] of
the final tree ˆy[0 : n − 1]. For example, in Figure 5,
the final solution is ˆy[0 : 7], while a sub-solution of
x[4 : 7] corresponds to a tree branch PP. Notice
that sub-solution ˆy[3 : 7] represents the same branch
as ˆy[4 : 7] due to branch deletion. Some computed
sub-solutions, e.g., ˆy[1 : 4], get dropped from the
final compressed tree.
We define a matrix of scores D[i, j, h] (Equa-
tion 17), where h is one of the nonterminal symbols
being considered for a cell indexed by i, j, i.e. a can-
didate for the root symbol of a branch ˆy[i : j]. When
all values D[i, j, h] are computed, we take
</bodyText>
<equation confidence="0.99019">
hˆ = arg max D[0, n − 1, h] (16)
h
</equation>
<bodyText confidence="0.975950588235294">
and backtrack to reconstruct the final compression
(the exact solution to equation 14).
Where Rh = Ir E R : r = h -+ pq V r = h -+ pl.
Index k determines a split point for child branches
of a subtree ˆy[i : j]. For example, in the Figure 5 the
split point for children of the subtree ˆy[0 : 7] is k =
2. The three cases ((1) – (3)) of the above equation
correspond to the following tree pruning cases:
Pruning Case (1): None of the children of the cur-
rent node is deleted. For example, in Figures 5 and
6, the PCFG rule PP -+ IN PP, corresponding
to the sequence “in black and white”, is retained.
Another situation that can be encountered is tree re-
parsing.
Late in the day. after my sunset shot
attempts. my cat strolled along the
fence and posed for this classic profile
</bodyText>
<figure confidence="0.886498090909091">
This bridge stands
late in the day.
after my sunset shot
attempts
Generaliza)on
Late in the day cat
posed for this profile
D[i, j, h] = max I (1) D[i,k,p]+D[k+1,j,q] (17)
k ∈ [i, j) +AO[r, ij]
r ∈ Rh (2) D[i, k, p] + AO[r, ij]
(3) D[k+1,j,p]+AO[r,ij]
</figure>
<page confidence="0.89437">
356
</page>
<figureCaption confidence="0.99842725">
Figure 5: CKY compression. Both the chosen rules and
phrases (blue bold font and blue solid arrows) and not
chosen rules and phrases (red italic smaller font and red
dashed lines) are shown.
</figureCaption>
<bodyText confidence="0.999022857142857">
Pruning Case (2)/(3): Deletion of the left/right
child respectively. There are two types of deletion,
as illustrated in Figures 5 and 6. The first corre-
sponds to deletion of a child node. For example,
the second child NN of rule NP -+ NP NN is
deleted, which yields deletion of “shot”. The sec-
ond type is a special case of propagating a node
to a higher-level of the tree. In Figure 6, this sit-
uation occurs when deleting JJ “Vintage”, which
causes the propagation of NN from cell 11 to cell
01. For this purpose, we expand the set of rules R
with additional special rules of the form h -+ h,
e.g., NN -+ NN, which allows propagation of tree
nodes to higher levels of the compressed tree.6
</bodyText>
<subsectionHeader confidence="0.996223">
4.2 Modeling Compression Criteria
</subsectionHeader>
<bodyText confidence="0.999921">
The Aφ term7 in Equation 17 denotes the sum of log
of potential functions for each criteria q:
</bodyText>
<equation confidence="0.984572">
Aφ[r, ij] = � θ · Af9(r,ij) (18)
9
</equation>
<bodyText confidence="0.918995888888889">
Note that Aφ depends on the current rule r, along
with the historical information before the current
step ij, such as the original rule rij, and ngrams on
the border between left and right child branches of
rule rij. We use the following four criteria fq in our
model, which are demonstrated in Figures 5 and 6.
I. Tree Structure: We capture PCFG rule prob-
abilities estimated from the corpus as Afpcfg =
log Ppcfg(r).
</bodyText>
<footnote confidence="0.929213666666667">
6We assign probabilities of these special propagation rules
to 1 so that they will not affect the final parse tree score. Turner
and Charniak (2005) handled propagation cases similarly.
7We use Δ to distinguish the potential value for the whole
sentence from the gain of the potential during a single step of
the algorithm.
</footnote>
<figureCaption confidence="0.995444">
Figure 6: CKY compression. Both the chosen rules and
phrases (blue bold font and blue solid arrows) and not
chosen rules and phrases (red italic smaller font and red
dashed lines) are shown.
</figureCaption>
<listItem confidence="0.786866">
II. Sequence Structure: We incorporate ngram
cohesion scores only across the border between two
branches of a subtree.
III. Branch Deletion Probabilities: We compute
probabilities of deletion for children as:
</listItem>
<equation confidence="0.995711333333333">
count(rt,rij)
Afdel = log P(rt|rij) = log (19)
count (rij )
</equation>
<bodyText confidence="0.959716647058823">
Where count(rt, rij) is the frequency in which rij is
transformed to rt by deletion of one of the children.
We estimate this probability from a training corpus,
described in §4.3. count(rij) is the count of rij in
uncompressed sentences.
IV. Vision Detection (Content Selection): We
want to keep words referring to actual objects in
the image. Thus, we use V (xj), a visual similarity
score, as our confidence of an object corresponding
to word xj. This similarity is obtained from the vi-
sual recognition predictions of (Deng et al., 2012b).
Note that some test instances include rules that
we have not observed during training. We default
to the original caption in those cases. The weights
θi are set using a tuning dataset. We control over-
compression by setting the weight for fdel to a small
value relative to the other weights.
</bodyText>
<subsectionHeader confidence="0.998431">
4.3 Human Compressed Captions
</subsectionHeader>
<bodyText confidence="0.999831833333333">
Although we model image caption generalization as
sentence compression, in practical applications we
may want the outputs of these two tasks to be differ-
ent. For example, there may be differences in what
should be deleted (named entities in newswire sum-
maries could be important to keep, while they may
</bodyText>
<figure confidence="0.999748176470588">
S
VP, PP
Rule
probability
PP
(Deletion, case 2)
Dele%on
probability
NP
NP, NN
NP
(Deletion, case 1)
CC-JJ
i
00
Vintage
JJ
Dele6on
probability
Vision
Confidence
NP, NN NP S
01
11
motorcycle
NN
NN
shot
Ngram
cohesion
VBN
done
j
IN
in JJ NP
black CC CC-JJ
Rule
probability
and
VP, PP
white
PP
JJ
✓✓ NN NN VBN IN JJ CC JJ
Vintage motorcycle shot done in black and white
0 1 2 3 4 5 6 7
k=2
Vision
confidence
Ngram
cohesion
</figure>
<page confidence="0.989611">
357
</page>
<bodyText confidence="0.985585166666667">
Orig: Note the pillows, they match the
chair that goes with it, plus the table
in the picture is included.
SeqCompression: The table in the
picture.
Orig: Only in winter;me we see
these birds here in the river.
SeqCompression: See these birds
in the river.
Orig: The world&apos;s most powerful
lighthouse si@ng beside the house
with the world&apos;s thickest curtains.
SeqCompression: Si@ng beside
the house
Orig: There&apos;s something about
having 5 trucks parked in front of my
house that makes me feel all
importantClike.
</bodyText>
<figure confidence="0.878575461538462">
SeqCompression: Front of my house.
TreePruning: Trucks in front my
house.
Grammar mistakes
Orig: Orange cloud on street
light C near Lanakila Street
(phone camera).
SeqCompression: Orange street
TreePruning: Phone camera.
Relevance problem
TreePruning: The chair with the table TreePruning: These birds in the TreePruning: Powerful lighthouse
in the picture. river. beside the house with the
curtains.
</figure>
<figureCaption confidence="0.999914">
Figure 7: Caption generalization: good/bad examples.
</figureCaption>
<bodyText confidence="0.999993923076923">
be extraneous for image caption generalization). To
learn the syntactic patterns for caption generaliza-
tion, we collect a small set of example compressed
captions (380 in total) using Amazon Mechanical
Turk (AMT) (Snow et al., 2008). For each image,
we asked 3 turkers to first list all visible objects in
an image and then to write a compressed caption by
removing not visually verifiable bits of text. We then
align the original and compressed captions to mea-
sure rule deletion probabilities, excluding misalign-
ments, similar to Knight and Marcu (2000). Note
that we remove this dataset from the 1M caption cor-
pus when we perform description generation.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9346162">
We use the 1M captioned image corpus of Ordonez
et al. (2011). We reserve 1K images as a test set, and
use the rest of the corpus for phrase extraction. We
experiment with the following approaches:
Proposed Approaches:
</bodyText>
<listItem confidence="0.975665733333333">
• TREEPRUNING: Our tree compression ap-
proach as described in §4.
• SEQ+TREE: Our tree composition approach as
described in §3.
• SEQ+TREE+PRUNING: SEQ+TREE using
compressed captions of TREEPRUNING as
building blocks.
Baselines for Composition:
• SEQ+LINGRULE: The most equivalent to the
older sequence-driven system (Kuznetsova et
al., 2012). Uses a few minor enhancements,
such as sentence-boundary statistics, to im-
prove grammaticality.
• SEQ: The §3 system without tree models and
mentioned enhancements of SEQ+LINGRULE.
</listItem>
<table confidence="0.999343">
Method Bleu Meteor
w/ (w/o)
penalty P R M
SEQ+LINGRULE 0.152 (0.152) 0.13 0.17 0.095
SEQ 0.138 (0.138) 0.12 0.18 0.094
SEQ+TREE 0.149 (0.149) 0.13 0.14 0.082
SEQ+PRUNING 0.177 (0.177) 0.15 0.16 0.101
SEQ+TREE+PRUNING 0.140 (0.189) 0.16 0.12 0.088
</table>
<tableCaption confidence="0.99712">
Table 1: Automatic Evaluation
</tableCaption>
<listItem confidence="0.9974945">
• SEQ+PRUNING: SEQ using compressed cap-
tions of TREEPRUNING as building blocks.
</listItem>
<bodyText confidence="0.998652666666667">
We also experiment with the compression of human
written captions, which are used to generate image
descriptions for the new target images.
</bodyText>
<subsectionHeader confidence="0.93748">
Baselines for Compression:
</subsectionHeader>
<bodyText confidence="0.943939166666667">
• SEQCOMPRESSION (Kuznetsova et al., 2013):
Inference operates over the sequence structure.
Although optimization is subject to constraints
derived from dependency parse, parsing is not
an explicit part of the inference structure. Ex-
ample outputs are shown in Figure 7.
</bodyText>
<subsectionHeader confidence="0.997781">
5.1 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.99996">
We perform automatic evaluation using two mea-
sures widely used in machine translation: BLEU (Pa-
pineni et al., 2002)8 and METEOR (Denkowski and
Lavie, 2011).9 We remove all punctuation and con-
vert captions to lower case. We use 1K test im-
ages from the captioned image corpus,10 and as-
sume the original captions as the gold standard cap-
tions to compare against. The results in Table 1
</bodyText>
<footnote confidence="0.9646496">
8We use the unigram NIST implementation: ftp://jaguar.
ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz
9With equal weight between precision and recall in Table 1.
10Except for those for which image URLs are broken, or
CPLEX did not return a solution.
</footnote>
<page confidence="0.984844">
358
</page>
<table confidence="0.930019722222222">
Method-1
Method-2
Criteria
Method-1 preferred over Method-2 (%)
all turkers turkers w/ κ &gt; 0.55 turkers w/ κ &gt; 0.6
Image Description Generation
SEQ+TREE SEQ Rel 72 72 72
SEQ+TREE SEQ Gmar 83 83 83
SEQ+TREE SEQ All 68 69 66
SEQ+TREE+PRUNING SEQ+TREE Rel 68 72 72
SEQ+TREE+PRUNING SEQ+TREE Gmar 41 38 41
SEQ+TREE+PRUNING SEQ+TREE All 63 64 66
SEQ+TREE SEQ+LINGRULE All 62 64 62
SEQ+TREE+PRUNING SEQ+LINGRULE All 67 75 77
SEQ+TREE+PRUNING SEQ+PRUNING All 73 75 75
SEQ+TREE+PRUNING HUMAN All 24 19 19
Image Caption Generalization
TREEPRUNING SEQCOMPRESSION∗ Rel 65 65 66
</table>
<tableCaption confidence="0.9813015">
Table 2: Human Evaluation: posed as a binary question “which of the two options is better?” with respect to Relevance
(Rel), Grammar (Gmar), and Overall (All). According to Pearson’s χ2 test, all results are statistically significant.
</tableCaption>
<bodyText confidence="0.999938">
show that both the integration of the tree structure
(+TREE) and the generalization of captions using
tree compression (+PRUNING) improve the BLEU
score without brevity penalty significantly,11 while
improving METEOR only moderately (due to an im-
provement on precision with a decrease in recall.)
</bodyText>
<subsectionHeader confidence="0.999728">
5.2 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.979850222222222">
Neither BLEU nor METEOR directly measure
grammatical correctness over long distances and
may not correspond perfectly to human judgments.
Therefore, we supplement automatic evaluation with
human evaluation. For human evaluations, we
present two options generated from two compet-
ing systems, and ask turkers to choose the one that
is better with respect to: relevance, grammar, and
overall. Results are shown in Table 2 with 3 turker
ratings per image. We filter out turkers based on
a control question. We then compute the selec-
tion rate (%) of preferring method-1 over method-2.
The agreement among turkers is a frequent concern.
Therefore, we vary the set of dependable users based
on their Cohen’s kappa score (κ) against other users.
It turns out, filtering users based on κ does not make
a big difference in determining the winning method.
As expected, tree-based systems significantly out-
perform sequence-based counterparts. For example,
11While 4-gram BLEU with brevity penalty is found to cor-
relate better with human judges by recent studies (Elliott and
Keller, 2014), we found that this is not the case for our task.
This may be due to the differences in the gold standard cap-
tions. We use naturally existing ones, which include a wider
range of content and style than crowd-sourced captions.
Treepruning: The bu&amp;erflies are Seq+Tree+pruning: The bu&amp;erflies are
a&amp;racted to the colourful flowers. a&amp;racted to the colourful flowers.
</bodyText>
<subsubsectionHeader confidence="0.491228">
Cap&gt;on Generaliza&gt;on Image Descrip&gt;on Genera&gt;on
</subsubsectionHeader>
<figureCaption confidence="0.998736333333333">
Figure 8: An example of a description preferred over hu-
man gold standard. Image description is improved due to
caption generalization.
</figureCaption>
<bodyText confidence="0.998183724137931">
SEQ+TREE is strongly preferred over SEQ, with a
selection rate of 83%. Somewhat surprisingly, im-
proved grammaticality also seems to improve rele-
vance scores (72%), possibly because it is harder to
appreciate the semantic relevance of automatic cap-
tions when they are less comprehensible. Also as
expected, compositions based on pruned tree frag-
ments significantly improve relevance (68–72%),
while slightly deteriorating grammar (38–41%).
Notably, the captions generated by our system are
preferred over the original (owner generated) cap-
tions 19–24% of the time. One such example is in-
cluded in Figure 8: “The butterflies are attracted to
the colorful flowers.”
Additional examples (good and bad) are pro-
vided in Figures 9 and 10. Many of these captions
are highly expressive while remaining semantically
Orig: The bu&amp;erflies are a&amp;racted
to the colourful flowers in Hope
Gardens.
SeqCompression: The colourful
flowers.
Seq: A bu&amp;erfly to the car was spo&amp;ed by
my nine year old cousin.
Seq+pruning: The bu&amp;erflies are
a&amp;racted to the colourful flowers to the
car.
Seq+Tree: The bu&amp;erflies are a&amp;racted to
the colourful flowers in Hope Gardens.
</bodyText>
<page confidence="0.996246">
359
</page>
<bodyText confidence="0.724149458333333">
Human: Some flower on a
bar in a hotel in Grapevine,
TX.
Seq+Tree+Pruning: The
flower was so vivid and
attractive.
Human: This stained glass
window is in the porch of
Kilcash Church, Slieve na
Mon.
Seq+Tree+Pruning: This
window depicts the church.
Human: Maybe the most common
bird in the neighborhood, not just
the most common water fowl in
the neighborhood!
Seq+Tree+Pruning: The duck was
having a feast.
Seq+Tree+Pruning: Blue
flowers have no scent.
Small white flowers have
no idea what they are.
Seq+Tree+Pruning:The
tower built on each side
</bodyText>
<note confidence="0.233399333333333">
Human: Shot in Blackpool on
Tescos car park with use of a
5M candle power light.
</note>
<figure confidence="0.566936">
Seq+Tree+Pruning:Red car in
the middle of the road.
Human: Spring in a white Human: Tower bridge
dress. London in black and
white.
Highly expressive Interes0ng choice of verb phrases Poe0c Informa0ve
</figure>
<figureCaption confidence="0.99971">
Figure 9: Description generation: good examples. Description preferred over human gold standard are highlighted.
</figureCaption>
<bodyText confidence="0.994880571428572">
Human: A butterfly in a Human: A delightful clock Human: Our cat sleeping Human: The floor of the Human: In the flower bed by Human: My orange is in a very
field in the Santa Monica in the town centre of St in the cot. market area in Tirumala was the large gate, and various blue state.
mountains. Helier with the iconic Jersey Seq+Tree+Pruning:Our decorated with these rangolis. other places in the garden. Seq+Tree+Pruning: Just an
Seq+Tree+Pruning: cow at the base. cat is si=ng in the bird Seq+Tree+Pruning: In atree Seq+Tree+Pruning: Random apple in the sky.
Monarch in her bedroom Seq+Tree+Pruning: Not the feeder and actually eats ball from the ground train flowers offered to me by two
before the wedding clock face in the world. the sun flower seed. station. little girls.
ceremony.
</bodyText>
<construct confidence="0.742661">
Literally not relevant, but Grammar Seman&amp;c dissonance due Completely wrong Extraneous informa&amp;on Vision detec&amp;on error
metaphorically crea&amp;ve! problems to generaliza&amp;on error
</construct>
<figureCaption confidence="0.981038">
Figure 10: Description generation: bad examples.
</figureCaption>
<bodyText confidence="0.999766111111111">
plausible, thanks to the expressive, but somewhat
predictable descriptions online users write about
their photos. Even among the bad examples (Fig-
ure 10) one can find highly creative captions with
not literal but metaphorical relevance: “Monarch in
her bedroom before the wedding ceremony”.12 The
complete system captions and the original captions
are available at http://ilp-cky.appspot.
com/
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9758752">
Sentence Fusion Sentence fusion has been stud-
ied mostly for multi-document summarization
(Barzilay and McKeown, 2005), where redundancy
across multiple sentences serves as a guideline for
syntactic and semantic validity of generation. In
contrast, we do not have the natural redundancy to
rely upon in our task, therefore requiring the compo-
sition algorithm to be intrinsically better constrained
for correct sentence structures.
12“Monarch” can be a type of butterfly.
Sentence Compression At the core of the image
caption generalization task is sentence compression.
Much work has considered deletion-only edits like
ours (Knight and Marcu, 2000; Turner and Char-
niak, 2005; Cohn and Lapata, 2007; Filippova and
Altun, 2013), while recent ones explore more com-
plex edits, such as substitutions, insertions and re-
ordering (Cohn and Lapata, 2008). The latter gener-
ally requires a larger training corpus. We leave more
expressive compression as a future research work.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9996797">
In this paper, we have presented a novel tree com-
position approach for generating expressive image
descriptions. As an optional preprocessing step, we
also presented a tree compression approach and re-
ported the empirical benefit of using automatically
compressed captions to improve image description
generation. By integrating both the tree structure
and the sequence structure, we have significantly im-
proved the quality of composed image captions over
several competitive baselines.
</bodyText>
<page confidence="0.996297">
360
</page>
<sectionHeader confidence="0.983454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772259615384">
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–328.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and character-
ization from noisy web data. In Proceedings of
the 11th European Conference on Computer Vision:
Part I, ECCV’10, pages 663–676, Berlin, Heidelberg.
Springer-Verlag.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399–429.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of the 2007Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 73–82, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008), pages 137–144, Manchester,
UK, August. Coling 2008 Organizing Committee.
Navneet Dalal and Bill Triggs. 2005. Histograms of ori-
ented gradients for human detection. In Proceedings
of the 2005 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’05)
- Volume 1 - Volume 01, CVPR ’05, pages 886–893,
Washington, DC, USA. IEEE Computer Society.
Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li.
2010. What does classifying more than 10,000 image
categories tell us? In ECCV.
Jia Deng, Alexander C. Berg, Sanjeev Satheesh, Hao Su,
Aditya Khosla, and Fei-Fei Li. 2012a. Large scale
visual recognition challenge. In http://www.image-
net.org/challenges/LSVRC/2012/index.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012b. Hedging your bets: Optimiz-
ing accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,
Yejin Choi, Hal Daume III, Alexander C. Berg, and
Tamara L. Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 762–
772, Montr´eal, Canada, June. Association for Compu-
tational Linguistics.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations. In
EMNLP, pages 1292–1302.
Desmond Elliott and Frank Keller. 2014. Comparing
automatic evaluation measures for image description.
In ACL (2), pages 452–457.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young1, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences for images. In European Confer-
ence on Computer Vision.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object detec-
tion with discriminatively trained part based models.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 32(9):1627–1645.
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
35(4):797–812.
Katja Filippova and Yasemin Altun. 2013. Overcoming
the lack of parallel data in sentence compression. In
EMNLP, pages 1481–1491.
ILOG, Inc. 2006. ILOG CPLEX: High-performance
software for mathematical programming and optimiza-
tion. See http://www.ilog.com/products/
cplex/.
Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson,
Sven J. Dickinson, and Sven Wachsmuth. 2010. Us-
ing language to learn structured appearance models for
image annotation. IEEE Trans. Pattern Anal. Mach.
Intell., 32(1):148–164.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423–430. Association for Computa-
tional Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression. In
AAAI/IAAI, pages 703–710.
Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga.
2002. Natural language description of human activi-
ties from video images based on concept hierarchy of
actions. IJCV, 50.
</reference>
<page confidence="0.980476">
361
</page>
<reference confidence="0.999890575471698">
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond J. Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. In AAAI.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
2012. Imagenet classification with deep convolutional
neural networks. In NIPS.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. BabyTalk: Understanding and generat-
ing simple image descriptions. In Conference on Com-
puter Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2012. Collective
generation of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 359–368, Jeju Island, Korea, July. Association
for Computational Linguistics.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2013. Generaliz-
ing image captions for image-text parallel corpus. In
The 51st Annual Meeting of the Association for Com-
putational Linguistics - Short Papers, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Thomas K. Leung and Jitendra Malik. 1999. Recog-
nizing surfaces using three-dimensional textons. In
ICCV, pages 1010–1017.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, pages 220–228,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput. Vision,
60:91–110, November.
Rebecca Mason and Eugene Charniak. 2013. Annota-
tion of online shopping images without labeled train-
ing examples. In Proceedings of Workshop on Vision
and Language, Atlanta, Georgia, June. Association for
Computational Linguistics.
Rebecca Mason. 2013. Domain-independent caption-
ing of domain-specific images. In Proceedings of the
2013 NAACL HLT Student Research Workshop, pages
69–76, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alexander C. Berg, Tamara L. Berg, and Hal Daum´e
III. 2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747–756.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Florent Perronnin, Zeynep Akata, Zaid Harchaoui, and
Cordelia Schmid. 2012. Towards good practice in
large-scale learning for image classification. In CVPR.
Dan Roth and Wen tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of the Annual Conference on Computa-
tional Natural Language Learning (CoNLL).
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’08, pages 254–263, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and de-
scribing images with sentences. In Transactions of the
Association for Computational Linguistics, pages 207
– 218, April.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 290–297, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude
Oliva, and Antonio Torralba. 2010. Sun database:
Large-scale scene recognition from abbey to zoo. In
CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444–454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee,
and Song-Chun Zhu. 2010. I2T: Image parsing to text
description. Proc. IEEE, 98(8).
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 53–63, Sofia, Bulgaria,
August. Association for Computational Linguistics.
</reference>
<page confidence="0.998264">
362
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372066">
<title confidence="0.998299">Composition and Compression of Trees for Image Descriptions</title>
<author confidence="0.401556">L</author>
<affiliation confidence="0.968393">Brook University Chapel Hill of Washington</affiliation>
<address confidence="0.916951">Stony Brook, NY Chapel Hill, NC Seattle, WA</address>
<email confidence="0.999732">@cs.stonybrook.edu@cs.unc.edu</email>
<abstract confidence="0.999524681818182">We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related image caption genwhere the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algocomponents are composition both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="38480" citStr="Barzilay and McKeown, 2005" startWordPosition="6396" endWordPosition="6399">phorically crea&amp;ve! problems to generaliza&amp;on error Figure 10: Description generation: bad examples. plausible, thanks to the expressive, but somewhat predictable descriptions online users write about their photos. Even among the bad examples (Figure 10) one can find highly creative captions with not literal but metaphorical relevance: “Monarch in her bedroom before the wedding ceremony”.12 The complete system captions and the original captions are available at http://ilp-cky.appspot. com/ 6 Related Work Sentence Fusion Sentence fusion has been studied mostly for multi-document summarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altu</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Jonathan Shih</author>
</authors>
<title>Automatic attribute discovery and characterization from noisy web data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European Conference on Computer Vision: Part I, ECCV’10,</booktitle>
<pages>663--676</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5495" citStr="Berg et al., 2010" startWordPosition="854" endWordPosition="857">3; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we also investigate a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea of composition based on extracte</context>
</contexts>
<marker>Berg, Berg, Shih, 2010</marker>
<rawString>Tamara L. Berg, Alexander C. Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization from noisy web data. In Proceedings of the 11th European Conference on Computer Vision: Part I, ECCV’10, pages 663–676, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1.</title>
<date>2006</date>
<booktitle>In Linguistic Data Consortium.</booktitle>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. In Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--399</pages>
<contexts>
<context position="11732" citStr="Clarke and Lapata, 2008" startWordPosition="1862" endWordPosition="1865"> position i and ending at position j. If a cell in the CKY matrix is labeled with a nonterminal symbol s, it means that the corresponding tree of Gij has s as its root. Although we visualize the operation using a CKYstyle representation in Figure 3, note that composition requires more complex combinatorial decisions than CKY parsing due to two additional considerations. We are: (1) selecting a subset of candidate phrases, and (2) re-ordering the selected phrases (hence making the problem NP-hard). Therefore, we encode our problem using Integer Linear Programming (ILP) (Roth and tau Yih, 2004; Clarke and Lapata, 2008) and use the CPLEX (ILOG, Inc, 2006) solver. 3.1 ILP Variables Variables for Sequence Structure: Variables α encode phrase selection and ordering: αik = 1 iff phrase i E P is selected (1) for position k E [0, N) Where k is one of the N=4 positions in a sentence.3 Additionally, we define variables for each pair of adjacent phrases to capture sequence cohesion: 3The number of positions is equal to the number of phrase al., 2010) types, since we select at most one from each type. 353 1&apos;021o=b1 re nt h #02 = 1 y: k=0e 02 k=0f PP-VP ce. �02 1 s g12 e p cato �� e 22 jkpq k+1) was 1staring Phra qtk+1</context>
<context position="16967" citStr="Clarke and Lapata (2008)" startWordPosition="2827" endWordPosition="2830">ILP formulation encoded in this w computationally lightweght. If the unit of co h i f t 4All weights are normalized using z-score. siti wa w, te L ition was word the ILP instan he ILP formulation encoded in βikse For = � e for s∈NT N-2 E ducg a c k=0 αijklsy e a j−1 e E k=i p lid tree st Fr × βijkr root of t f th lec � the a va he r r∈R resent possibledother choices that are Thissobjective is comprised of thre (confidenceFscores): Fi, Fij, Fr a onvenience, OneacanalvieweFi as aCcon and Manning, 2003). ables (Equations 2, 4, 5). Constraints for a product of two variables have been discussed by Clarke and Lapata (2008). For Equation 2, we add the following constraints (similar constraints are also added for Equations 4,5). ∀ijk, αijk ≤ αik (7) αijk ≤ αj(k+1) αijk + (1 − αik) + (1 − αj(k+1)) ≥ 1 Consistency between Tree Leafs and Sequences: The ordering of phrases implied by αijk must be consistent with the ordering of phrases implied by the O variables. This can be achieved by aligning the leaf cells (i.e., Okks) in the CKY-style matrix with α variables as follows: X∀ik, αik ≤ Okks (8) sENTi X∀k, Xαik = Okks (9) i sENT Where NTi refers to the set of PCFG nonterminals that are compatible with a phrase type p</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. Journal of Artificial Intelligence Research, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>73--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="39060" citStr="Cohn and Lapata, 2007" startWordPosition="6484" endWordPosition="6487">ummarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By i</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 73–82, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>137--144</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="39208" citStr="Cohn and Lapata, 2008" startWordPosition="6507" endWordPosition="6510">generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By integrating both the tree structure and the sequence structure, we have significantly improved the quality of composed image captions over several co</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137–144, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Navneet Dalal</author>
<author>Bill Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05,</booktitle>
<pages>886--893</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="8783" citStr="Dalal and Triggs, 2005" startWordPosition="1360" endWordPosition="1363">ide in isolation. In all of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query image detection, we extract four types of phrases (as tree fragments). First, we retrieve relevant noun phrases from images with visually similar object detections. We use color, texture (Leung and Malik, 1999), and shape (Dalal and Triggs, 2005; Lowe, 2004) based features encoded in a histogram of vector quantized responses to measure visual similarity. Second, we extract verb phrases for which the corresponding noun phrase takes the subject role. Third, from those images with “stuff” detections, e.g.“water”, or “sky” (typically mass nouns), we extract prepositional phrases based on similarity of both visual appearance and relative spatial relationships between detected objects and “stuff”. Finally, we use global “scene” similarity2 to extract prepositional phrases referring to the overall scene, e.g., “at the conference,” or “in th</context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05, pages 886–893, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Alexander C Berg</author>
<author>Kai Li</author>
<author>Fei-Fei Li</author>
</authors>
<title>What does classifying more than 10,000 image categories tell us?</title>
<date>2010</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="2924" citStr="Deng et al., 2010" startWordPosition="438" endWordPosition="441">s been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired. However, it is not clear whether th</context>
</contexts>
<marker>Deng, Berg, Li, Li, 2010</marker>
<rawString>Jia Deng, Alexander C. Berg, Kai Li, and Fei-Fei Li. 2010. What does classifying more than 10,000 image categories tell us? In ECCV.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jia Deng</author>
<author>Alexander C Berg</author>
</authors>
<title>Sanjeev Satheesh, Hao Su, Aditya Khosla, and Fei-Fei Li. 2012a. Large scale visual recognition challenge.</title>
<booktitle>In http://www.imagenet.org/challenges/LSVRC/2012/index.</booktitle>
<marker>Deng, Berg, </marker>
<rawString>Jia Deng, Alexander C. Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Fei-Fei Li. 2012a. Large scale visual recognition challenge. In http://www.imagenet.org/challenges/LSVRC/2012/index.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Jonathan Krause</author>
<author>Alexander C Berg</author>
<author>L Fei-Fei</author>
</authors>
<title>Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition.</title>
<date>2012</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="2904" citStr="Deng et al., 2012" startWordPosition="434" endWordPosition="437">activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired. However, it is </context>
<context position="27208" citStr="Deng et al., 2012" startWordPosition="4618" endWordPosition="4621">n for children as: count(rt,rij) Afdel = log P(rt|rij) = log (19) count (rij ) Where count(rt, rij) is the frequency in which rij is transformed to rt by deletion of one of the children. We estimate this probability from a training corpus, described in §4.3. count(rij) is the count of rij in uncompressed sentences. IV. Vision Detection (Content Selection): We want to keep words referring to actual objects in the image. Thus, we use V (xj), a visual similarity score, as our confidence of an object corresponding to word xj. This similarity is obtained from the visual recognition predictions of (Deng et al., 2012b). Note that some test instances include rules that we have not observed during training. We default to the original caption in those cases. The weights θi are set using a tuning dataset. We control overcompression by setting the weight for fdel to a small value relative to the other weights. 4.3 Human Compressed Captions Although we model image caption generalization as sentence compression, in practical applications we may want the outputs of these two tasks to be different. For example, there may be differences in what should be deleted (named entities in newswire summaries could be import</context>
</contexts>
<marker>Deng, Krause, Berg, Fei-Fei, 2012</marker>
<rawString>Jia Deng, Jonathan Krause, Alexander C. Berg, and L. Fei-Fei. 2012b. Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition. In Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="31612" citStr="Denkowski and Lavie, 2011" startWordPosition="5327" endWordPosition="5330"> blocks. We also experiment with the compression of human written captions, which are used to generate image descriptions for the new target images. Baselines for Compression: • SEQCOMPRESSION (Kuznetsova et al., 2013): Inference operates over the sequence structure. Although optimization is subject to constraints derived from dependency parse, parsing is not an explicit part of the inference structure. Example outputs are shown in Figure 7. 5.1 Automatic Evaluation We perform automatic evaluation using two measures widely used in machine translation: BLEU (Papineni et al., 2002)8 and METEOR (Denkowski and Lavie, 2011).9 We remove all punctuation and convert captions to lower case. We use 1K test images from the captioned image corpus,10 and assume the original captions as the gold standard captions to compare against. The results in Table 1 8We use the unigram NIST implementation: ftp://jaguar. ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz 9With equal weight between precision and recall in Table 1. 10Except for those for which image URLs are broken, or CPLEX did not return a solution. 358 Method-1 Method-2 Criteria Method-1 preferred over Method-2 (%) all turkers turkers w/ κ &gt; 0.55 turkers w/ κ &gt;</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Karl Stratos</author>
<author>Kota Yamaguchi</author>
<author>Yejin Choi</author>
<author>Hal Daume Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Detecting visual text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>762--772</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5475" citStr="Dodge et al., 2012" startWordPosition="850" endWordPosition="853">Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we also investigate a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea of compositi</context>
</contexts>
<marker>Dodge, Goyal, Han, Mensch, Mitchell, Stratos, Yamaguchi, Choi, Berg, Berg, 2012</marker>
<rawString>Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin Choi, Hal Daume III, Alexander C. Berg, and Tamara L. Berg. 2012. Detecting visual text. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762– 772, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image description using visual dependency representations.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1292--1302</pages>
<contexts>
<context position="2536" citStr="Elliott and Keller, 2013" startWordPosition="375" endWordPosition="378">f photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on desc</context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations. In EMNLP, pages 1292–1302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Comparing automatic evaluation measures for image description.</title>
<date>2014</date>
<booktitle>In ACL (2),</booktitle>
<pages>452--457</pages>
<contexts>
<context position="34298" citStr="Elliott and Keller, 2014" startWordPosition="5748" endWordPosition="5751"> We filter out turkers based on a control question. We then compute the selection rate (%) of preferring method-1 over method-2. The agreement among turkers is a frequent concern. Therefore, we vary the set of dependable users based on their Cohen’s kappa score (κ) against other users. It turns out, filtering users based on κ does not make a big difference in determining the winning method. As expected, tree-based systems significantly outperform sequence-based counterparts. For example, 11While 4-gram BLEU with brevity penalty is found to correlate better with human judges by recent studies (Elliott and Keller, 2014), we found that this is not the case for our task. This may be due to the differences in the gold standard captions. We use naturally existing ones, which include a wider range of content and style than crowd-sourced captions. Treepruning: The bu&amp;erflies are Seq+Tree+pruning: The bu&amp;erflies are a&amp;racted to the colourful flowers. a&amp;racted to the colourful flowers. Cap&gt;on Generaliza&gt;on Image Descrip&gt;on Genera&gt;on Figure 8: An example of a description preferred over human gold standard. Image description is improved due to caption generalization. SEQ+TREE is strongly preferred over SEQ, with a sel</context>
</contexts>
<marker>Elliott, Keller, 2014</marker>
<rawString>Desmond Elliott and Frank Keller. 2014. Comparing automatic evaluation measures for image description. In ACL (2), pages 452–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young1</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences for images.</title>
<date>2010</date>
<booktitle>In European Conference on Computer Vision.</booktitle>
<marker>Farhadi, Hejrati, Sadeghi, Young1, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young1, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences for images. In European Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part based models.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="8395" citStr="Felzenszwalb et al., 2010" startWordPosition="1295" endWordPosition="1298"> the syntactic parse of the corresponding textual descrip1http://ilp-cky.appspot.com/ 352 tion. This extraction strategy, originally proposed by Kuznetsova et al. (2012), attempts to make the best use of linguistic regularities with respect to objects, actions, and scenes, making it possible to obtain richer textual descriptions than what current state-of-the-art vision techniques can provide in isolation. In all of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query image detection, we extract four types of phrases (as tree fragments). First, we retrieve relevant noun phrases from images with visually similar object detections. We use color, texture (Leung and Malik, 1999), and shape (Dalal and Triggs, 2005; Lowe, 2004) based features encoded in a histogram of vector quantized responses to measure visual similarity. Second, we extract verb phrases for which the corresponding noun phrase takes the subject role. Thir</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. 2010. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic caption generation for news images.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="4878" citStr="Feng and Lapata, 2013" startWordPosition="751" endWordPosition="754">compose “I noticed that this funny cow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; B</context>
</contexts>
<marker>Feng, Lapata, 2013</marker>
<rawString>Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(4):797–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Yasemin Altun</author>
</authors>
<title>Overcoming the lack of parallel data in sentence compression.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1481--1491</pages>
<contexts>
<context position="39088" citStr="Filippova and Altun, 2013" startWordPosition="6488" endWordPosition="6491">and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image description generation. By integrating both the tree str</context>
</contexts>
<marker>Filippova, Altun, 2013</marker>
<rawString>Katja Filippova and Yasemin Altun. 2013. Overcoming the lack of parallel data in sentence compression. In EMNLP, pages 1481–1491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inc ILOG</author>
</authors>
<title>ILOG CPLEX: High-performance software for mathematical programming and optimization. See http://www.ilog.com/products/ cplex/.</title>
<date>2006</date>
<marker>ILOG, 2006</marker>
<rawString>ILOG, Inc. 2006. ILOG CPLEX: High-performance software for mathematical programming and optimization. See http://www.ilog.com/products/ cplex/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jamieson</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
<author>Sven J Dickinson</author>
<author>Sven Wachsmuth</author>
</authors>
<title>Using language to learn structured appearance models for image annotation.</title>
<date>2010</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="20344" citStr="Jamieson et al., 2010" startWordPosition="3398" endWordPosition="3401"> to use expressive language, and less likely to make syntactic or semantic errors. Our phrase extraction process can be viewed at a high level as visually-grounded or visually-situated paraphrasing. Also, because the unit of operation is tree fragments, the ILP formulation encoded in this work is computationally lightweight. If the unit of composition was words, the ILP instances would be significantly more computationally intensive, and more likely to suffer from grammatical and semantic errors. 4 Tree Compression As noted by recent studies (Mason and Charniak, 2013; Kuznetsova et al., 2013; Jamieson et al., 2010), naturally existing image captions often include contextual information that does not directly describe visual content, which ultimately hinders their usefulness for describing other images. Therefore, to improve the fidelity of the generated descriptions, we explore image caption generalization as an N−1X t=k O0ts (12) Okks ≤ X sENT X ∀ij k 355 A cat strolled along the fence and posed for this classic profile Figure 4: Compressed captions (on the left) are more applicable for describing new images (on the right). optional pre-processing step. Figure 4 illustrates a concrete example of image </context>
</contexts>
<marker>Jamieson, Fazly, Stevenson, Dickinson, Wachsmuth, 2010</marker>
<rawString>Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson, Sven J. Dickinson, and Sven Wachsmuth. 2010. Using language to learn structured appearance models for image annotation. IEEE Trans. Pattern Anal. Mach. Intell., 32(1):148–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="29779" citStr="Knight and Marcu (2000)" startWordPosition="5047" endWordPosition="5050">h the curtains. Figure 7: Caption generalization: good/bad examples. be extraneous for image caption generalization). To learn the syntactic patterns for caption generalization, we collect a small set of example compressed captions (380 in total) using Amazon Mechanical Turk (AMT) (Snow et al., 2008). For each image, we asked 3 turkers to first list all visible objects in an image and then to write a compressed caption by removing not visually verifiable bits of text. We then align the original and compressed captions to measure rule deletion probabilities, excluding misalignments, similar to Knight and Marcu (2000). Note that we remove this dataset from the 1M caption corpus when we perform description generation. 5 Experiments We use the 1M captioned image corpus of Ordonez et al. (2011). We reserve 1K images as a test set, and use the rest of the corpus for phrase extraction. We experiment with the following approaches: Proposed Approaches: • TREEPRUNING: Our tree compression approach as described in §4. • SEQ+TREE: Our tree composition approach as described in §3. • SEQ+TREE+PRUNING: SEQ+TREE using compressed captions of TREEPRUNING as building blocks. Baselines for Composition: • SEQ+LINGRULE: The m</context>
<context position="39010" citStr="Knight and Marcu, 2000" startWordPosition="6475" endWordPosition="6478">fusion has been studied mostly for multi-document summarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed capt</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization - step one: Sentence compression. In AAAI/IAAI, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsuhiro Kojima</author>
<author>Takeshi Tamura</author>
<author>Kunio Fukunaga</author>
</authors>
<title>Natural language description of human activities from video images based on concept hierarchy of actions.</title>
<date>2002</date>
<journal>IJCV,</journal>
<volume>50</volume>
<contexts>
<context position="3355" citStr="Kojima et al., 2002" startWordPosition="505" endWordPosition="508">tructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired. However, it is not clear whether the semantic expressiveness of these approaches can eventually scale up to the casual, but highly expressive language peo351 Transactions of the Association for Computational Linguistics, 2 (2014) 351–362. Action Editor: Hal Daume III. Submitted 2/2014; Revised 5/2014; Published 10/2014. c�2014 Association for Computational Linguistics. Target Image Object Ac/on Stuff Scene A cow standing in the I noticed that this funny A bird h</context>
</contexts>
<marker>Kojima, Tamura, Fukunaga, 2002</marker>
<rawString>Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga. 2002. Natural language description of human activities from video images based on concept hierarchy of actions. IJCV, 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niveda Krishnamoorthy</author>
<author>Girish Malkarnenkar</author>
<author>Raymond J Mooney</author>
<author>Kate Saenko</author>
<author>Sergio Guadarrama</author>
</authors>
<title>Generating natural-language video descriptions using text-mined knowledge.</title>
<date>2013</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="2510" citStr="Krishnamoorthy et al., 2013" startWordPosition="371" endWordPosition="374">from automatic organization of photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning.</context>
</contexts>
<marker>Krishnamoorthy, Malkarnenkar, Mooney, Saenko, Guadarrama, 2013</marker>
<rawString>Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J. Mooney, Kate Saenko, and Sergio Guadarrama. 2013. Generating natural-language video descriptions using text-mined knowledge. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2950" citStr="Krizhevsky et al., 2012" startWordPosition="442" endWordPosition="445">ke in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired. However, it is not clear whether the semantic expressiveness </context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. Imagenet classification with deep convolutional neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>BabyTalk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="2442" citStr="Kulkarni et al., 2011" startWordPosition="359" endWordPosition="362">ng efforts could potentially be useful for many applications: from automatic organization of photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two ma</context>
<context position="6570" citStr="Kulkarni et al., 2011" startWordPosition="1019" endWordPosition="1022">ew description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea of composition based on extracted phrases is not new in itself (Kuznetsova et al., 2012), however, we make several technical and empirical contributions. First, we propose a novel stochastic tree composition algorithm based on extracted tree fragments that integrates both tree structure and sequence cohesion into structural inference. Our algorithm permits a substantially higher level of linguistic expressiveness, flexibility, and creativity than those based on rules or templates (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012), while also addressing long-distance grammatical relations in a more principled way than those based on hand-coded constraints (Kuznetsova et al., 2012). Second, we address image caption generalization as an optional subtask of image caption generation, and propose a tree compression algorithm that performs a light-weight parsing to search for the optimal set of tree branches to prune. Our work is the first to report empirical benefits of automatically compressed captions for image captioning. The proposed approaches attain significantly better perfo</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. BabyTalk: Understanding and generating simple image descriptions. In Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>359--368</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="4855" citStr="Kuznetsova et al., 2012" startWordPosition="747" endWordPosition="750">ple, it would be hard to compose “I noticed that this funny cow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text</context>
<context position="6151" citStr="Kuznetsova et al., 2012" startWordPosition="958" endWordPosition="961"> a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea of composition based on extracted phrases is not new in itself (Kuznetsova et al., 2012), however, we make several technical and empirical contributions. First, we propose a novel stochastic tree composition algorithm based on extracted tree fragments that integrates both tree structure and sequence cohesion into structural inference. Our algorithm permits a substantially higher level of linguistic expressiveness, flexibility, and creativity than those based on rules or templates (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012), while also addressing long-distance grammatical relations in a more principled way than those based on hand-coded constraints (Kuznetsov</context>
<context position="7938" citStr="Kuznetsova et al. (2012)" startWordPosition="1225" endWordPosition="1228">roved image caption corpus with automatic generalization, which is publicly available.1 2 Harvesting Tree Fragments Given a query image, we retrieve images that are visually similar to the query image, then extract potentially useful segments (i.e., phrases) from their corresponding image descriptions. We then compose a new image description using these retrieved text fragments (§3). Extraction of useful phrases is guided by both visual similarity and the syntactic parse of the corresponding textual descrip1http://ilp-cky.appspot.com/ 352 tion. This extraction strategy, originally proposed by Kuznetsova et al. (2012), attempts to make the best use of linguistic regularities with respect to objects, actions, and scenes, making it possible to obtain richer textual descriptions than what current state-of-the-art vision techniques can provide in isolation. In all of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query </context>
<context position="30455" citStr="Kuznetsova et al., 2012" startWordPosition="5153" endWordPosition="5156">on corpus when we perform description generation. 5 Experiments We use the 1M captioned image corpus of Ordonez et al. (2011). We reserve 1K images as a test set, and use the rest of the corpus for phrase extraction. We experiment with the following approaches: Proposed Approaches: • TREEPRUNING: Our tree compression approach as described in §4. • SEQ+TREE: Our tree composition approach as described in §3. • SEQ+TREE+PRUNING: SEQ+TREE using compressed captions of TREEPRUNING as building blocks. Baselines for Composition: • SEQ+LINGRULE: The most equivalent to the older sequence-driven system (Kuznetsova et al., 2012). Uses a few minor enhancements, such as sentence-boundary statistics, to improve grammaticality. • SEQ: The §3 system without tree models and mentioned enhancements of SEQ+LINGRULE. Method Bleu Meteor w/ (w/o) penalty P R M SEQ+LINGRULE 0.152 (0.152) 0.13 0.17 0.095 SEQ 0.138 (0.138) 0.12 0.18 0.094 SEQ+TREE 0.149 (0.149) 0.13 0.14 0.082 SEQ+PRUNING 0.177 (0.177) 0.15 0.16 0.101 SEQ+TREE+PRUNING 0.140 (0.189) 0.16 0.12 0.088 Table 1: Automatic Evaluation • SEQ+PRUNING: SEQ using compressed captions of TREEPRUNING as building blocks. We also experiment with the compression of human written cap</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 359–368, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Generalizing image captions for image-text parallel corpus.</title>
<date>2013</date>
<booktitle>In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5600" citStr="Kuznetsova et al., 2013" startWordPosition="870" endWordPosition="873"> substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we also investigate a related task of image caption generalization (Kuznetsova et al., 2013), which aims to improve the semantic image-text alignment by removing bits of text from existing captions that are less likely to be transferable to other images. The high-level idea of our system is to harvest useful bits of text (as tree fragments) from existing image descriptions using detected visual content similarity, and then to compose a new description by selectively combining these extracted (and optionally pruned) tree fragments. This overall idea of composition based on extracted phrases is not new in itself (Kuznetsova et al., 2012), however, we make several technical and empirica</context>
<context position="20320" citStr="Kuznetsova et al., 2013" startWordPosition="3394" endWordPosition="3397">ten captions, we are able to use expressive language, and less likely to make syntactic or semantic errors. Our phrase extraction process can be viewed at a high level as visually-grounded or visually-situated paraphrasing. Also, because the unit of operation is tree fragments, the ILP formulation encoded in this work is computationally lightweight. If the unit of composition was words, the ILP instances would be significantly more computationally intensive, and more likely to suffer from grammatical and semantic errors. 4 Tree Compression As noted by recent studies (Mason and Charniak, 2013; Kuznetsova et al., 2013; Jamieson et al., 2010), naturally existing image captions often include contextual information that does not directly describe visual content, which ultimately hinders their usefulness for describing other images. Therefore, to improve the fidelity of the generated descriptions, we explore image caption generalization as an N−1X t=k O0ts (12) Okks ≤ X sENT X ∀ij k 355 A cat strolled along the fence and posed for this classic profile Figure 4: Compressed captions (on the left) are more applicable for describing new images (on the right). optional pre-processing step. Figure 4 illustrates a co</context>
<context position="31204" citStr="Kuznetsova et al., 2013" startWordPosition="5265" endWordPosition="5268">t tree models and mentioned enhancements of SEQ+LINGRULE. Method Bleu Meteor w/ (w/o) penalty P R M SEQ+LINGRULE 0.152 (0.152) 0.13 0.17 0.095 SEQ 0.138 (0.138) 0.12 0.18 0.094 SEQ+TREE 0.149 (0.149) 0.13 0.14 0.082 SEQ+PRUNING 0.177 (0.177) 0.15 0.16 0.101 SEQ+TREE+PRUNING 0.140 (0.189) 0.16 0.12 0.088 Table 1: Automatic Evaluation • SEQ+PRUNING: SEQ using compressed captions of TREEPRUNING as building blocks. We also experiment with the compression of human written captions, which are used to generate image descriptions for the new target images. Baselines for Compression: • SEQCOMPRESSION (Kuznetsova et al., 2013): Inference operates over the sequence structure. Although optimization is subject to constraints derived from dependency parse, parsing is not an explicit part of the inference structure. Example outputs are shown in Figure 7. 5.1 Automatic Evaluation We perform automatic evaluation using two measures widely used in machine translation: BLEU (Papineni et al., 2002)8 and METEOR (Denkowski and Lavie, 2011).9 We remove all punctuation and convert captions to lower case. We use 1K test images from the captioned image corpus,10 and assume the original captions as the gold standard captions to comp</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2013</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2013. Generalizing image captions for image-text parallel corpus. In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Leung</author>
<author>Jitendra Malik</author>
</authors>
<title>Recognizing surfaces using three-dimensional textons.</title>
<date>1999</date>
<booktitle>In ICCV,</booktitle>
<pages>1010--1017</pages>
<contexts>
<context position="8748" citStr="Leung and Malik, 1999" startWordPosition="1354" endWordPosition="1357">-the-art vision techniques can provide in isolation. In all of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query image detection, we extract four types of phrases (as tree fragments). First, we retrieve relevant noun phrases from images with visually similar object detections. We use color, texture (Leung and Malik, 1999), and shape (Dalal and Triggs, 2005; Lowe, 2004) based features encoded in a histogram of vector quantized responses to measure visual similarity. Second, we extract verb phrases for which the corresponding noun phrase takes the subject role. Third, from those images with “stuff” detections, e.g.“water”, or “sky” (typically mass nouns), we extract prepositional phrases based on similarity of both visual appearance and relative spatial relationships between detected objects and “stuff”. Finally, we use global “scene” similarity2 to extract prepositional phrases referring to the overall scene, e</context>
</contexts>
<marker>Leung, Malik, 1999</marker>
<rawString>Thomas K. Leung and Jitendra Malik. 1999. Recognizing surfaces using three-dimensional textons. In ICCV, pages 1010–1017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2459" citStr="Li et al., 2011" startWordPosition="363" endWordPosition="366">ially be useful for many applications: from automatic organization of photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary </context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220–228, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>Int. J. Comput. Vision,</journal>
<pages>60--91</pages>
<contexts>
<context position="8796" citStr="Lowe, 2004" startWordPosition="1364" endWordPosition="1365"> of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query image detection, we extract four types of phrases (as tree fragments). First, we retrieve relevant noun phrases from images with visually similar object detections. We use color, texture (Leung and Malik, 1999), and shape (Dalal and Triggs, 2005; Lowe, 2004) based features encoded in a histogram of vector quantized responses to measure visual similarity. Second, we extract verb phrases for which the corresponding noun phrase takes the subject role. Third, from those images with “stuff” detections, e.g.“water”, or “sky” (typically mass nouns), we extract prepositional phrases based on similarity of both visual appearance and relative spatial relationships between detected objects and “stuff”. Finally, we use global “scene” similarity2 to extract prepositional phrases referring to the overall scene, e.g., “at the conference,” or “in the market”. We</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision, 60:91–110, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Annotation of online shopping images without labeled training examples.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop on Vision and Language,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="20295" citStr="Mason and Charniak, 2013" startWordPosition="3390" endWordPosition="3393">ct phrases from human written captions, we are able to use expressive language, and less likely to make syntactic or semantic errors. Our phrase extraction process can be viewed at a high level as visually-grounded or visually-situated paraphrasing. Also, because the unit of operation is tree fragments, the ILP formulation encoded in this work is computationally lightweight. If the unit of composition was words, the ILP instances would be significantly more computationally intensive, and more likely to suffer from grammatical and semantic errors. 4 Tree Compression As noted by recent studies (Mason and Charniak, 2013; Kuznetsova et al., 2013; Jamieson et al., 2010), naturally existing image captions often include contextual information that does not directly describe visual content, which ultimately hinders their usefulness for describing other images. Therefore, to improve the fidelity of the generated descriptions, we explore image caption generalization as an N−1X t=k O0ts (12) Okks ≤ X sENT X ∀ij k 355 A cat strolled along the fence and posed for this classic profile Figure 4: Compressed captions (on the left) are more applicable for describing new images (on the right). optional pre-processing step. </context>
</contexts>
<marker>Mason, Charniak, 2013</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2013. Annotation of online shopping images without labeled training examples. In Proceedings of Workshop on Vision and Language, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
</authors>
<title>Domain-independent captioning of domain-specific images.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 NAACL HLT Student Research Workshop,</booktitle>
<pages>69--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="4891" citStr="Mason, 2013" startWordPosition="755" endWordPosition="756"> this funny cow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2</context>
</contexts>
<marker>Mason, 2013</marker>
<rawString>Rebecca Mason. 2013. Domain-independent captioning of domain-specific images. In Proceedings of the 2013 NAACL HLT Student Research Workshop, pages 69–76, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections. In</title>
<date>2012</date>
<booktitle>EACL,</booktitle>
<pages>747--756</pages>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg, Berg, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander C. Berg, Tamara L. Berg, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In EACL, pages 747–756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="4914" citStr="Ordonez et al., 2011" startWordPosition="757" endWordPosition="760">ow was staring at me” or “You can see these beautiful hills only in the countryside” in a purely bottom-up manner based on the exact content detected. The key technical bottleneck is that the range of describable content (i.e., objects, attributes, actions) is ultimately confined by the set of items that can be reliably recognized by state-ofthe-art vision techniques. The second direction, in a complementary avenue to the first, has explored ways to make use of the rich spectrum of visual descriptions contributed by online citizens (Kuznetsova et al., 2012; Feng and Lapata, 2013; Mason, 2013; Ordonez et al., 2011). In these approaches, the set of what can be described can be substantially larger than the set of what can be recognized, where the former is shaped and defined by the data, rather than by humans. This allows the resulting descriptions to be substantially more expressive, elaborate, and interesting than what would be possible in a purely bottom-up manner. Our work contributes to this second line of research. One challenge in utilizing naturally existing multimodal data, however, is the noisy semantic alignment between images and text (Dodge et al., 2012; Berg et al., 2010). Therefore, we als</context>
<context position="8263" citStr="Ordonez et al. (2011)" startWordPosition="1276" endWordPosition="1279">age description using these retrieved text fragments (§3). Extraction of useful phrases is guided by both visual similarity and the syntactic parse of the corresponding textual descrip1http://ilp-cky.appspot.com/ 352 tion. This extraction strategy, originally proposed by Kuznetsova et al. (2012), attempts to make the best use of linguistic regularities with respect to objects, actions, and scenes, making it possible to obtain richer textual descriptions than what current state-of-the-art vision techniques can provide in isolation. In all of our experiments we use the captioned image corpus of Ordonez et al. (2011), first pre-processing the corpus for relevant content by running deformable part model object detectors (Felzenszwalb et al., 2010). For our study, we run detectors for 89 object classes set a high confidence threshold for detection. As illustrated in Figure 1, for a query image detection, we extract four types of phrases (as tree fragments). First, we retrieve relevant noun phrases from images with visually similar object detections. We use color, texture (Leung and Malik, 1999), and shape (Dalal and Triggs, 2005; Lowe, 2004) based features encoded in a histogram of vector quantized response</context>
<context position="15068" citStr="Ordonez et al., 2011" startWordPosition="2515" endWordPosition="2519">idtarrows bine the sliecteddset of phrases.eNonterm,nal symbols in (also in blue) represent the1chosen PCFG rules to comsmaller fontc(in red) and dotted arrow s (alsogin red) rep not selected. e types of weights Fi)represents the phrase selection score based on visual similarity, described in §2. Fij quantifies the sequence cohesion across phrase boundaries. Forothis, we use ngram scores (n E [2,5]) between adjacent phrases computed using the Google Web 1-T corpus (Brants andoFranz., 2006). Finally, Frrquantifies PCFGIrule scores (log probabilities) estimated fromrthet1M imagehcaptionlcorpuse(Ordonez et al., 2011) parsedrusing the Stanfordoparser (Klein tent selectionscore, ere h pq 2 R whileddFij andYFracorrespond toclinguisticefluency Disio Th um h nalscores capturing sequenceeandttreeistructure respectively. If we set positive values for allhof these weights, the optimization functionbwould be beased s r semantic erros. Our phase ion are tree fragments rather than individual tord verbos production, since selcting an addiwords. There are three pracical benefits: (1 tactc and semantic expressiveness (2) correct an be viewe at a high lvel as tional phrse will increase th objctive function. To tacic and</context>
<context position="29956" citStr="Ordonez et al. (2011)" startWordPosition="5078" endWordPosition="5081">e collect a small set of example compressed captions (380 in total) using Amazon Mechanical Turk (AMT) (Snow et al., 2008). For each image, we asked 3 turkers to first list all visible objects in an image and then to write a compressed caption by removing not visually verifiable bits of text. We then align the original and compressed captions to measure rule deletion probabilities, excluding misalignments, similar to Knight and Marcu (2000). Note that we remove this dataset from the 1M caption corpus when we perform description generation. 5 Experiments We use the 1M captioned image corpus of Ordonez et al. (2011). We reserve 1K images as a test set, and use the rest of the corpus for phrase extraction. We experiment with the following approaches: Proposed Approaches: • TREEPRUNING: Our tree compression approach as described in §4. • SEQ+TREE: Our tree composition approach as described in §3. • SEQ+TREE+PRUNING: SEQ+TREE using compressed captions of TREEPRUNING as building blocks. Baselines for Composition: • SEQ+LINGRULE: The most equivalent to the older sequence-driven system (Kuznetsova et al., 2012). Uses a few minor enhancements, such as sentence-boundary statistics, to improve grammaticality. • S</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31572" citStr="Papineni et al., 2002" startWordPosition="5320" endWordPosition="5324"> captions of TREEPRUNING as building blocks. We also experiment with the compression of human written captions, which are used to generate image descriptions for the new target images. Baselines for Compression: • SEQCOMPRESSION (Kuznetsova et al., 2013): Inference operates over the sequence structure. Although optimization is subject to constraints derived from dependency parse, parsing is not an explicit part of the inference structure. Example outputs are shown in Figure 7. 5.1 Automatic Evaluation We perform automatic evaluation using two measures widely used in machine translation: BLEU (Papineni et al., 2002)8 and METEOR (Denkowski and Lavie, 2011).9 We remove all punctuation and convert captions to lower case. We use 1K test images from the captioned image corpus,10 and assume the original captions as the gold standard captions to compare against. The results in Table 1 8We use the unigram NIST implementation: ftp://jaguar. ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz 9With equal weight between precision and recall in Table 1. 10Except for those for which image URLs are broken, or CPLEX did not return a solution. 358 Method-1 Method-2 Criteria Method-1 preferred over Method-2 (%) all tu</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Perronnin</author>
<author>Zeynep Akata</author>
<author>Zaid Harchaoui</author>
<author>Cordelia Schmid</author>
</authors>
<title>Towards good practice in large-scale learning for image classification.</title>
<date>2012</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="2885" citStr="Perronnin et al., 2012" startWordPosition="430" endWordPosition="433">n their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impair</context>
</contexts>
<marker>Perronnin, Akata, Harchaoui, Schmid, 2012</marker>
<rawString>Florent Perronnin, Zeynep Akata, Zaid Harchaoui, and Cordelia Schmid. 2012. Towards good practice in large-scale learning for image classification. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 254–263, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<pages>207--218</pages>
<contexts>
<context position="2580" citStr="Socher et al., 2014" startWordPosition="383" endWordPosition="386">h with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, a</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. In Transactions of the Association for Computational Linguistics, pages 207 – 218, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>290--297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="26034" citStr="Turner and Charniak (2005)" startWordPosition="4424" endWordPosition="4427">ons for each criteria q: Aφ[r, ij] = � θ · Af9(r,ij) (18) 9 Note that Aφ depends on the current rule r, along with the historical information before the current step ij, such as the original rule rij, and ngrams on the border between left and right child branches of rule rij. We use the following four criteria fq in our model, which are demonstrated in Figures 5 and 6. I. Tree Structure: We capture PCFG rule probabilities estimated from the corpus as Afpcfg = log Ppcfg(r). 6We assign probabilities of these special propagation rules to 1 so that they will not affect the final parse tree score. Turner and Charniak (2005) handled propagation cases similarly. 7We use Δ to distinguish the potential value for the whole sentence from the gain of the potential during a single step of the algorithm. Figure 6: CKY compression. Both the chosen rules and phrases (blue bold font and blue solid arrows) and not chosen rules and phrases (red italic smaller font and red dashed lines) are shown. II. Sequence Structure: We incorporate ngram cohesion scores only across the border between two branches of a subtree. III. Branch Deletion Probabilities: We compute probabilities of deletion for children as: count(rt,rij) Afdel = lo</context>
<context position="39037" citStr="Turner and Charniak, 2005" startWordPosition="6479" endWordPosition="6483">mostly for multi-document summarization (Barzilay and McKeown, 2005), where redundancy across multiple sentences serves as a guideline for syntactic and semantic validity of generation. In contrast, we do not have the natural redundancy to rely upon in our task, therefore requiring the composition algorithm to be intrinsically better constrained for correct sentence structures. 12“Monarch” can be a type of butterfly. Sentence Compression At the core of the image caption generalization task is sentence compression. Much work has considered deletion-only edits like ours (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2007; Filippova and Altun, 2013), while recent ones explore more complex edits, such as substitutions, insertions and reordering (Cohn and Lapata, 2008). The latter generally requires a larger training corpus. We leave more expressive compression as a future research work. 7 Conclusion In this paper, we have presented a novel tree composition approach for generating expressive image descriptions. As an optional preprocessing step, we also presented a tree compression approach and reported the empirical benefit of using automatically compressed captions to improve image descr</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 290–297, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianxiong Xiao</author>
<author>James Hays</author>
<author>Krista A Ehinger</author>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Sun database: Large-scale scene recognition from abbey to zoo.</title>
<date>2010</date>
<booktitle>In CVPR.</booktitle>
<marker>Xiao, Hays, Ehinger, Oliva, Torralba, 2010</marker>
<rawString>Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Ching Teo</author>
<author>Hal Daume</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Corpus-guided sentence generation of natural images.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2419" citStr="Yang et al., 2011" startWordPosition="355" endWordPosition="358">atic image captioning efforts could potentially be useful for many applications: from automatic organization of photo collections, to facilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks.</context>
<context position="6589" citStr="Yang et al., 2011" startWordPosition="1023" endWordPosition="1026">tively combining these extracted (and optionally pruned) tree fragments. This overall idea of composition based on extracted phrases is not new in itself (Kuznetsova et al., 2012), however, we make several technical and empirical contributions. First, we propose a novel stochastic tree composition algorithm based on extracted tree fragments that integrates both tree structure and sequence cohesion into structural inference. Our algorithm permits a substantially higher level of linguistic expressiveness, flexibility, and creativity than those based on rules or templates (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012), while also addressing long-distance grammatical relations in a more principled way than those based on hand-coded constraints (Kuznetsova et al., 2012). Second, we address image caption generalization as an optional subtask of image caption generation, and propose a tree compression algorithm that performs a light-weight parsing to search for the optimal set of tree branches to prune. Our work is the first to report empirical benefits of automatically compressed captions for image captioning. The proposed approaches attain significantly better performance for both ima</context>
</contexts>
<marker>Yang, Teo, Daume, Aloimonos, 2011</marker>
<rawString>Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Z Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song-Chun Zhu</author>
</authors>
<title>I2T: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>98</volume>
<issue>8</issue>
<contexts>
<context position="3310" citStr="Yao et al., 2010" startWordPosition="497" endWordPosition="500">en requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those items (e.g., objects, attributes) that are detected by vision recognition, which subsequently confines what should be described and how (Yao et al., 2010; Kulkarni et al., 2011; Kojima et al., 2002). Approaches in this direction could be ideal for various practical applications such as image description for the visually impaired. However, it is not clear whether the semantic expressiveness of these approaches can eventually scale up to the casual, but highly expressive language peo351 Transactions of the Association for Computational Linguistics, 2 (2014) 351–362. Action Editor: Hal Daume III. Submitted 2/2014; Revised 5/2014; Published 10/2014. c�2014 Association for Computational Linguistics. Target Image Object Ac/on Stuff Scene A cow stand</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. I2T: Image parsing to text description. Proc. IEEE, 98(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>53--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2558" citStr="Yu and Siskind, 2013" startWordPosition="379" endWordPosition="382">cilitating image search with complex natural language queries, to enhancing web accessibility for the visually impaired. On the intellectual side, by learning to describe the visual world from naturally existing web data, our study extends the domains of language grounding to the highly expressive language that people use in their everyday online activities. There has been a recent spike in efforts to automatically describe visual content in natural language (Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Farhadi et al., 2010; Krishnamoorthy et al., 2013; Elliott and Keller, 2013; Yu and Siskind, 2013; Socher et al., 2014). This reflects the long standing understanding that encoding the complexities and subtleties of image content often requires more expressive language constructs than a set of tags. Now that visual recognition algorithms are beginning to produce reliable estimates of image content (Perronnin et al., 2012; Deng et al., 2012a; Deng et al., 2010; Krizhevsky et al., 2012), the time seems ripe to begin exploring higher level semantic tasks. There have been two main complementary directions explored for automatic image captioning. The first focuses on describing exactly those i</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53–63, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
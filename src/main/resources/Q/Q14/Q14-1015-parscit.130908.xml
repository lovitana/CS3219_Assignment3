<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.972282">
Dynamic Language Models for Streaming Text
</title>
<author confidence="0.99967">
Dani Yogatama* Chong Wang* Bryan R. Routledge† Noah A. Smith* Eric P. Xing*
</author>
<affiliation confidence="0.9066645">
*School of Computer Science
†Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998767">
*{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, †routledge@cmu.edu
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997268538461538">
We present a probabilistic language model that
captures temporal dynamics and conditions on
arbitrary non-linguistic context features. These
context features serve as important indicators
of language changes that are otherwise difficult
to capture using text data by itself. We learn
our model in an efficient online fashion that is
scalable for large, streaming data. With five
streaming datasets from two different genres—
economics news articles and social media—we
evaluate our model on the task of sequential
language modeling. Our model consistently
outperforms competing models.
</bodyText>
<sectionHeader confidence="0.998457" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994779661017">
Language models are a key component in many NLP
applications, such as machine translation and ex-
ploratory corpus analysis. Language models are typi-
cally assumed to be static—the word-given-context
distributions do not change over time. Examples
include n-gram models (Jelinek, 1997) and proba-
bilistic topic models like latent Dirichlet allocation
(Blei et al., 2003); we use the term “language model”
to refer broadly to probabilistic models of text.
Recently, streaming datasets (e.g., social media)
have attracted much interest in NLP. Since such data
evolve rapidly based on events in the real world, as-
suming a static language model becomes unrealistic.
In general, more data is seen as better, but treating all
past data equally runs the risk of distracting a model
with irrelevant evidence. On the other hand, cau-
tiously using only the most recent data risks overfit-
ting to short-term trends and missing important time-
insensitive effects (Blei and Lafferty, 2006; Wang
et al., 2008). Therefore, in this paper, we take steps
toward methods for capturing long-range temporal
dynamics in language use.
Our model also exploits observable context vari-
ables to capture temporal variation that is otherwise
difficult to capture using only text. Specifically for
the applications we consider, we use stock market
data as exogenous evidence on which the language
model depends. For example, when an important
company’s price moves suddenly, the language model
should be based not on the very recent history, but
should be similar to the language model for a day
when a similar change happened, since people are
likely to say similar things (either about that com-
pany, or about conditions relevant to the change).
Non-linguistic contexts such as stock price changes
provide useful auxiliary information that might indi-
cate the similarity of language models across differ-
ent timesteps.
We also turn to a fully online learning framework
(Cesa-Bianchi and Lugosi, 2006) to deal with non-
stationarity and dynamics in the data that necessitate
adaptation of the model to data in real time. In on-
line learning, streaming examples are processed only
when they arrive. Online learning also eliminates
the need to store large amounts of data in memory.
Strictly speaking, online learning is distinct from
stochastic learning, which for language models built
on massive datasets has been explored by Hoffman
et al. (2013) and Wang et al. (2011). Those tech-
niques are still for static modeling. Language model-
ing for streaming datasets in the context of machine
translation was considered by Levenberg and Os-
borne (2009) and Levenberg et al. (2010). Goyal
et al. (2009) introduced a streaming algorithm for
large scale language modeling by approximating n-
gram frequency counts. We propose a general online
learning algorithm for language modeling that draws
inspiration from regret minimization in sequential
predictions (Cesa-Bianchi and Lugosi, 2006) and on-
</bodyText>
<page confidence="0.987022">
181
</page>
<bodyText confidence="0.93680252">
Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier.
Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics.
line variational algorithms (Sato, 2001; Honkela and
Valpola, 2003).
To our knowledge, our model is the first to bring
together temporal dynamics, conditioning on non-
linguistic context, and scalable online learning suit-
able for streaming data and extensible to include
topics and n-gram histories. The main idea of our
model is independent of the choice of the base lan-
guage model (e.g., unigrams, bigrams, topic models,
etc.). In this paper, we focus on unigram and bi-
gram language models in order to evaluate the basic
idea on well understood models, and to show how it
can be extended to higher-order n-grams. We leave
extensions to topic models for future work.
We propose a novel task to evaluate our proposed
language model. The task is to predict economics-
related text at a given time, taking into account the
changes in stock prices up to the corresponding day.
This can be seen an inverse of the setup considered
by Lavrenko et al. (2000), where news is assumed
to influence stock prices. We evaluate our model
on economics news in various languages (English,
German, and French), as well as Twitter data.
</bodyText>
<sectionHeader confidence="0.987625" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999876666666667">
In this section, we first discuss the background for
sequential predictions then describe how to formulate
online language modeling as sequential predictions.
</bodyText>
<subsectionHeader confidence="0.974829">
2.1 Sequential Predictions
</subsectionHeader>
<bodyText confidence="0.999011379310345">
Let w1, w2, ... , wT be a sequence of response vari-
ables, revealed one at a time. The goal is to design
a good learner to predict the next response, given
previous responses and additional evidence which
we denote by xt E RM (at time t). Throughout this
paper, we use the term features for x. Specifically, at
each round t, the learner receives xt and makes a pre-
diction ˆwt, by choosing a parameter vector αt E RM.
In this paper, we refer to α as feature coefficients.
There has been an enormous amount of work on
online learning for sequential predictions, much of it
building on convex optimization. For a sequence of
loss functions `1, `2, . . . , `T (parameterized by α),
an online learning algorithm is a strategy to minimize
the regret, with respect to the best fixed α∗ in hind-
sight.1 Regret guarantees assume a Lipschitz con-
1Formally, the regret is defined as RegretT(α*) =
dition on the loss function ` that can be prohibitive
for complex models. See Cesa-Bianchi and Lugosi
(2006), Rakhlin (2009), Bubeck (2011), and Shalev-
Shwartz (2012) for in-depth discussion and review.
There has also been work on online and stochastic
learning for Bayesian models (Sato, 2001; Honkela
and Valpola, 2003; Hoffman et al., 2013), based on
variational inference. The goal is to approximate pos-
terior distributions of latent variables when examples
arrive one at a time.
In this paper, we will use both kinds of techniques
to learn language models for streaming datasets.
</bodyText>
<subsectionHeader confidence="0.999586">
2.2 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.9951174375">
Consider an online language modeling problem, in
the spirit of sequential predictions. The task is to
build a language model that accurately predicts the
texts generated on day t, conditioned on observ-
able features up to day t, x1:t. Every day, after
the model makes a prediction, the actual texts wt
are revealed and we suffer a loss. The loss is de-
fined as the negative log likelihood of the model
`t = − logp(wt  |α, 131:t−1, x1:t−1, n1:t−1), where
α and 131:T are the model parameters and n is a back-
ground distribution (details are given in §3.2). We
can then update the model and proceed to day t + 1.
Notice the similarity to the sequential prediction de-
scribed above. Importantly, this is a realistic setup for
building evolving language models from large-scale
streaming datasets.
</bodyText>
<sectionHeader confidence="0.986801" genericHeader="method">
3 Model
</sectionHeader>
<subsectionHeader confidence="0.988921">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.999589333333333">
We index timesteps by t E {1, ... , T } and word
types by v E {1, ... , V }, both are always given as
subscripts. We denote vectors in boldface and use
1 : T as a shorthand for {1, 2, ... , T}. We assume
words of the form {wt}Tt=1 for wt E RV , which is
the vector of word frequences at timetstep t. Non-
linguistic context features are {xt}Tt=1 for xt E RM.
The goal is to learn parameters α and 131:T, which
will be described in detail next.
</bodyText>
<subsectionHeader confidence="0.998799">
3.2 Generative Story
</subsectionHeader>
<bodyText confidence="0.9991215">
The main idea of our model is illustrated by the fol-
lowing generative story for the unigram language
</bodyText>
<footnote confidence="0.476435">
PTt=1 $t(xt, αt, wt) − infα∗ PTt=1 $t(xt, α*, wt).
</footnote>
<page confidence="0.992309">
182
</page>
<bodyText confidence="0.998994333333333">
model. (We will discuss the extension to higher-order
language models later.) A graphical representation
of our proposed model is given in Figure 1.
</bodyText>
<listItem confidence="0.968933333333333">
1. Draw feature coefficients α - N(0, λI).2 Here
α is a vector in RM, where M is the dimension-
ality of the feature vector.
2. For each timestep t:
(a) Observe non-linguistic context features xt.
(b) Draw ,3t -
</listItem>
<equation confidence="0.987646333333333">
N−1 S exp(α&gt;f(xt,xk))
,3k,ϕI/
(Etk=1 k Lj_1 δj exp(α&gt;f(xt,xj)) .
</equation>
<bodyText confidence="0.99671">
Here, ,3t is a vector in RV , where V is
the size of the word vocabulary, W is
the variance parameter and Sk is a fixed
hyperparameter; we discuss them below.
</bodyText>
<listItem confidence="0.773572">
(c) For each word wt,v, draw wt,v �
</listItem>
<equation confidence="0.998775">
Categorical(Ej,V exp(n1:t−1,v+βt,v) )
exp(n1:t−1,j+βt,j) J
</equation>
<bodyText confidence="0.998838269230769">
In the last step, ,3t and n are mapped to the V-
dimensional simplex, forming a distribution over
words. n1:t−1 E RV is a background (log) distri-
bution, inspired by a similar idea in Eisenstein et al.
(2011). In this paper, we set n1:t−1,v to be the log-
frequency of v up to time t - 1. We can interpret ,3
as a time-dependent deviation from the background
log-frequencies that incorporates world-context. This
deviation comes in the form of a weighted average of
earlier deviation vectors.
The intuition behind the model is that the probabil-
ity of a word appearing at day t depends on the back-
ground log-frequencies, the deviation coefficients of
the word at previous timesteps ,31:t−1, and the sim-
ilarity of current conditions of the world (based on
observable features x) to previous timesteps through
f(xt, xk). That is, f is a function that takes d-
dimensional feature vectors at two timesteps xt and
xk and returns a similarity vector f(xt, xk) E RM
(see §6.1.1 for an example of f that we use in our
experiments). The similarity is parameterized by α,
and decays over time with rate Sk. In this work, we
assume a fixed window size c (i.e., we consider c
most recent timesteps), so that S1:t−c−1 = 0 and
St−c:t−1 = 1. This allows up to cth order depen-
dencies.3 Setting S this way allows us to bound the
</bodyText>
<footnote confidence="0.9971495">
2Feature coefficients α can be also drawn from other distri-
butions such as α ∼ Laplace(0, A).
3In online Bayesian learning, it is known that forgetting
inaccurate estimates from earlier timesteps is important (Sato,
</footnote>
<figure confidence="0.519012777777778">
wq
xq
)3q )3 13 13t
q
W w wt
↵
X xt
t
T
</figure>
<figureCaption confidence="0.92667">
Figure 1: Graphical representation of the model. The
</figureCaption>
<bodyText confidence="0.985294806451613">
subscript indices q, r, s are shorthands for the previ-
ous timesteps t - 3, t - 2, t - 1. Only four timesteps
are shown here. There are arrows from previous
,3t−4,,3t−5, . . . , ,3t−c to ,3t, where c is the window size
as described in §3.2. They are not shown here, for read-
ability.
number of past vectors ,3 that need to be kept in
memory. We set ,30 to 0.
Although the generative story described above
is for unigram language models, extensions can be
made to more complex models (e.g., mixture of un-
igrams, topic models, etc.) and to longer n-gram
contexts. In the case of topic models, the model
will be related to dynamic topic models (Blei and
Lafferty, 2006) augmented by context features, and
the learning procedure in §4 can be used to perform
online learning of dynamic topic models. However,
our model captures longer-range dependencies than
dynamic topic models, and can condition on non-
linguistic features or metadata. In the case of higher-
order n-grams, one simple way is to draw more ,3,
one for each history. For example, for a bigram
model, ,3 is in RV 2, rather than RV in the unigram
model. We consider both unigram and bigram lan-
guage models in our experiments in §6. However, the
main idea presented in this paper is largely indepen-
dent of the base model.
Related work. Mimno and McCallum (2008) and
Eisenstein et al. (2010) similarly conditioned text on
2001; Honkela and Valpola, 2003). Since we set S1:t−c−1 = 0,
at every timestep t, Sk leads to forgetting older examples.
</bodyText>
<page confidence="0.995672">
183
</page>
<bodyText confidence="0.999917333333333">
observable features (e.g., author, publication venue,
geography, and other document-level metadata), but
conducted inference in a batch setting, thus their ap-
proaches are not suitable for streaming data. It is not
immediately clear how to generalize their approach to
dynamic settings. Algorithmically, our work comes
closest to the online dynamic topic model of Iwata
et al. (2010), except that we also incorporate context
features.
</bodyText>
<sectionHeader confidence="0.847622" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<bodyText confidence="0.9988335">
The goal of the learning procedure is to minimize the
overall negative log likelihood,
</bodyText>
<equation confidence="0.999955">
− log L(D) =
Z− log dβ1:Tp(β1:T  |α, x1:T)p(w1:T  |β1:T, n).
</equation>
<bodyText confidence="0.9993456">
However, this quantity is intractable. Instead, we
derive an upper bound for this quantity and minimize
that upper bound. Using Jensen’s inequality, the vari-
ational upper bound on the negative log likelihood
is:
</bodyText>
<equation confidence="0.999871">
Z− log L(D) ≤ − dβ1:Tq(β1:T  |γ1:T) (4)
log p(β1:T  |α, x1:T)p(w1:T  |β1:T, n)
q(β1:T  |γ1:T)
</equation>
<bodyText confidence="0.99997125">
Specifically, we use mean-field variational inference
where the variables in the variational distribution q
are completely independent. We use Gaussian distri-
butions as our variational distributions for β, denoted
by γ in the bound in Eq. 4. We denote the parameters
of the Gaussian variational distribution for βt,v (word
v at timestep t) by µt,v (mean) and σt,v (variance).
Figure 2 shows the functional form of the varia-
tional bound that we seek to minimize, denoted by ˆB.
The two main steps in the optimization of the bound
are inferring βt and updating feature coefficients α.
We next describe each step in detail.
</bodyText>
<subsectionHeader confidence="0.99828">
4.1 Learning
</subsectionHeader>
<bodyText confidence="0.978112346153846">
The goal of the learning procedure is to minimize the
upper bound in Figure 2 with respect to α. However,
since the data arrives in an online fashion, and speed
is very important for processing streaming datasets,
the model needs to be updated at every timestep t (in
our experiments, daily).
Notice that at timestep t, we only have access
to x1:t and w1:t, and we perform learning at every
timestep after the text for the current timestep wt
is revealed. We do not know xt+1:T and wt+1:T.
Nonetheless, we want to update our model so that
it can make a better prediction at t + 1. Therefore,
we can only minimize the bound until timestep t.
Let Ck °_ Pt-p(αpf(Tf(xt)xj)) Our learning al-
gorithm is a variational Expectation-Maximization
algorithm (Wainwright and Jordan, 2008).
E-step Recall that we use variational inference and
the variational parameters for β are µ and σ. As
shown in Figure 2, since the log-sum-exp in the last
term of B is problematic, we introduce additional
variational parameters ζ to simplify B and obtain
Bˆ (Eqs. 2–3). The E-step deals with all the local
variables µ, σ, and ζ.
Fixing other variables and taking the derivative
of the bound Bˆ w.r.t. ζt and setting it to zero,
we obtain the closed-form update for ζt: ζt =
</bodyText>
<equation confidence="0.9922135">
σtv)
Pv∈V exp (n1:t−1,v) exp µt,v + 2
</equation>
<bodyText confidence="0.914039375">
To minimize with respect to µt and σt, we apply
gradient-based methods since there are no closed-
form solutions. The derivative w.r.t. µt,v is:
µt,v − Ckµk,v
Although we require iterative methods in the E-step,
we find it to be reasonably fast in practice.4 Specifi-
cally, we use the L-BFGS quasi-Newton algorithm
(Liu and Nocedal, 1989).
We can further improve the bound by updating
the variational parameters for timestep 1 : t − 1, i.e.,
µ1:t−1 and σ1:t−1, as well. However, this will require
storing the texts from previous timesteps. Addition-
ally, this will complicate the M-step update described
4Approximately 16.5 seconds/day (walltime) to learn the
model on the EN:NA dataset on a 2.40GHz CPU with 24GB
memory.
</bodyText>
<equation confidence="0.963998833333333">
.
∂ Bˆ
=
∂µt,v
ϕ � �
− nt,v + nt µt,v + σt,v
</equation>
<bodyText confidence="0.832323">
exp (n1:t−1,v) exp ,
ζt 2
where nt = Pv∈V nt,v.
The derivative w.r.t. σt,v is:
</bodyText>
<equation confidence="0.755048352941177">
∂ Bˆ 1 1 nt exp (n1:t−1,v) exp (µt,v + σt,v �.
= + + 2ζt 2
∂σt,v 2σt,v 2ϕ
184
B = − T Eq[log p(pt  |pk, α, xt)] − T Eq[logp(wt  |pt, nt)] − H(q) (1)
X X
t=1 t=1
⎤ ⎡
⎥ ⎣X X
⎦ − Eq n1:t−1,v + βt,v − log exp(n1:t−1,j + βt,j)
v∈wt j∈V
= T ⎧ 1X σt,j t−1 2
X ⎨⎪ 2 log − Eq (pt − Pk=t−c Ckpk)
t=1 ⎪⎩ j∈V ϕ
2ϕ
(2)
T ⎧ 1X σt,v t−1 2 t−1 2
</equation>
<table confidence="0.9910091">
≤X ⎨⎪ 2 log ϕ + (µt − Pk=t−c Ckµk) + Qt + Pk=t−c CkQk
t=1 ⎪⎩ j∈V 2ϕ
2ϕ
− vXt ⎛ ⎞ ⎫
� ⎪⎬
1 X �
⎝µt,v − log ζt − µt,j + σt,j
exp (n1:t−1,j) exp ⎠ + const (3)
ζt 2 ⎭⎪
j∈V
</table>
<figureCaption confidence="0.8261198">
Figure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The
derivation from line 1 to line 2 is done by replacing the probability distributions p(pt  |pk, α, xt) and p(wt  |pt, nt)
by their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions
and further bound B by introducing additional variational parameters C using Jensen’s inequality on the log-sum-exp in
the last term. We denote the new bound ˆB.
</figureCaption>
<bodyText confidence="0.9997826">
below. Therefore, for each s &lt; t, we choose to fix
µs and Qs once they are learned at timestep s.
M-step In the M-step, we update the global pa-
rameter α, fixing µ1:t. Fixing other parameters and
taking the derivative of Bˆ w.r.t. α, we obtain:5
</bodyText>
<equation confidence="0.946369916666667">
(µt−P Etkt —c ∂Cke )
ϕ
+ Pt−1 ∂Ck
k =t−c CkQk ∂α
ϕ
where:
∂Ck =Ckf(xt, xk)
∂α
Ck Pt−1
Ps=t−c f (xt, xs) exp(α&gt;f (xt, xs))
t−1
s=t−c exp(α&gt;f(xt, xs))
</equation>
<bodyText confidence="0.998706">
We follow the convex optimization strategy and sim-
ply perform a stochastic gradient update: αt+1 =
</bodyText>
<equation confidence="0.868117">
αt + ηt ∂ Bˆ
∂αt (Zinkevich, 2003). While the variational
bound Bˆ is not convex, given the local variables µ1:t
</equation>
<bodyText confidence="0.9699535">
5In our implementation, we augment α with a squared L2
regularization term (i.e., we assume that α is drawn from a
normal distribution with mean zero and variance a) and use the
FOBOS algorithm (Duchi and Singer, 2009). The derivative
of the regularization term is simple and is not shown here. Of
course, other regularizers (e.g., the L1-norm, which we use for
other parameters, or the L1/∞-norm) can also be explored.
and Q1:t, optimizing α at timestep t without know-
ing the future becomes a convex problem.6 Since
we do not reestimate µ1:t−1 and Q1:t−1 in the E-step,
the choice to perform online gradient descent instead
of iteratively performing batch optimization at every
timestep is theoretically justified.
Notice that our overall learning procedure is still
to minimize the variational upper bound ˆB. All these
choices are made to make the model suitable for
learning in real time from large streaming datasets.
Preliminary experiments showed that performing
more than one EM iteration per day does not consid-
erably improve performance, so in our experiments
we perform one EM iteration per day.
To learn the parameters of the model, we rely on
approximations and optimize an upper bound ˆB. We
have opted for this approach over alternatives (such
as MCMC methods) because of our interest in the
online, large-data setting. Our experiments show that
we are still able to learn reasonable parameter esti-
mates by optimizing ˆB. Like online variational meth-
ods for other latent-variable models such as LDA
(Sato, 2001; Hoffman et al., 2013), open questions re-
main about the tightness of such approximations and
the identifiability of model parameters. We note, how-
</bodyText>
<footnote confidence="0.601133666666667">
6As a result, our algorithm is Hannan consistent w.r.t. the
best fixed α (for ˆB) in hindsight; i.e., the average regret goes to
zero as T goes to ∞.
</footnote>
<equation confidence="0.857206">
∂Bˆ
∂α =
,
.
</equation>
<page confidence="0.987679">
185
</page>
<bodyText confidence="0.9999085">
ever, that our model does not include latent mixtures
of topics and may be generally easier to estimate.
</bodyText>
<sectionHeader confidence="0.984567" genericHeader="method">
5 Prediction
</sectionHeader>
<bodyText confidence="0.982640125">
As described in §2.2, our model is evaluated by the
loss suffered at every timestep, where the loss is
defined as the negative log likelihood of the model
on text at timestep wt. Therefore, at each timestep t,
we need to predict (the distribution of) wt. In order
to do this, for each word v E V , we simply compute
the deviation means βt,v as weighted combinations
of previous means, where the weights are determined
by the world-context similarity encoded in x:
exp(α&gt;f(xt, xk))
�t−1 µk,v.
j=t−c exp(α&gt;f(xt, xj))
Recall that the word distribution that we use for
prediction is obtained by applying the operator 7r
that maps βt and n to the V-dimensional simplex,
forming a distribution over words: 7r(βt, n1:t_1)v =
</bodyText>
<equation confidence="0.750903">
exp(n1:t−1,v+βt,v) V
</equation>
<bodyText confidence="0.998511333333333">
exp(n1: , where n1:t_1,v E 8 is a
Pj∈Vbackground distribution (the log-frequency of word
v observed up to time t − 1).
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999983571428571">
In our experiments, we consider the problem of pre-
dicting economy-related text appearing in news and
microblogs, based on observable features that reflect
current economic conditions in the world at a given
time. In the following, we describe our dataset in de-
tail, then show experimental results on text prediction.
In all experiments, we set the window size c = 7 (one
</bodyText>
<equation confidence="0.9901325">
week) or c = 14 (two weeks), λ = 1
2|V  |(V is the
</equation>
<bodyText confidence="0.9969265">
size of vocabulary of the dataset under consideration),
and c o= 1.
</bodyText>
<subsectionHeader confidence="0.988043">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999585">
Our data contains metadata and text corpora. The
metadata is used as our features, whereas the text
corpora are used for learning language models and
predictions. The dataset (excluding Twitter) can
be downloaded at http://www.ark.cs.cmu.
edu/DynamicLM.
</bodyText>
<subsectionHeader confidence="0.819461">
6.1.1 Metadata
</subsectionHeader>
<bodyText confidence="0.99959525">
We use end-of-day stock prices gathered from
finance.yahoo.com for each stock included in
the Standard &amp; Poor’s 500 index (S&amp;P 500). The
index includes large (by market value) companies
listed on US stock exchanges.7 We calculate daily
(continuously compounded) returns for each stock, o:
ro,t = log Po,t − log Po,t_1, where Po,t is the closing
stock price.8 We make a simplifying assumption that
text for day t is generated after Po,t is observed.9
In general, stocks trade Monday to Friday (except
for federal holidays and natural disasters). For days
when stocks do not trade, we set ro,t = 0 for all
stocks since any price change is not observed.
We transform returns into similarity values as fol-
lows: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k)
and 0 otherwise. While this limits the model by ig-
noring the magnitude of price changes, it is still rea-
sonable to capture the similarity between two days.10
There are 500 stocks in the S&amp;P 500, so xt E 8500
and f(xt, xk) E 8500.
</bodyText>
<subsectionHeader confidence="0.548578">
6.1.2 Text data
</subsectionHeader>
<bodyText confidence="0.999953352941176">
We have five streams of text data. The first four
corpora are news streams tracked through Reuters.11
Two of them are written in English, North American
Business Report (EN:NA) and Japanese Investment
News (EN:JP). The remaining two are German Eco-
nomic News Service (DE, in German) and French
Economic News Service (FR, in French). For all four
of the Reuters streams, we collected news data over
a period of thirteen months (392 days), 2012-05-26
to 2013-06-21. See Table 1 for descriptive statistics
of these datasets. Numerical terms are mapped to a
single word, and all letters are downcased.
The last text stream comes from the Deca-
hose/Gardenhose stream from Twitter. We collected
public tweets that contain ticker symbols (i.e., sym-
bols that are used to denote stocks of a particular
company in a stock market), preceded by the dollar
</bodyText>
<footnote confidence="0.996165">
7For a list of companies listed in the S&amp;P 500 as of
2012, see http://en.wikipedia.org/wiki/List_
of_S\%26P_500_companies. This set was fixed during
the time periods of all our experiments.
8We use the “adjusted close” on Yahoo that includes interim
dividend cash flows and also adjusts for “splits” (changes in the
number of outstanding shares).
9This is done in order to avoid having to deal with hourly
timesteps. In addition, intraday price data is only available
through commercial data provided.
10Note that daily stock returns are equally likely to be positive
or negative and display little serial correlation.
11http://www.reuters.com
</footnote>
<equation confidence="0.9724215">
Eq[βt,v  |µt,vI = �t − 1
k=t−c
</equation>
<page confidence="0.995415">
186
</page>
<table confidence="0.999926142857143">
Dataset Total # Doc. Avg. # Doc. #Days Unigrams Bigrams
Total # Tokens Size Vocab. Total # Tokens Size Vocab.
EN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000
EN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000
FR 62,355 160 392 11,942,271 10,000 3,773,517 5,000
DE 51,515 132 392 9,027,823 10,000 3,499,965 5,000
Twitter 214,794 336 639 1,660,874 10,000 551,768 5,000
</table>
<tableCaption confidence="0.999938">
Table 1: Statistics about the datasets. Average number of documents (third column) is per day.
</tableCaption>
<bodyText confidence="0.99341175">
.
sign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These
tags are generally used to indicate tweets about the
stock market. We look at tweets from the period
2011-01-01 to 2012-09-30 (639 days). As a result,
we have approximately 100–800 tweets per day. We
tokenized the tweets using the CMU ARK TweetNLP
tools,12 numerical terms are mapped to a single word,
and all letters are downcased.
We perform two experiments using unigram and
bigram language models as the base models. For
each dataset, we consider the top 10,000 unigrams
after removing corpus-specific stopwords (the top
100 words with highest frequencies). For the bigram
experiments, we only use 5,000 words to limit the
number of unique bigrams so that we can simulate
experiments for the entire time horizon in a reason-
able amount of time. In standard open-vocabulary
language modeling experiments, the treatment of un-
known words deserves care. We have opted for a
controlled, closed-vocabulary experiment, since stan-
dard smoothing techniques will almost surely interact
with temporal dynamics and context in interesting
ways that are out of scope in the present work.
</bodyText>
<subsectionHeader confidence="0.99934">
6.2 Baselines
</subsectionHeader>
<bodyText confidence="0.99981425">
Since this is a forecasting task, at each timestep, we
only have access to data from previous timesteps.
Our model assumes that all words in all documents
in a corpus come from a single multinomial distri-
bution. Therefore, we compare our approach to the
corresponding base models (standard unigram and bi-
gram language models) over the same vocabulary (for
each stream). The first one maintains counts of every
word and updates the counts at each timestep. This
corresponds to a base model that uses all of the avail-
able data up to the current timestep (“base all”). The
second one replaces counts of every word with the
</bodyText>
<footnote confidence="0.274074">
12https://www.ark.cs.cmu.edu/TweetNLP
</footnote>
<bodyText confidence="0.999964666666667">
counts from the previous timestep (“base one”). Ad-
ditionally, we also compare with a base model whose
counts decay exponentially (“base exp”). That is, the
counts from previous timesteps decay by exp(−-ys),
where s is the distance between previous timesteps
and the current timestep and -y is the decay constant.
We set the decay constant -y = 1. We put a symmetric
Dirichlet prior on the counts (“add-one” smoothing);
this is analogous to our treatment of the background
frequencies n in our model. Note that our model,
similar to “base all,” uses all available data up to
timestep t − 1 when making predictions for timestep
t. The window size c only determines which previ-
ous timesteps’ models can be chosen for making a
prediction today. The past models themselves are es-
timated from all available data up to their respective
timesteps.
We also compare with two strong baselines: a lin-
ear interpolation of “base one” models for the past
week (“int. week”) and a linear interpolation of “base
all” and “base one” (“int one all”). The interpolation
weights are learned online using the normalized expo-
nentiated gradient algorithm (Kivinen and Warmuth,
1997), which has been shown to enjoy a stronger
regret guarantee compared to standard online gra-
dient descent for learning a convex combination of
weights.
</bodyText>
<subsectionHeader confidence="0.956928">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999961666666667">
We evaluate the perplexity on unseen dataset to eval-
uate the performance of our model. Specifically, we
use per-word predictive perplexity:
</bodyText>
<equation confidence="0.920503">
CT
perplexity = exp _ Et=1 log p(wt  |cx, x1:t, n1:t−1)
Et=1 Ej∈V wt,j
</equation>
<bodyText confidence="0.846312333333333">
Note that the denominator is the number of tokens
up to timestep T. Lower perplexity is better.
Table 2 and Table 3 show the perplexity results for
</bodyText>
<page confidence="0.959708">
187
</page>
<table confidence="0.998562666666667">
Dataset base all base one base exp int. week int. one all c = 7 c = 14
EN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285
EN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689
FR 3,603 3,910 3,678 3,625 3,416 3,404 3,438
DE 3,789 4,199 3,979 3,926 3,634 3,649 3,687
Twitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819
</table>
<tableCaption confidence="0.90641525">
Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in “base all,” “base
one,” and “base exp” are unigram language models. “int. week” is a linear interpolation of “base one” from the past
week. “int. one all” is a linear interpolation of “base one” and “base all”. The rightmost two columns are versions of
our model. Best results are highlighted in bold.
</tableCaption>
<table confidence="0.999687">
Dataset base all base one base exp int. week int. one all c = 7
EN:NA 242 2,229 1,880 2,200 244 223
EN:JP 185 2,101 1,726 2,050 189 167
FR 159 2,084 1,707 2,068 166 139
DE 268 2,634 2,267 2,644 282 243
Twitter 756 4,245 4,253 5,859 4,046 739
</table>
<tableCaption confidence="0.866976">
Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in “base all,” “base
one,” and “base exp” are bigram language models. “int. week” is a linear interpolation of “base one” from the past
week. “int. one all” is a linear interpolation of “base one” and “base all”. The rightmost column is a version of our
model with c = 7. Best results are highlighted in bold.
</tableCaption>
<bodyText confidence="0.999957947368421">
each of the datasets for unigram and bigram experi-
ments respectively. Our model outperformed other
competing models in all cases but one. Recall that we
only define the similarity function of world context
as: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k) and
0 otherwise. A better similarity function (e.g., one
that takes into account market size of the company
and the magnitude of increase or decrease in the stock
price) might be able to improve the performance fur-
ther. We leave this for future work. Furthermore,
the variations can be captured using models from the
past week. We discuss why increasing c from 7 to 14
did not improve performance of the model in more
detail in §6.4.
We can also see how the models performed over
time. Figure 4 traces perplexity for four Reuters news
stream datasets.13 We can see that in some cases the
performance of the “base all” model degraded over
time, whereas our model is more robust to temporal
</bodyText>
<footnote confidence="0.863971666666667">
13In both experiments, in order to manage the time and space
complexities of updating β, we apply a sparsity shrinkage tech-
nique by using OWL-QN (Andrew and Gao, 2007) when maxi-
mizing it, with regularization constant set to 1. Intuitively, this
is equivalent to encouraging the deviation vector to be sparse
(Eisenstein et al., 2011).
</footnote>
<bodyText confidence="0.998834117647059">
shifts.
In the bigram experiments, we only ran our model
with c = 7, since we need to maintain β in RV2,
instead of RV in the unigram model. The goal of
this experiment is to determine whether our method
still adds benefit to more expressive language mod-
els. Note that the weights of the linear interpolation
models are also learned in an online fashion since
there are no classical training, development, and test
sets in our setting. Since the “base one” model per-
formed poorly in this experiment, the performance of
the interpolated models also suffered. For example,
the “int. one all” model needed time to learn that the
“base one” model has to be downweighted (we started
with all interpolated models having uniform weights),
so it was not able to outperform even the “base all”
model.
</bodyText>
<subsectionHeader confidence="0.999154">
6.4 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.9966205">
It should not be surprising that conditioning on
world-context reduces perplexity (Cover and Thomas,
1991). A key attraction of our model, we believe, lies
in the ability to inspect its parameters.
Deviation coefficients. Inspecting the model al-
lows us to gain insight into temporal trends. We
</bodyText>
<page confidence="0.987903">
188
</page>
<figure confidence="0.996450666666667">
Twitter:Google Twitter:Microsoft
β
0.0 0.5 1.0 1.5 2.0
β
0.0 0.5 1.0 1.5
rMSNT
msft
#microsoft
microsoft
#goog
rGOOG
google+
@google
0 100 200 300 400 500 600 0 100 200 300 400 500 600
timestep timestep
</figure>
<figureCaption confidence="0.995789666666667">
Figure 3: Deviation coefficients β over time for Google- and Microsoft-related words on Twitter with unigram base
model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually
followed by increases in 0 of related words.
</figureCaption>
<bodyText confidence="0.999426757575758">
investigate the deviations learned by our model on the
Twitter dataset. Examples are shown in Figure 3. The
left plot shows 0 for four words related to Google:
goog, #goog, @google, google+. For compari-
son, we also show the return of Google stock for the
corresponding timestep (scaled by 50 and centered at
0.5 for readability, smoothed using loess (Cleveland,
1979), denoted by rGOOG in the plot). We can see
that significant changes of return of Google stocks
(e.g., the rGOOG spikes between timesteps 50–100,
150–200, 490–550 in the plot) occurred alongside
an increase in 0 of Google-related words. Similar
trends can also be observed for Microsoft-related
words in the right plot. The most significant loss of
return of Microsoft stocks (the downward spike near
timestep 500 in the plot) is followed by a sudden
sharp increase in 0 of the words #microsoft and
microsoft.
Feature coefficients. We can also inspect the
learned feature coefficients α to investigate which
stocks have higher associations with the text that
is generated. Our feature coefficients are designed
to reflect which changes (or lack of changes) in
stock prices influence the word distribution more,
not which stocks are talked about more often. We
find that the feature coefficients do not correlate with
obvious company characteristics like market capi-
talization (firm size). For example, on the Twitter
dataset with bigram base models, the five stocks with
the highest weights are: ConAgra Foods Inc., Intel
Corp., Bristol-Myers Squibb, Frontier Communica-
tions Corp., and Amazon.com Inc. Strongly negative
weights tended to align with streams with less activ-
</bodyText>
<figure confidence="0.472662">
time lags
</figure>
<figureCaption confidence="0.9986548">
Figure 5: Distributions of the selection probabilities of
models from the previous c = 14 timesteps, on the EN:NA
dataset with unigram base model. For simplicity, we show
E-step modes. The histogram shows that the model tends
to favor models from days closer to the current date.
</figureCaption>
<bodyText confidence="0.9999635625">
ity, suggesting that these were being used to smooth
across all c days of history. A higher weight for stock
o implies an increase in probability of choosing mod-
els from previous timesteps s, when the state of the
world for the current timestep t and timestep s is the
same (as represented by our similarity function) with
respect to stock o (all other things being equal), and
a decrease in probability for a lower weight.
Selected models. Besides feature coefficients, our
model captures temporal shift by modeling similar-
ity across the most recent c days. During inference,
our model weights different word distributions from
the past. The similarity is encoded in the pairwise
features f(xt, xk) and the parameters α. Figure 5
shows the distributions of the strongest-posterior
models from previous timesteps, based on how far
</bodyText>
<figure confidence="0.98870452">
1 2 3 4 5 6 7 8 9 10 11 12 13 14
frequency
0 20 40 60 80
189
EN:NA
perplexity �tSeo all
200 400 600 complete
0 50 100 150 200 250 300 350
timestep
EN:JP
perplexity �tSeo all
200 400 600 complete
0 50 100 150 200 250 300 350
timestep
FR
perplexity ct. one all
200 400 600 ompallete
0 50 100 150 200 250 300 350
timestep
DE
perplexity int. one all
300 500 700 base all
complete
0 50 100 150 200 250 300 350
timestep
</figure>
<figureCaption confidence="0.99977">
Figure 4: Perplexity over time for four Reuters news streams (c = 7) with bigram base models.
</figureCaption>
<page confidence="0.987296">
190
</page>
<bodyText confidence="0.99998084375">
in the past they are at the time of use, aggregated
across rounds on the EN:NA dataset, for window size
c = 14. It shows that the model tends to favor models
from days closer to the current date, with the t − 1
models selected the most, perhaps because the state
of the world today is more similar to dates closer to
today compare to more distant dates. The plot also
explains why increasing c from 7 to 14 did not im-
prove performance of the model, since most of the
variation in our datasets can be captured with models
from the past week.
Topics. Latent topic variables have often figured
heavily in approaches to dynamic language model-
ing. In preliminary experiments incorporating single-
membership topic variables (i.e., each document be-
longs to a single topic, as in a mixture of unigrams),
we saw no benefit to perplexity. Incorporating top-
ics also increases computational cost, since we must
maintain and estimate one language model per topic,
per timestep. It is straightforward to design mod-
els that incorporate topics with single- or mixed-
membership as in LDA (Blei et al., 2003), an in-
teresting future direction.
Potential applications. Dynamic language models
like ours can be potentially useful in many applica-
tions, either as a standalone language model, e.g.,
predictive text input, whose performance may de-
pend on the temporal dimension; or as a component
in applications like machine translation or speech
recognition. Additionally, the model can be seen as
a step towards enhancing text understanding with
numerical, contextual data.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999884">
We presented a dynamic language model for stream-
ing datasets that allows conditioning on observable
real-world context variables, exemplified in our ex-
periments by stock market data. We showed how to
perform learning and inference in an online fashion
for this model. Our experiments showed the predic-
tive benefit of such conditioning and online learning
by comparing to similar models that ignore temporal
dimensions and observable variables that influence
the text.
</bodyText>
<sectionHeader confidence="0.993972" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996697058823529">
The authors thank several anonymous reviewers for help-
ful feedback on earlier drafts of this paper and Brendan
O’Connor for help with collecting Twitter data. This re-
search was supported in part by Google, by computing
resources at the Pittsburgh Supercomputing Center, by
National Science Foundation grant IIS-1111142, AFOSR
grant FA95501010247, ONR grant N000140910758, and
by the Intelligence Advanced Research Projects Activ-
ity via Department of Interior National Business Center
contract number D12PC00347. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as nec-
essarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822741935484">
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of ll-regularized log-linear models. In Proc. of ICML.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proc. of ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
S´ebastien Bubeck. 2011. Introduction to online opti-
mization. Technical report, Department of Operations
Research and Financial Engineering, Princeton Univer-
sity.
Nicol`o Cesa-Bianchi and G´abor Lugosi. 2006. Prediction,
Learning, and Games. Cambridge University Press.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American Statistical Association, 74(368):829–836.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley &amp; Sons.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(7):2899–
2934.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc. of
ICML.
Amit Goyal, Hal Daume III, and Suresh Venkatasubrama-
nian. 2009. Streaming for large scale NLP: Language
modeling. In Proc. of HLT-NAACL.
</reference>
<page confidence="0.978796">
191
</page>
<reference confidence="0.999633705882352">
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. Jour-
nal of Machine Learning Research, 14:1303–1347.
Antti Honkela and Harri Valpola. 2003. On-line varia-
tional Bayesian learning. In Proc. of ICA.
Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and
Naonori Ueda. 2010. Online multiscale dynamic topic
models. In Proc. of KDD.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1–63.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Mining
of concurrent text and time series. In Proc. of KDD
Workshop on Text Mining.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for sta-
tistical machine translation. In Proc. of HLT-NAACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503–528.
David Mimno and Andrew McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In Proc. of UAI.
Alexander Rakhlin. 2009. Lecture notes on online learn-
ing. Technical report, Department of Statistics, The
Wharton School, University of Pennsylvania.
Masaaki Sato. 2001. Online model selection based on the
variational bayes. Neural Computation, 13(7):1649–
1681.
Shai Shalev-Shwartz. 2012. Online learning and online
convex optimization. Foundations and Trends in Ma-
chine Learning, 4(2):107–194.
Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1–2):1–305.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Proc.
of UAI.
Chong Wang, John Paisley, and David M. Blei. 2011. On-
line variational inference for the hierarchical Dirichlet
process. In Proc. of AISTATS.
Martin Zinkevich. 2003. Online convex programming
and generalized infinitesimal gradient ascent. In Proc.
of ICML.
</reference>
<page confidence="0.998186">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979995">
<title confidence="0.999906">Dynamic Language Models for Streaming Text</title>
<author confidence="0.998301">Chong Bryan R A Eric P</author>
<affiliation confidence="0.99763">of Computer School of Carnegie Mellon</affiliation>
<address confidence="0.993024">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999637857142857">We present a probabilistic language model that captures temporal dynamics and conditions on features. These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself. We learn our model in an efficient online fashion that is scalable for large, streaming data. With five streaming datasets from two different genres— economics news articles and social media—we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of ll-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="29939" citStr="Andrew and Gao, 2007" startWordPosition="5111" endWordPosition="5114">or future work. Furthermore, the variations can be captured using models from the past week. We discuss why increasing c from 7 to 14 did not improve performance of the model in more detail in §6.4. We can also see how the models performed over time. Figure 4 traces perplexity for four Reuters news stream datasets.13 We can see that in some cases the performance of the “base all” model degraded over time, whereas our model is more robust to temporal 13In both experiments, in order to manage the time and space complexities of updating β, we apply a sparsity shrinkage technique by using OWL-QN (Andrew and Gao, 2007) when maximizing it, with regularization constant set to 1. Intuitively, this is equivalent to encouraging the deviation vector to be sparse (Eisenstein et al., 2011). shifts. In the bigram experiments, we only ran our model with c = 7, since we need to maintain β in RV2, instead of RV in the unigram model. The goal of this experiment is to determine whether our method still adds benefit to more expressive language models. Note that the weights of the linear interpolation models are also learned in an online fashion since there are no classical training, development, and test sets in our setti</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of ll-regularized log-linear models. In Proc. of ICML. David M. Blei and John D. Lafferty. 2006. Dynamic topic models. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1269" citStr="Blei et al., 2003" startWordPosition="173" endWordPosition="176">ge, streaming data. With five streaming datasets from two different genres— economics news articles and social media—we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models. 1 Introduction Language models are a key component in many NLP applications, such as machine translation and exploratory corpus analysis. Language models are typically assumed to be static—the word-given-context distributions do not change over time. Examples include n-gram models (Jelinek, 1997) and probabilistic topic models like latent Dirichlet allocation (Blei et al., 2003); we use the term “language model” to refer broadly to probabilistic models of text. Recently, streaming datasets (e.g., social media) have attracted much interest in NLP. Since such data evolve rapidly based on events in the real world, assuming a static language model becomes unrealistic. In general, more data is seen as better, but treating all past data equally runs the risk of distracting a model with irrelevant evidence. On the other hand, cautiously using only the most recent data risks overfitting to short-term trends and missing important timeinsensitive effects (Blei and Lafferty, 20</context>
<context position="36065" citStr="Blei et al., 2003" startWordPosition="6155" endWordPosition="6158">iation in our datasets can be captured with models from the past week. Topics. Latent topic variables have often figured heavily in approaches to dynamic language modeling. In preliminary experiments incorporating singlemembership topic variables (i.e., each document belongs to a single topic, as in a mixture of unigrams), we saw no benefit to perplexity. Incorporating topics also increases computational cost, since we must maintain and estimate one language model per topic, per timestep. It is straightforward to design models that incorporate topics with single- or mixedmembership as in LDA (Blei et al., 2003), an interesting future direction. Potential applications. Dynamic language models like ours can be potentially useful in many applications, either as a standalone language model, e.g., predictive text input, whose performance may depend on the temporal dimension; or as a component in applications like machine translation or speech recognition. Additionally, the model can be seen as a step towards enhancing text understanding with numerical, contextual data. 7 Conclusion We presented a dynamic language model for streaming datasets that allows conditioning on observable real-world context varia</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Bubeck</author>
</authors>
<title>Introduction to online optimization.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Department of Operations Research and Financial Engineering, Princeton University.</institution>
<contexts>
<context position="6406" citStr="Bubeck (2011)" startWordPosition="1008" endWordPosition="1009"> E RM. In this paper, we refer to α as feature coefficients. There has been an enormous amount of work on online learning for sequential predictions, much of it building on convex optimization. For a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is to build a language model that a</context>
</contexts>
<marker>Bubeck, 2011</marker>
<rawString>S´ebastien Bubeck. 2011. Introduction to online optimization. Technical report, Department of Operations Research and Financial Engineering, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>G´abor Lugosi</author>
</authors>
<title>Prediction, Learning, and Games.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2865" citStr="Cesa-Bianchi and Lugosi, 2006" startWordPosition="427" endWordPosition="430">dence on which the language model depends. For example, when an important company’s price moves suddenly, the language model should be based not on the very recent history, but should be similar to the language model for a day when a similar change happened, since people are likely to say similar things (either about that company, or about conditions relevant to the change). Non-linguistic contexts such as stock price changes provide useful auxiliary information that might indicate the similarity of language models across different timesteps. We also turn to a fully online learning framework (Cesa-Bianchi and Lugosi, 2006) to deal with nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was </context>
<context position="6375" citStr="Cesa-Bianchi and Lugosi (2006)" startWordPosition="1002" endWordPosition="1005">rediction ˆwt, by choosing a parameter vector αt E RM. In this paper, we refer to α as feature coefficients. There has been an enormous amount of work on online learning for sequential predictions, much of it building on convex optimization. For a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is t</context>
</contexts>
<marker>Cesa-Bianchi, Lugosi, 2006</marker>
<rawString>Nicol`o Cesa-Bianchi and G´abor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William S Cleveland</author>
</authors>
<title>Robust locally weighted regression and smoothing scatterplots.</title>
<date>1979</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>74</volume>
<issue>368</issue>
<contexts>
<context position="32072" citStr="Cleveland, 1979" startWordPosition="5470" endWordPosition="5471">ents β over time for Google- and Microsoft-related words on Twitter with unigram base model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually followed by increases in 0 of related words. investigate the deviations learned by our model on the Twitter dataset. Examples are shown in Figure 3. The left plot shows 0 for four words related to Google: goog, #goog, @google, google+. For comparison, we also show the return of Google stock for the corresponding timestep (scaled by 50 and centered at 0.5 for readability, smoothed using loess (Cleveland, 1979), denoted by rGOOG in the plot). We can see that significant changes of return of Google stocks (e.g., the rGOOG spikes between timesteps 50–100, 150–200, 490–550 in the plot) occurred alongside an increase in 0 of Google-related words. Similar trends can also be observed for Microsoft-related words in the right plot. The most significant loss of return of Microsoft stocks (the downward spike near timestep 500 in the plot) is followed by a sudden sharp increase in 0 of the words #microsoft and microsoft. Feature coefficients. We can also inspect the learned feature coefficients α to investigat</context>
</contexts>
<marker>Cleveland, 1979</marker>
<rawString>William S. Cleveland. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74(368):829–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="31033" citStr="Cover and Thomas, 1991" startWordPosition="5294" endWordPosition="5297">ation models are also learned in an online fashion since there are no classical training, development, and test sets in our setting. Since the “base one” model performed poorly in this experiment, the performance of the interpolated models also suffered. For example, the “int. one all” model needed time to learn that the “base one” model has to be downweighted (we started with all interpolated models having uniform weights), so it was not able to outperform even the “base all” model. 6.4 Analysis and Discussion It should not be surprising that conditioning on world-context reduces perplexity (Cover and Thomas, 1991). A key attraction of our model, we believe, lies in the ability to inspect its parameters. Deviation coefficients. Inspecting the model allows us to gain insight into temporal trends. We 188 Twitter:Google Twitter:Microsoft β 0.0 0.5 1.0 1.5 2.0 β 0.0 0.5 1.0 1.5 rMSNT msft #microsoft microsoft #goog rGOOG google+ @google 0 100 200 300 400 500 600 0 100 200 300 400 500 600 timestep timestep Figure 3: Deviation coefficients β over time for Google- and Microsoft-related words on Twitter with unigram base model (c = 7). Significant changes (increases or decreases) in the returns of Google and Mi</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>10</volume>
<issue>7</issue>
<pages>2934</pages>
<contexts>
<context position="17612" citStr="Duchi and Singer, 2009" startWordPosition="3034" endWordPosition="3037"> the derivative of Bˆ w.r.t. α, we obtain:5 (µt−P Etkt —c ∂Cke ) ϕ + Pt−1 ∂Ck k =t−c CkQk ∂α ϕ where: ∂Ck =Ckf(xt, xk) ∂α Ck Pt−1 Ps=t−c f (xt, xs) exp(α&gt;f (xt, xs)) t−1 s=t−c exp(α&gt;f(xt, xs)) We follow the convex optimization strategy and simply perform a stochastic gradient update: αt+1 = αt + ηt ∂ Bˆ ∂αt (Zinkevich, 2003). While the variational bound Bˆ is not convex, given the local variables µ1:t 5In our implementation, we augment α with a squared L2 regularization term (i.e., we assume that α is drawn from a normal distribution with mean zero and variance a) and use the FOBOS algorithm (Duchi and Singer, 2009). The derivative of the regularization term is simple and is not shown here. Of course, other regularizers (e.g., the L1-norm, which we use for other parameters, or the L1/∞-norm) can also be explored. and Q1:t, optimizing α at timestep t without knowing the future becomes a convex problem.6 Since we do not reestimate µ1:t−1 and Q1:t−1 in the E-step, the choice to perform online gradient descent instead of iteratively performing batch optimization at every timestep is theoretically justified. Notice that our overall learning procedure is still to minimize the variational upper bound ˆB. All th</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10(7):2899– 2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="9187" citStr="Eisenstein et al. (2011)" startWordPosition="1504" endWordPosition="1507">ture vector. 2. For each timestep t: (a) Observe non-linguistic context features xt. (b) Draw ,3t - N−1 S exp(α&gt;f(xt,xk)) ,3k,ϕI/ (Etk=1 k Lj_1 δj exp(α&gt;f(xt,xj)) . Here, ,3t is a vector in RV , where V is the size of the word vocabulary, W is the variance parameter and Sk is a fixed hyperparameter; we discuss them below. (c) For each word wt,v, draw wt,v � Categorical(Ej,V exp(n1:t−1,v+βt,v) ) exp(n1:t−1,j+βt,j) J In the last step, ,3t and n are mapped to the Vdimensional simplex, forming a distribution over words. n1:t−1 E RV is a background (log) distribution, inspired by a similar idea in Eisenstein et al. (2011). In this paper, we set n1:t−1,v to be the logfrequency of v up to time t - 1. We can interpret ,3 as a time-dependent deviation from the background log-frequencies that incorporates world-context. This deviation comes in the form of a weighted average of earlier deviation vectors. The intuition behind the model is that the probability of a word appearing at day t depends on the background log-frequencies, the deviation coefficients of the word at previous timesteps ,31:t−1, and the similarity of current conditions of the world (based on observable features x) to previous timesteps through f(x</context>
<context position="30105" citStr="Eisenstein et al., 2011" startWordPosition="5137" endWordPosition="5140">f the model in more detail in §6.4. We can also see how the models performed over time. Figure 4 traces perplexity for four Reuters news stream datasets.13 We can see that in some cases the performance of the “base all” model degraded over time, whereas our model is more robust to temporal 13In both experiments, in order to manage the time and space complexities of updating β, we apply a sparsity shrinkage technique by using OWL-QN (Andrew and Gao, 2007) when maximizing it, with regularization constant set to 1. Intuitively, this is equivalent to encouraging the deviation vector to be sparse (Eisenstein et al., 2011). shifts. In the bigram experiments, we only ran our model with c = 7, since we need to maintain β in RV2, instead of RV in the unigram model. The goal of this experiment is to determine whether our method still adds benefit to more expressive language models. Note that the weights of the linear interpolation models are also learned in an online fashion since there are no classical training, development, and test sets in our setting. Since the “base one” model performed poorly in this experiment, the performance of the interpolated models also suffered. For example, the “int. one all” model ne</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse additive generative models of text. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daume</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="3556" citStr="Goyal et al. (2009)" startWordPosition="541" endWordPosition="544">adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 2003). To our knowledge,</context>
</contexts>
<marker>Goyal, Daume, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daume III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Hoffman</author>
<author>David M Blei</author>
<author>Chong Wang</author>
<author>John Paisley</author>
</authors>
<title>Stochastic variational inference.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--1303</pages>
<contexts>
<context position="3309" citStr="Hoffman et al. (2013)" startWordPosition="499" endWordPosition="502">y information that might indicate the similarity of language models across different timesteps. We also turn to a fully online learning framework (Cesa-Bianchi and Lugosi, 2006) to deal with nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguisti</context>
<context position="6609" citStr="Hoffman et al., 2013" startWordPosition="1038" endWordPosition="1041">or a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is to build a language model that accurately predicts the texts generated on day t, conditioned on observable features up to day t, x1:t. Every day, after the model makes a prediction, the actual texts wt are revealed and we suffer a loss</context>
<context position="18942" citStr="Hoffman et al., 2013" startWordPosition="3249" endWordPosition="3252">iminary experiments showed that performing more than one EM iteration per day does not considerably improve performance, so in our experiments we perform one EM iteration per day. To learn the parameters of the model, we rely on approximations and optimize an upper bound ˆB. We have opted for this approach over alternatives (such as MCMC methods) because of our interest in the online, large-data setting. Our experiments show that we are still able to learn reasonable parameter estimates by optimizing ˆB. Like online variational methods for other latent-variable models such as LDA (Sato, 2001; Hoffman et al., 2013), open questions remain about the tightness of such approximations and the identifiability of model parameters. We note, how6As a result, our algorithm is Hannan consistent w.r.t. the best fixed α (for ˆB) in hindsight; i.e., the average regret goes to zero as T goes to ∞. ∂Bˆ ∂α = , . 185 ever, that our model does not include latent mixtures of topics and may be generally easier to estimate. 5 Prediction As described in §2.2, our model is evaluated by the loss suffered at every timestep, where the loss is defined as the negative log likelihood of the model on text at timestep wt. Therefore, a</context>
</contexts>
<marker>Hoffman, Blei, Wang, Paisley, 2013</marker>
<rawString>Matt Hoffman, David M. Blei, Chong Wang, and John Paisley. 2013. Stochastic variational inference. Journal of Machine Learning Research, 14:1303–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti Honkela</author>
<author>Harri Valpola</author>
</authors>
<title>On-line variational Bayesian learning.</title>
<date>2003</date>
<booktitle>In Proc. of ICA.</booktitle>
<contexts>
<context position="4137" citStr="Honkela and Valpola, 2003" startWordPosition="616" endWordPosition="619"> Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 2003). To our knowledge, our model is the first to bring together temporal dynamics, conditioning on nonlinguistic context, and scalable online learning suitable for streaming data and extensible to include topics and n-gram histories. The main idea of our model is independent of the choice of the base language model (e.g., unigrams, bigrams, topic models, etc.). In this paper, we focus on unigram and bigram language models in order to evaluate the basic idea on well understood models, and to show how it can be extended to higher-order n-grams. We leave extensions to topic models for future work. W</context>
<context position="6586" citStr="Honkela and Valpola, 2003" startWordPosition="1034" endWordPosition="1037">g on convex optimization. For a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is to build a language model that accurately predicts the texts generated on day t, conditioned on observable features up to day t, x1:t. Every day, after the model makes a prediction, the actual texts wt are reveal</context>
<context position="12003" citStr="Honkela and Valpola, 2003" startWordPosition="2012" endWordPosition="2015">pic models. However, our model captures longer-range dependencies than dynamic topic models, and can condition on nonlinguistic features or metadata. In the case of higherorder n-grams, one simple way is to draw more ,3, one for each history. For example, for a bigram model, ,3 is in RV 2, rather than RV in the unigram model. We consider both unigram and bigram language models in our experiments in §6. However, the main idea presented in this paper is largely independent of the base model. Related work. Mimno and McCallum (2008) and Eisenstein et al. (2010) similarly conditioned text on 2001; Honkela and Valpola, 2003). Since we set S1:t−c−1 = 0, at every timestep t, Sk leads to forgetting older examples. 183 observable features (e.g., author, publication venue, geography, and other document-level metadata), but conducted inference in a batch setting, thus their approaches are not suitable for streaming data. It is not immediately clear how to generalize their approach to dynamic settings. Algorithmically, our work comes closest to the online dynamic topic model of Iwata et al. (2010), except that we also incorporate context features. 4 Learning and Inference The goal of the learning procedure is to minimiz</context>
</contexts>
<marker>Honkela, Valpola, 2003</marker>
<rawString>Antti Honkela and Harri Valpola. 2003. On-line variational Bayesian learning. In Proc. of ICA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoharu Iwata</author>
<author>Takeshi Yamada</author>
<author>Yasushi Sakurai</author>
<author>Naonori Ueda</author>
</authors>
<title>Online multiscale dynamic topic models.</title>
<date>2010</date>
<booktitle>In Proc. of KDD.</booktitle>
<contexts>
<context position="12478" citStr="Iwata et al. (2010)" startWordPosition="2086" endWordPosition="2089"> base model. Related work. Mimno and McCallum (2008) and Eisenstein et al. (2010) similarly conditioned text on 2001; Honkela and Valpola, 2003). Since we set S1:t−c−1 = 0, at every timestep t, Sk leads to forgetting older examples. 183 observable features (e.g., author, publication venue, geography, and other document-level metadata), but conducted inference in a batch setting, thus their approaches are not suitable for streaming data. It is not immediately clear how to generalize their approach to dynamic settings. Algorithmically, our work comes closest to the online dynamic topic model of Iwata et al. (2010), except that we also incorporate context features. 4 Learning and Inference The goal of the learning procedure is to minimize the overall negative log likelihood, − log L(D) = Z− log dβ1:Tp(β1:T |α, x1:T)p(w1:T |β1:T, n). However, this quantity is intractable. Instead, we derive an upper bound for this quantity and minimize that upper bound. Using Jensen’s inequality, the variational upper bound on the negative log likelihood is: Z− log L(D) ≤ − dβ1:Tq(β1:T |γ1:T) (4) log p(β1:T |α, x1:T)p(w1:T |β1:T, n) q(β1:T |γ1:T) Specifically, we use mean-field variational inference where the variables i</context>
</contexts>
<marker>Iwata, Yamada, Sakurai, Ueda, 2010</marker>
<rawString>Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and Naonori Ueda. 2010. Online multiscale dynamic topic models. In Proc. of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1185" citStr="Jelinek, 1997" startWordPosition="162" endWordPosition="163">self. We learn our model in an efficient online fashion that is scalable for large, streaming data. With five streaming datasets from two different genres— economics news articles and social media—we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models. 1 Introduction Language models are a key component in many NLP applications, such as machine translation and exploratory corpus analysis. Language models are typically assumed to be static—the word-given-context distributions do not change over time. Examples include n-gram models (Jelinek, 1997) and probabilistic topic models like latent Dirichlet allocation (Blei et al., 2003); we use the term “language model” to refer broadly to probabilistic models of text. Recently, streaming datasets (e.g., social media) have attracted much interest in NLP. Since such data evolve rapidly based on events in the real world, assuming a static language model becomes unrealistic. In general, more data is seen as better, but treating all past data equally runs the risk of distracting a model with irrelevant evidence. On the other hand, cautiously using only the most recent data risks overfitting to sh</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyrki Kivinen</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Exponentiated gradient versus gradient descent for linear predictors.</title>
<date>1997</date>
<journal>Information and Computation,</journal>
<pages>132--1</pages>
<contexts>
<context position="26943" citStr="Kivinen and Warmuth, 1997" startWordPosition="4579" endWordPosition="4582">“base all,” uses all available data up to timestep t − 1 when making predictions for timestep t. The window size c only determines which previous timesteps’ models can be chosen for making a prediction today. The past models themselves are estimated from all available data up to their respective timesteps. We also compare with two strong baselines: a linear interpolation of “base one” models for the past week (“int. week”) and a linear interpolation of “base all” and “base one” (“int one all”). The interpolation weights are learned online using the normalized exponentiated gradient algorithm (Kivinen and Warmuth, 1997), which has been shown to enjoy a stronger regret guarantee compared to standard online gradient descent for learning a convex combination of weights. 6.3 Results We evaluate the perplexity on unseen dataset to evaluate the performance of our model. Specifically, we use per-word predictive perplexity: CT perplexity = exp _ Et=1 log p(wt |cx, x1:t, n1:t−1) Et=1 Ej∈V wt,j Note that the denominator is the number of tokens up to timestep T. Lower perplexity is better. Table 2 and Table 3 show the perplexity results for 187 Dataset base all base one base exp int. week int. one all c = 7 c = 14 EN:N</context>
</contexts>
<marker>Kivinen, Warmuth, 1997</marker>
<rawString>Jyrki Kivinen and Manfred K. Warmuth. 1997. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Matt Schmill</author>
<author>Dawn Lawrie</author>
<author>Paul Ogilvie</author>
<author>David Jensen</author>
<author>James Allan</author>
</authors>
<title>Mining of concurrent text and time series.</title>
<date>2000</date>
<booktitle>In Proc. of KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="5017" citStr="Lavrenko et al. (2000)" startWordPosition="768" endWordPosition="771">is independent of the choice of the base language model (e.g., unigrams, bigrams, topic models, etc.). In this paper, we focus on unigram and bigram language models in order to evaluate the basic idea on well understood models, and to show how it can be extended to higher-order n-grams. We leave extensions to topic models for future work. We propose a novel task to evaluate our proposed language model. The task is to predict economicsrelated text at a given time, taking into account the changes in stock prices up to the corresponding day. This can be seen an inverse of the setup considered by Lavrenko et al. (2000), where news is assumed to influence stock prices. We evaluate our model on economics news in various languages (English, German, and French), as well as Twitter data. 2 Background In this section, we first discuss the background for sequential predictions then describe how to formulate online language modeling as sequential predictions. 2.1 Sequential Predictions Let w1, w2, ... , wT be a sequence of response variables, revealed one at a time. The goal is to design a good learner to predict the next response, given previous responses and additional evidence which we denote by xt E RM (at time</context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul Ogilvie, David Jensen, and James Allan. 2000. Mining of concurrent text and time series. In Proc. of KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based randomised language models for SMT.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3507" citStr="Levenberg and Osborne (2009)" startWordPosition="531" endWordPosition="535">nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. line variational algorithms (Sato, 20</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Stream-based randomised language models for SMT. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="3535" citStr="Levenberg et al. (2010)" startWordPosition="537" endWordPosition="540">he data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 200</context>
</contexts>
<marker>Levenberg, Callison-Burch, Osborne, 2010</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="15260" citStr="Liu and Nocedal, 1989" startWordPosition="2560" endWordPosition="2563">rs ζ to simplify B and obtain Bˆ (Eqs. 2–3). The E-step deals with all the local variables µ, σ, and ζ. Fixing other variables and taking the derivative of the bound Bˆ w.r.t. ζt and setting it to zero, we obtain the closed-form update for ζt: ζt = σtv) Pv∈V exp (n1:t−1,v) exp µt,v + 2 To minimize with respect to µt and σt, we apply gradient-based methods since there are no closedform solutions. The derivative w.r.t. µt,v is: µt,v − Ckµk,v Although we require iterative methods in the E-step, we find it to be reasonably fast in practice.4 Specifically, we use the L-BFGS quasi-Newton algorithm (Liu and Nocedal, 1989). We can further improve the bound by updating the variational parameters for timestep 1 : t − 1, i.e., µ1:t−1 and σ1:t−1, as well. However, this will require storing the texts from previous timesteps. Additionally, this will complicate the M-step update described 4Approximately 16.5 seconds/day (walltime) to learn the model on the EN:NA dataset on a 2.40GHz CPU with 24GB memory. . ∂ Bˆ = ∂µt,v ϕ � � − nt,v + nt µt,v + σt,v exp (n1:t−1,v) exp , ζt 2 where nt = Pv∈V nt,v. The derivative w.r.t. σt,v is: ∂ Bˆ 1 1 nt exp (n1:t−1,v) exp (µt,v + σt,v �. = + + 2ζt 2 ∂σt,v 2σt,v 2ϕ 184 B = − T Eq[log </context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Topic models conditioned on arbitrary features with Dirichletmultinomial regression.</title>
<date>2008</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="11911" citStr="Mimno and McCallum (2008)" startWordPosition="1998" endWordPosition="2001">ures, and the learning procedure in §4 can be used to perform online learning of dynamic topic models. However, our model captures longer-range dependencies than dynamic topic models, and can condition on nonlinguistic features or metadata. In the case of higherorder n-grams, one simple way is to draw more ,3, one for each history. For example, for a bigram model, ,3 is in RV 2, rather than RV in the unigram model. We consider both unigram and bigram language models in our experiments in §6. However, the main idea presented in this paper is largely independent of the base model. Related work. Mimno and McCallum (2008) and Eisenstein et al. (2010) similarly conditioned text on 2001; Honkela and Valpola, 2003). Since we set S1:t−c−1 = 0, at every timestep t, Sk leads to forgetting older examples. 183 observable features (e.g., author, publication venue, geography, and other document-level metadata), but conducted inference in a batch setting, thus their approaches are not suitable for streaming data. It is not immediately clear how to generalize their approach to dynamic settings. Algorithmically, our work comes closest to the online dynamic topic model of Iwata et al. (2010), except that we also incorporate</context>
</contexts>
<marker>Mimno, McCallum, 2008</marker>
<rawString>David Mimno and Andrew McCallum. 2008. Topic models conditioned on arbitrary features with Dirichletmultinomial regression. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rakhlin</author>
</authors>
<title>Lecture notes on online learning.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, The Wharton School, University of Pennsylvania.</institution>
<contexts>
<context position="6391" citStr="Rakhlin (2009)" startWordPosition="1006" endWordPosition="1007">ameter vector αt E RM. In this paper, we refer to α as feature coefficients. There has been an enormous amount of work on online learning for sequential predictions, much of it building on convex optimization. For a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is to build a langua</context>
</contexts>
<marker>Rakhlin, 2009</marker>
<rawString>Alexander Rakhlin. 2009. Lecture notes on online learning. Technical report, Department of Statistics, The Wharton School, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Sato</author>
</authors>
<title>Online model selection based on the variational bayes.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<volume>13</volume>
<issue>7</issue>
<pages>1681</pages>
<contexts>
<context position="4109" citStr="Sato, 2001" startWordPosition="614" endWordPosition="615">e (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. Submitted 10/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 2003). To our knowledge, our model is the first to bring together temporal dynamics, conditioning on nonlinguistic context, and scalable online learning suitable for streaming data and extensible to include topics and n-gram histories. The main idea of our model is independent of the choice of the base language model (e.g., unigrams, bigrams, topic models, etc.). In this paper, we focus on unigram and bigram language models in order to evaluate the basic idea on well understood models, and to show how it can be extended to higher-order n-grams. We leave extensions to top</context>
<context position="6559" citStr="Sato, 2001" startWordPosition="1032" endWordPosition="1033">f it building on convex optimization. For a sequence of loss functions `1, `2, . . . , `T (parameterized by α), an online learning algorithm is a strategy to minimize the regret, with respect to the best fixed α∗ in hindsight.1 Regret guarantees assume a Lipschitz con1Formally, the regret is defined as RegretT(α*) = dition on the loss function ` that can be prohibitive for complex models. See Cesa-Bianchi and Lugosi (2006), Rakhlin (2009), Bubeck (2011), and ShalevShwartz (2012) for in-depth discussion and review. There has also been work on online and stochastic learning for Bayesian models (Sato, 2001; Honkela and Valpola, 2003; Hoffman et al., 2013), based on variational inference. The goal is to approximate posterior distributions of latent variables when examples arrive one at a time. In this paper, we will use both kinds of techniques to learn language models for streaming datasets. 2.2 Problem Formulation Consider an online language modeling problem, in the spirit of sequential predictions. The task is to build a language model that accurately predicts the texts generated on day t, conditioned on observable features up to day t, x1:t. Every day, after the model makes a prediction, the</context>
<context position="18919" citStr="Sato, 2001" startWordPosition="3247" endWordPosition="3248">tasets. Preliminary experiments showed that performing more than one EM iteration per day does not considerably improve performance, so in our experiments we perform one EM iteration per day. To learn the parameters of the model, we rely on approximations and optimize an upper bound ˆB. We have opted for this approach over alternatives (such as MCMC methods) because of our interest in the online, large-data setting. Our experiments show that we are still able to learn reasonable parameter estimates by optimizing ˆB. Like online variational methods for other latent-variable models such as LDA (Sato, 2001; Hoffman et al., 2013), open questions remain about the tightness of such approximations and the identifiability of model parameters. We note, how6As a result, our algorithm is Hannan consistent w.r.t. the best fixed α (for ˆB) in hindsight; i.e., the average regret goes to zero as T goes to ∞. ∂Bˆ ∂α = , . 185 ever, that our model does not include latent mixtures of topics and may be generally easier to estimate. 5 Prediction As described in §2.2, our model is evaluated by the loss suffered at every timestep, where the loss is defined as the negative log likelihood of the model on text at ti</context>
</contexts>
<marker>Sato, 2001</marker>
<rawString>Masaaki Sato. 2001. Online model selection based on the variational bayes. Neural Computation, 13(7):1649– 1681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
</authors>
<title>Online learning and online convex optimization. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<volume>4</volume>
<issue>2</issue>
<marker>Shalev-Shwartz, 2012</marker>
<rawString>Shai Shalev-Shwartz. 2012. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical models, exponential families, and variational inference. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="14412" citStr="Wainwright and Jordan, 2008" startWordPosition="2410" endWordPosition="2413">on, and speed is very important for processing streaming datasets, the model needs to be updated at every timestep t (in our experiments, daily). Notice that at timestep t, we only have access to x1:t and w1:t, and we perform learning at every timestep after the text for the current timestep wt is revealed. We do not know xt+1:T and wt+1:T. Nonetheless, we want to update our model so that it can make a better prediction at t + 1. Therefore, we can only minimize the bound until timestep t. Let Ck °_ Pt-p(αpf(Tf(xt)xj)) Our learning algorithm is a variational Expectation-Maximization algorithm (Wainwright and Jordan, 2008). E-step Recall that we use variational inference and the variational parameters for β are µ and σ. As shown in Figure 2, since the log-sum-exp in the last term of B is problematic, we introduce additional variational parameters ζ to simplify B and obtain Bˆ (Eqs. 2–3). The E-step deals with all the local variables µ, σ, and ζ. Fixing other variables and taking the derivative of the bound Bˆ w.r.t. ζt and setting it to zero, we obtain the closed-form update for ζt: ζt = σtv) Pv∈V exp (n1:t−1,v) exp µt,v + 2 To minimize with respect to µt and σt, we apply gradient-based methods since there are </context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Martin J. Wainwright and Michael I. Jordan. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1–2):1–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David M Blei</author>
<author>David Heckerman</author>
</authors>
<title>Continuous time dynamic topic models.</title>
<date>2008</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="1891" citStr="Wang et al., 2008" startWordPosition="275" endWordPosition="278"> use the term “language model” to refer broadly to probabilistic models of text. Recently, streaming datasets (e.g., social media) have attracted much interest in NLP. Since such data evolve rapidly based on events in the real world, assuming a static language model becomes unrealistic. In general, more data is seen as better, but treating all past data equally runs the risk of distracting a model with irrelevant evidence. On the other hand, cautiously using only the most recent data risks overfitting to short-term trends and missing important timeinsensitive effects (Blei and Lafferty, 2006; Wang et al., 2008). Therefore, in this paper, we take steps toward methods for capturing long-range temporal dynamics in language use. Our model also exploits observable context variables to capture temporal variation that is otherwise difficult to capture using only text. Specifically for the applications we consider, we use stock market data as exogenous evidence on which the language model depends. For example, when an important company’s price moves suddenly, the language model should be based not on the very recent history, but should be similar to the language model for a day when a similar change happene</context>
</contexts>
<marker>Wang, Blei, Heckerman, 2008</marker>
<rawString>Chong Wang, David M. Blei, and David Heckerman. 2008. Continuous time dynamic topic models. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>John Paisley</author>
<author>David M Blei</author>
</authors>
<title>Online variational inference for the hierarchical Dirichlet process.</title>
<date>2011</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<contexts>
<context position="3332" citStr="Wang et al. (2011)" startWordPosition="504" endWordPosition="507">ndicate the similarity of language models across different timesteps. We also turn to a fully online learning framework (Cesa-Bianchi and Lugosi, 2006) to deal with nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. A</context>
</contexts>
<marker>Wang, Paisley, Blei, 2011</marker>
<rawString>Chong Wang, John Paisley, and David M. Blei. 2011. Online variational inference for the hierarchical Dirichlet process. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Zinkevich</author>
</authors>
<title>Online convex programming and generalized infinitesimal gradient ascent.</title>
<date>2003</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="17315" citStr="Zinkevich, 2003" startWordPosition="2985" endWordPosition="2986">ing Jensen’s inequality on the log-sum-exp in the last term. We denote the new bound ˆB. below. Therefore, for each s &lt; t, we choose to fix µs and Qs once they are learned at timestep s. M-step In the M-step, we update the global parameter α, fixing µ1:t. Fixing other parameters and taking the derivative of Bˆ w.r.t. α, we obtain:5 (µt−P Etkt —c ∂Cke ) ϕ + Pt−1 ∂Ck k =t−c CkQk ∂α ϕ where: ∂Ck =Ckf(xt, xk) ∂α Ck Pt−1 Ps=t−c f (xt, xs) exp(α&gt;f (xt, xs)) t−1 s=t−c exp(α&gt;f(xt, xs)) We follow the convex optimization strategy and simply perform a stochastic gradient update: αt+1 = αt + ηt ∂ Bˆ ∂αt (Zinkevich, 2003). While the variational bound Bˆ is not convex, given the local variables µ1:t 5In our implementation, we augment α with a squared L2 regularization term (i.e., we assume that α is drawn from a normal distribution with mean zero and variance a) and use the FOBOS algorithm (Duchi and Singer, 2009). The derivative of the regularization term is simple and is not shown here. Of course, other regularizers (e.g., the L1-norm, which we use for other parameters, or the L1/∞-norm) can also be explored. and Q1:t, optimizing α at timestep t without knowing the future becomes a convex problem.6 Since we d</context>
</contexts>
<marker>Zinkevich, 2003</marker>
<rawString>Martin Zinkevich. 2003. Online convex programming and generalized infinitesimal gradient ascent. In Proc. of ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
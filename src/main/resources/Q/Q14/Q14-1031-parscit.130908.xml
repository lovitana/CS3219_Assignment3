<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997206">
Locally Non-Linear Learning for Statistical Machine Translation
via Discretization and Structured Regularization
</title>
<author confidence="0.984164">
Jonathan H. Clark∗ Chris Dyer† Alon Lavie†
</author>
<affiliation confidence="0.9321">
*Microsoft Research †Carnegie Mellon University
</affiliation>
<address confidence="0.873522">
Redmond, WA 98052, USA Pittsburgh, PA 15213, USA
</address>
<email confidence="0.998447">
jonathan.clark@microsoft.com {cdyer,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99788345">
Linear models, which support efficient learn-
ing and inference, are the workhorses of statis-
tical machine translation; however, linear de-
cision rules are less attractive from a modeling
perspective. In this work, we introduce a tech-
nique for learning arbitrary, rule-local, non-
linear feature transforms that improve model
expressivity, but do not sacrifice the efficient
inference and learning associated with linear
models. To demonstrate the value of our tech-
nique, we discard the customary log transform
of lexical probabilities and drop the phrasal
translation probability in favor of raw counts.
We observe that our algorithm learns a vari-
ation of a log transform that leads to better
translation quality compared to the explicit log
transform. We conclude that non-linear re-
sponses play an important role in SMT, an ob-
servation that we hope will inform the efforts
of feature engineers.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985710553191489">
Linear models using log-transformed probabilities
as features have emerged as the dominant model
in MT systems. This practice can be traced back
to the IBM noisy channel models (Brown et al.,
1993), which decompose decoding into the product
of a translation model (TM) and a language model
(LM), motivated by Bayes’ Rule. When Och and
Ney (2002) introduced a log-linear model for trans-
lation (a linear sum of log-space features), they
noted that the noisy channel model was a special
case of their model using log probabilities. This
∗This work was conducted as part of the first author’s Ph.D.
work at Carnegie Mellon University.
same formulation persisted even after the introduc-
tion of MERT (Och, 2003), which optimizes a lin-
ear model; again, using two log probability fea-
tures (TM and LM) with equal weight recovered the
noisy channel model. Yet systems now use many
more features, some of which are not even probabil-
ities. We no longer believe that equal weights be-
tween the TM and LM provides optimal translation
quality; the probabilities in the TM do not obey the
chain rule nor Bayes’ rule, nullifying several the-
oretical mathematical justifications for multiplying
probabilities. The story of multiplying probabilities
may just amount to heavily penalizing small values.
The community has abandoned the original mo-
tivations for a linear interpolation of two log-
transformed features. Is there empirical evidence
that we should continue using this particular trans-
formation? Do we have any reason to believe it is
better than other non-linear transformations? To an-
swer these, we explore the issue of non-linearity in
models for MT. In the process, we will discuss the
impact of linearity on feature engineering and de-
velop a general mechanism for learning a class of
non-linear transformations of real-valued features.
Applying a non-linear transformation such as
log to features is one way of achieving a non-linear
response function, even if those features are aggre-
gated in a linear model. Alternatively, we could
achieve a non-linear response using a natively non-
linear model such as a SVM (Wang et al., 2007) or
RankBoost (Sokolov et al., 2012). However, MT
is a structured prediction problem, in which a full
hypothesis is composed of partial hypotheses. MT
decoders take advantage of the fact that the model
</bodyText>
<page confidence="0.99474">
393
</page>
<bodyText confidence="0.95000804">
Transactions of the Association for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore.
Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
score decomposes as a linear sum over both local
features and partial hypotheses to efficiently per-
form inference in these structured spaces (§2) – cur-
rently, there are no scalable solutions to integrating
the hypothesis-level non-linear feature transforms
typically associated with kernel methods while still
maintaining polynomial time search. Another alter-
native is incorporating a recurrent neural network
(Schwenk, 2012; Auli et al., 2013; Kalchbrenner
and Blunsom, 2013) or an additive neural network
(Liu et al., 2013a). While these models have shown
promise as methods of augmenting existing mod-
els, they have not yet offered a path for replacing
or transforming existing real-valued features.
In this article, we discuss background (§2), de-
scribe local discretization, our approach to learning
non-linear transformations of individual features,
compare it with globally non-linear models (§3),
present our experimental setup (§5), empirically ver-
ify the importance of non-linear feature transforma-
tions in MT and demonstrate that discretization can
be used to recover non-linear transformations (§6),
discuss related work (§7), and conclude (§8).
</bodyText>
<sectionHeader confidence="0.959302" genericHeader="introduction">
2 Background and Definitions
</sectionHeader>
<subsectionHeader confidence="0.988979">
2.1 Feature Locality &amp; Structured Hypotheses
</subsectionHeader>
<bodyText confidence="0.9951195">
Decoding a given source sentence f can be ex-
pressed as search over target hypotheses e, each with
an associated complete derivation D. To find the
best-scoring hypothesis ˆe(f), a linear model applies
a set of weights w to a complete hypothesis’ feature
vector H:
</bodyText>
<equation confidence="0.985773">
wiHi(f, e, D) (1)
</equation>
<bodyText confidence="0.926117166666667">
However, this hides many of the realities of perform-
ing inference in modern decoders. Traditional in-
ference would be intractable if every feature were
allowed access to the entire derivation D and its as-
sociated target hypothesis e. Decoders take advan-
tage of the fact that features decompose over par-
tial derivations d. For a complete derivation D, the
global features H(D) are an efficient summation
over local features h(d):
=
ˆe(f)
arg max
e,D
This contrasts with non-local features such as the
language model (LM), which cannot be exactly cal-
culated given an arbitrary partial hypothesis, which
may lack both left and right context.1 Such features
require special handling including future cost esti-
mation. In this study, we limit ourselves to local
features, leaving the traditional non-local LM fea-
ture unchanged. In general, feature locality is rel-
ative to a particular structured hypothesis space,
and is unrelated to the structured features described
in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.999414">
2.2 Feature Non-Linearity and Separability
</subsectionHeader>
<bodyText confidence="0.9998811875">
Unlike models that rely primarily on a large number
of sparse indicator features, state-of-the-art machine
translation systems rely heavily on a small number
of dense real-valued features. However, unlike indi-
cator features, real-valued features may benefit from
non-linear transformations to allow a linear model to
better fit the data.
Decoders use a linear model to rank hypotheses,
selecting the highest-ranked derivation. Since the
absolute score of the model is irrelevant, non-linear
responses are useful only in cases where they elicit
novel rankings. In this section, we will discuss these
cases in terms of separability. Here, we are sepa-
rating the correctly ranked pairs of hypotheses from
the incorrect in the implicit pairwise rankings de-
fined by the total ordering on hypotheses provided
by our model.
When the local feature vectors h of each oracle-
best2 hypothesis (or hypotheses) are distinct from
those of all other competing hypotheses, we say that
the inputs are oracle separable given the feature set.
If there exists a weight vector that distinguishes the
oracle-best ranking from all other rankings under a
linear model, we say that the inputs are linearly sep-
arable given the feature set. If the inputs are ora-
cle separable but not linearly separable, we say that
there are non-linearities that are unexplained by the
feature set. For example, this can happen if a feature
is positively related to quality in some regions but
negatively related in other regions.
As we add more sentences to our corpus, sepa-
rability becomes increasingly difficult. For a given
</bodyText>
<footnote confidence="0.999725666666667">
1This is especially problematic for chart-based decoders.
2We define the oracle-best hypothesis in terms of some ex-
ternal quality measure such as BLEU
</footnote>
<equation confidence="0.99496925">
ˆe(f) = arg max
e,D
|H|
E
i=0
|H|
E Ewi
hi(d)
i=0 dED
� v �
Hi(D)
(2)
</equation>
<page confidence="0.98581">
394
</page>
<bodyText confidence="0.9998694">
corpus, if all hypotheses are oracle separable, we
can always produce the oracle translation – assum-
ing an optimal (and potentially very complex) model
and weight vector. If our hypothesis space also con-
tains all reference translations, we can always re-
cover the reference. In practice, both of these condi-
tions are typically violated to a certain degree. How-
ever, if we modify our feature set such that some
lower-ranked higher-quality hypothesis can be sepa-
rated from all higher-ranked lower-quality hypothe-
ses, then we can improve translation quality. For
this reason, we believe that separability remains an
informative tool for thinking about modeling in MT.
Currently, non-linearities in novel real-valued fea-
tures are typically addressed via manual feature en-
gineering involving a good deal of trial and error
(Gimpel and Smith, 2009)3 or by manually discretiz-
ing features (e.g. indicator features for count=N).
We will explore one technique for automatically
avoiding non-linearities in Section 3.
</bodyText>
<subsectionHeader confidence="0.999914">
2.3 Learning with Large Feature Sets
</subsectionHeader>
<bodyText confidence="0.999914631578947">
While MERT has proven to be a strong baseline, it
does not scale to larger feature sets in terms of both
inefficiency and overfitting. While MIRA (Chiang
et al., 2008), Rampion (Gimpel and Smith, 2012),
and HOLS (Flanigan et al., 2013) have been shown
to be effective over larger feature sets, they are dif-
ficult to explicitly regularize – this will become im-
portant in Section 4.2. Therefore, we use the PRO
optimizer (Hopkins and May, 2011) as our baseline
learner since it has been shown to perform compa-
rably to MERT for a small number of features, and
to significantly outperform MERT for a large num-
ber of features (Hopkins and May, 2011; Ganitke-
vitch et al., 2012). Other very recent MT optimiz-
ers such as the linear structured SVM (Cherry and
Foster, 2012), AROW (Chiang, 2012) and regular-
ized MERT (Galley et al., 2013) are also compatible
with the discretization and structured regularization
techniques described in this article.4
</bodyText>
<footnote confidence="0.9912374">
3Gimpel et al. eventually used raw probabilities in their
model rather than log-probabilities.
4Since we dispense with nearly all of the original dense fea-
tures and our structured regularizer is scale sensitive, one would
need to use the Bl-renormalized variant of regularized MERT.
</footnote>
<sectionHeader confidence="0.896628" genericHeader="method">
3 Discretization and Feature Induction
</sectionHeader>
<bodyText confidence="0.999953">
In this section, we propose a feature induction tech-
nique based on discretization that produces a feature
set that is less prone to non-linearities (see §2.2).
We define feature induction as a function 41)(y)
that takes the result of the feature function y =
h(x) E R and returns a tuple (y&apos;, j) where y&apos; E R is
a transformed feature value and j is the transformed
feature index.5 Building on equation 2, we can apply
feature induction as follows:
</bodyText>
<equation confidence="0.955948">
=
|H|
X
ˆe(f)
arg max
e,D dED
</equation>
<bodyText confidence="0.99423525">
At first glance, one might be tempted to sim-
ply choose some non-linear function for 41) (e.g.
log(x), exp(x), sin(x), xn). However, even if we
were to restrict ourselves to some “standard” set of
non-linear functions, many of these functions have
hyperparameters that are not directly tunable by con-
ventional optimizers (e.g.period and amplitude for
sin, n in xn).
</bodyText>
<figureCaption confidence="0.568528333333333">
Figure 1: Top: A traditional learning procedure, assign-
ing a set of weights to a fixed feature set. Bottom: Dis-
cretization, our feature induction technique, expands the
</figureCaption>
<bodyText confidence="0.944855">
feature set as part of learning, while still producing a lin-
ear model for inference, albeit with more features.
Discretization allows us to avoid many non-
linearities (§2.2) while preserving the fast inference
provided by feature locality (§2.1). We first dis-
cretize real-valued features into a set of indicator
</bodyText>
<footnote confidence="0.81006225">
5One could also imagine a feature transformation function
Φ that returns a vector of bins for a single value returned by a
feature function h or a transformation that has access to values
from multiple feature functions at once.
</footnote>
<figure confidence="0.993825647058824">
H
Original Linear Model: w • H
H
Learning
w
w&apos;
H&apos;
Induced Linear Model: w&apos; • H&apos;
Learning
Feature
Induction
X w&apos;jy&apos;
i=0
(y&apos;,j)=Φi(hi(d))
 |{z }
H&apos;(f,e,D)
(3)
</figure>
<page confidence="0.995399">
395
</page>
<bodyText confidence="0.973228176470588">
features and then use a conventional optimizer to
learn a weight for each indicator feature (Figure 1).
This technique is sometimes referred to as binning
and is closely related to quantization. Effectively,
discretization allows us to re-shape a feature func-
tion (Figure 2). In fact, given an infinite number of
bins, we can perform any non-linear transformation
of the original function.
Figure 2: Left: A real-valued feature. Bold dots repre-
sent points where we could imagine bins being placed.
However, since we may only adjust wo, these “bins” will
be rigidly fixed along the feature function’s value. Right:
After discretizing the feature into 4 bins, we may now
adjust 4 weights independently, to achieve a non-linear
re-shaping of the function.
For indicator discretization, we define Φi in terms
of a binning function BINi(x) E R → N:
</bodyText>
<equation confidence="0.999529">
Φi(x) = (1,i BINi(x)) (4)
</equation>
<bodyText confidence="0.999971333333333">
where the — operator indicates concatenation of a
feature identifier with a bin identifier to form a new,
unique feature identifier.
</bodyText>
<subsectionHeader confidence="0.998298">
3.1 Local Discretization
</subsectionHeader>
<bodyText confidence="0.99992537037037">
Unlike other approaches to non-linear learning in
MT, we perform non-linear transformation on par-
tial hypotheses as in equation 3 where discretiza-
tion is applied as Φi(hi(d)), which allows locally
non-linear transformations, instead of applying Φ to
complete hypotheses as in Φi(Hi(D)), which would
allow globally non-linear transformations. This en-
ables our transformed model to produce non-linear
responses with regard to the initial feature set H
while inference remains linear with regard to the
optimized parameters w&apos;. Importantly, our trans-
formed feature set requires no additional non-local
information for inference.
By performing transformations within a local
context, we effectively reinterpret the feature set.
For example, the familiar target word count feature
found in many modern MT systems is often concep-
tualized as “what is the count of target words in the
complete hypothesis?” A hypothesis-level view of
discretization would view this as “Did this hypoth-
esis have 5 target words?”. Only one such feature
will fire for each hypothesis. However, local dis-
cretization reinterprets this feature as “How many
phrases in the complete hypothesis have 1 target
word?” Many such features are likely to fire for
each hypothesis. We provide a further example of
this technique in Figure 3.
</bodyText>
<figureCaption confidence="0.793644666666667">
Figure 3: We perform discretization locally on each
grammar rule or phrase pair, operating on the local fea-
ture vectors h. In this example, the original real-valued
features are crossed out with a solid gray line and their
discretized indicator features are written above. When
forming a complete hypothesis from partial hypotheses,
we sum the counts of these indicator features to ob-
tain the complete feature vector H. In this example,
H = {HTM 0.1 : 2, HTM 0.2 : 1, HCount 2 : 1}
</figureCaption>
<bodyText confidence="0.999990692307692">
In terms of predictive power, this transformation
can provide the learned model with increased abil-
ity to discriminate between hypotheses. This is pri-
marily a result of moving to a higher-dimensional
feature space. As we introduce new parameters, we
expect that some hypotheses that were previously in-
distinguishable under H become separable under H&apos;
(§2.2). We show specific examples comparing lin-
ear, locally non-linear, and globally non-linear mod-
els in Figures 4 - 6. As seen in these examples, lo-
cally non-linear models (Eq. 3, 4) are not an approx-
imation nor a subset of globally non-linear models,
but rather a different class of models.
</bodyText>
<subsectionHeader confidence="0.999906">
3.2 Binning Algorithm
</subsectionHeader>
<bodyText confidence="0.998783">
To initialize the learning procedure, we construct
the binning function BIN used by the indicator di-
</bodyText>
<figure confidence="0.978462">
0.5 1.0
h1 h2 h3 h4
0.5
h0
1.0
hTM_0.1=1
hTM=0.113
hTM_0.1=1 hTM_0.2=1
hTM=0.1 hTM=0.2
hCount_2=1
hCount=2
el gato come furtivamente
</figure>
<page confidence="0.964056">
396
</page>
<table confidence="0.983534">
Linear Globally Non-Linear Locally Non-Linear
Ranking ∗S1 3 he says h:1.0 = {H:2.0} ∗S1 3 he says
h:1.0 said h:2.0 = {H:4.0} h1:1 h1:1 = {H1:2}
S2 3 she ∗S1 3 he says S2 3 she said
h:2.0 h:1.0 h:1.0 = {H2:1} h2:1 h2:1 = {H2:2}
S2 3 she said
h:2.0 h:2.0 = {H4:1}
S1 4 small kitten
h:2.0 h:2.0 = {H4:1}
∗S24 big lion
h:3.0 h:3.0 = {H6:1}
S1 4 small kitten S1 4 small kitten
h:2.0 h:2.0 = {H:4.0} h2:1 h2:1 = {H2:2}
∗S2 4 big lion ∗S2
h:3.0 h:3.0 = {H:6.0} 4 big lion
h3:1 h3:1 = {H3:2}
Pairs (S13, S23) {OH:-2.0} ⊕ (S13, S23) {OH2:1, OH4:-1} ⊕ (S13, S23) {OH1:2, OH2:-2}
(S2 3, S1 3) {OH:2.0} � (S2 3, S1 3) {OH2:-1, OH4:1} � (S2 3, S1 3) {OH1:-2, OH2:2}
(S24, S14) {OH:-2.0} e (S24, S14) {OH4:1, OH6:-1} e (S24, S14) {OH2:2, OH3:-2}
(S14, S24) {OH:2.0} ⊕ (S14, S24) {OH4:-1, OH6:1} ⊕ (S1 4, S2 4) {OH2:-2, OH3:2}
Pairwise O O Inseparable e ΔH4 ΔH2
Ranking ⊕ ΔH -1 1 UH6:-1 e 1 UH3:-1
2 0 2 ΔH2 ΔH1
1 -1 1
U 1(�i3:1 O+
Separable Separable
</table>
<figureCaption confidence="0.772108444444444">
Figure 4: An example showing a collinearity over multiple input sentences S3, S4 in which the oracle-best hypothesis
is “trapped” along a line with other lower quality hypotheses in the linear model’s output space. Ranking shows how
the hypotheses would appear in a k-best list with each partial derivation having its partial feature vector h under it; the
complete feature vector H is shown to the right of each hypothesis and the oracle-best hypothesis is notated with a ∗.
Pairs explicates the implicit pairwise rankings. Pairwise Ranking graphs those pairs in order to visualize whether or
not the hypotheses are separable. (⊕ indicates that the pair of hypotheses is ranked correctly according to the extrinsic
metric and e indicates the pair is ranked incorrectly. In the pairwise ranking row, some ⊕ and e points are annotated
with their positions along the third axis H3 (omitted for clarity). Collinearity can also occur with a single input having
at least 3 hypotheses.
</figureCaption>
<table confidence="0.997566818181818">
Linear Globally Non-Linear Locally Non-Linear
Ranking ∗S1 2 some things ∗S1 2 some things
∗S1 2 some things h:2.0 h:2.0 = {H4:1} h2:1 h2:1 = {H2:2}
h:2.0 h:2.0 = {H:4.0} S2 2 something S2 2 something
S2 2 something h:4.0 = {H4:1} h4:1 = {H4:1}
h:4.0 = {H:4.0}
Pairs (S12, S2 (S1 2, S2 2,(S1 S22) {OH2:2 OH4:-1}
2) {OH:0.0} ⊕ 2) {OH4:0} ⊕ (S22, S12) {OH2:-2, OH4:1}
(S2, S2) {OH:0.0} e (S2, S12) {OH4:0} e
Pairwise Inseparable Inseparable Separable
Ranking
</table>
<figureCaption confidence="0.9990862">
Figure 5: An example showing a trivial “collision” in which two hypotheses of differing quality receive the same
model score until local discretization is applied. The two hypotheses are indistinguishable under a linear model with
the feature set H, as shown by the zero-difference in the “pairs” row. While a globally non-linear transformation
does not yield any improvement, local discretization allows the hypotheses to be properly ranked due to the higher-
dimensional feature space H2, H4. See Figure 4 for an explanation of notation.
</figureCaption>
<figure confidence="0.999385833333333">
⊕
E)
E)
⊕
⊕
E)
</figure>
<page confidence="0.555396">
397
</page>
<figureCaption confidence="0.997491">
Figure 6: An example demonstrating a non-linear decision boundary induced by discretization. The non-linear nature
of the decision boundary can be seen clearly when the induced feature set HA1 , HB1 , HB−4 (right) is considered in the
original feature space HA, HB (left). In the pairwise ranking row, two axes (HA1 , HB1 ) are plotted while the third
axis HB−4 is indicated only as stand-off annotations for clarity . Given a larger number of hypotheses, such situations
could also arise within a single sentence. See Figure 4 for an explanation of notation.
</figureCaption>
<figure confidence="0.915883591836735">
Inseparable
Separable
Linear
*hB:1.0 = {HB:1.0}
hA:1.0 hA:1.0 = {HA:2.0}
*hA:1.0 hA:1.0 hB:1.0 = {HA:2.0, HB:1.0}
hA:0.0 = {}
* hB:-4.0 hB:1.0 hB:1.0 hB:1.0 = {HB:-1.0}
hA:1.0 = {HA:1.0}
*hB:-4.0 hA:1.0 = {HA:1.0, HB:-4.0}
hB:1.0 hB:1.0 = {HB:2.0}
Locally Non-Linear
*hB1 :1 = {HB1 :1}
A A A
h1 :1 h1 :1 = {H1 :2}
*hA1 :1 hA1 :1 hB1 :1 = {HA1 :2, HB1 :1}
hA1 :0 = {}
*hB�4:1 hB1 :1 hB1 :1 hB1 :1 = {HB4:1, HB1 :3}
hA1 :1 = {HA1 :1}
*hB�4:1 hA1 :1 = {HA1 :1, HB4:1}
B B B
h1 :1 h1 :1 = {H1 :2}
Ranking
HB:1-4
HB:-1-4
ΔHB1
⊖
⊕
-3 ⊖ ⊖ 3
⊕
⊖
⊕
-3
HB:1-4
HB:-1-4
ΔHA1
⊕3
ΔHB
6
⊖
Pairwise
Ranking
-6
⊕
⊕
⊕
-6 ⊖ ⊖⊖ 6
⊕
ΔHA
</figure>
<bodyText confidence="0.997957423076923">
cretizer Φ. We have two desiderata: (1) any mono-
tonic transformation of a feature should not affect
the induced binning since we should not require
feature engineers to determine the optimal feature
transformation and (2) no bin’s data should be so
sparse that the optimizer cannot reliably estimate a
weight for each bin. Therefore, we construct bins
that are (i) populated uniformly subject to (ii) each
bin containing no more than one feature value. We
call this approach uniform population feature bin-
ning. While one could consider the predictive power
of the features when determining bin boundaries,
this would suggest that we should jointly optimize
and determine bin boundaries, which is beyond the
scope of this work. This problem has recently been
considered for NLP by Suzuki and Nagata (2013)
and for MT by Liu et al. (2013b), though the latter
involves decoding the entire training data.
Let X be the list of feature values to bin where
i indexes feature values xi ∈ X and their associ-
ated frequencies fi. We want each bin to have a
uniform size u. For the sake of simplifying our fi-
nal algorithm, we first create adjusted frequencies
f&apos; i so that very frequent feature values will not oc-
cupy more than 100% of a bin via the following al-
gorithm, which iterates over k:
</bodyText>
<equation confidence="0.9812268">
fk (5)
i
fk+1
i = min(fki ,uk) (6)
which returns u&apos; = uk when fki &lt; uk ∀i. Next,
</equation>
<bodyText confidence="0.8977065">
we solve for a binning B of N bins where bj is the
population of each bin:
</bodyText>
<equation confidence="0.81636">
|bj − u&apos; |(7)
</equation>
<bodyText confidence="0.99994225">
We use Algorithm 1 to produce this binning. In our
experiments, we construct a translation model for
each sentence in our tuning corpus; we then add a
feature value instances to X for each rule instance.
</bodyText>
<equation confidence="0.6676696">
|X|
uk = 1
|X|
�
i=1
</equation>
<figure confidence="0.958580692307693">
1
N
N
E
j=1
arg min
B
Algorithm 1 POPULATEBINSUNIFORMLY(X, N)
&gt; Remaining values for bj, s.t. bk &gt; 0 bk
def R(j) = JXJ − (N − j − 1)
&gt; Remaining frequency mass within ideal bound
def C(j) = j · u0 − Ejk bk
i i 1 &gt; Current feature value
for j E [1, N] do
while i &lt; R(j) and fi &lt; C(j) do
bj i bj U {xi}
i i i + 1
end while
&gt; Handle value that straddles ideal boundaries
by minimizing its violation of the ideal
if i &lt; R(j) and fi−C(j) &lt; 0.5 then
bj i bj U {xi} f
i i i + 1
end if
end for
return B
</figure>
<sectionHeader confidence="0.787516" genericHeader="method">
4 Structured Regularization
</sectionHeader>
<bodyText confidence="0.999987">
Unfortunately, choosing the right number of bins
can have important effects on the model, including:
Fidelity. If we choose too few bins, we risk degrad-
ing the model’s performance by discarding impor-
tant distinctions encoded in fine differences between
the feature values. In the extreme, we could reduce
a real-valued feature to a single indicator feature.
Sparsity. If we choose too many bins, we risk mak-
ing each indicator feature too sparse, which is likely
to result in the optimizer overfitting such that we
generalize poorly to unseen data.
While one may be tempted to simply throw more
data or millions of sparse features at the problem, we
elect to more strategically use existing data, since (1)
large in-domain tuning data is not always available,
and (2) when it is available, it can add considerable
computational expense. In this section, we explore
methods for mitigating data sparsity by embedding
more knowledge into the learning procedure.
</bodyText>
<subsectionHeader confidence="0.996927">
4.1 Overlapping Bins
</subsectionHeader>
<bodyText confidence="0.999946333333333">
One very simplistic way we could combat sparsity is
to extend the edges of each bin such that they cover
their neighbors’ values (see Equation 4):
</bodyText>
<equation confidence="0.99923">
Φ0i(x) = (1, i _BINi(x)) if x E Ui+1
k=i−1BINk (8)
</equation>
<bodyText confidence="0.9994686">
This way, each bin will have more data points to
estimate its weight, reducing data sparsity, and the
bins will mutually constrain each other, reducing the
ability to overfit. We include this technique as a con-
trastive baseline for structured regularization.
</bodyText>
<subsectionHeader confidence="0.98944">
4.2 Linear Neighbor Regularization
</subsectionHeader>
<bodyText confidence="0.999940703703704">
Regularization has long been used to discourage op-
timization solutions that give too much weight to
any one feature. This encodes our prior knowl-
edge that such solutions are unlikely to generalize.
Regularization terms such as the `p norm are fre-
quently used in gradient-based optimizers including
our baseline implementation of PRO.
Unregularized discretization is potentially brittle
with regard to the number of bins chosen. Primar-
ily, it suffers from sparsity. At the same time, we
note that we know much more about discretized
features than initial features since we control how
they are formed. These features make up a struc-
tured feature space. With these things in mind, we
propose linear neighbor regularization, a structured
regularizer that embeds a small amount of knowl-
edge into the objective function: that the indicator
features resulting from the discretization of a sin-
gle real-valued feature are spatially related. We ex-
pect similar weights to be given to the indicator fea-
tures that represent neighboring values of the origi-
nal real-valued feature such that the resulting trans-
formation appears somewhat smooth.
To incorporate this knowledge of nearby bins, the
linear neighbor regularizer RLNR penalizes each fea-
ture’s weight by the squared amount it differs from
its neighbors’ midpoint:
</bodyText>
<equation confidence="0.9943634">
�j 1 2
RLNR (w, ✓) = C 1 (wj−1 + wj+1) − wj I (9)
|h|−1
RLNR(w) = β E RLNR(w, j) (10)
j=2
</equation>
<bodyText confidence="0.999698833333333">
This is a special case of the feature network reg-
ularizer of Sandler (2010). Unlike traditional regu-
larizers, we do not hope to reduce the active feature
count. With the PRO loss l and a `2 regularizater
R2, our final loss function internal to each iteration
of PRO is:
</bodyText>
<equation confidence="0.995838">
L(w) = l(x, y; w) + R2(w) + RLNR(w) (11)
</equation>
<page confidence="0.995463">
399
</page>
<subsectionHeader confidence="0.993849">
4.3 Monotone Neighbor Regularization
</subsectionHeader>
<bodyText confidence="0.989278666666667">
However, as 0 → oc, the linear neighbor regular-
izer RLNR forces a linear arrangement of weights –
this violates our premise that we should be agnos-
tic to non-linear transformations. We now describe a
structured regularizer RMNR whose limiting solution
is any monotone arrangement of weights. We aug-
ment RLNR with a smooth damping term D(w, j),
which has the shape of a bathtub curve with steep-
ness γ:
</bodyText>
<equation confidence="0.9989686">
D(w,j) = tanh2γ 2 (1jw1 + wj+1) − wj (12)
2 (wj−1 − wj+1)
|h|−1
RMNR(w) = 0 E D(w,j)RLNR(w,j) (13)
j=2
</equation>
<bodyText confidence="0.9945118">
D is nearly zero while wj E [wj−1, wj+1] and nearly
one otherwise. Briefly, the numerator measures how
far wj is from the midpoint of wj−1 and wj+1 while
the denominator scales that distance by the radius
from the midpoint to the neighboring weight.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="method">
5 Experimental Setup6
</sectionHeader>
<bodyText confidence="0.9999472">
Formalism: In our experiments, we use a hierarchi-
cal phrase-based translation model (Chiang, 2007).
A corpus of parallel sentences is first word-aligned,
and then phrase translations are extracted heuristi-
cally. In addition, hierarchical grammar rules are
extracted where phrases are nested. In general, our
choice of formalism is rather unimportant – our
techniques should apply to most common phrase-
based and chart-based paradigms including Hiero
and syntactic systems.
Decoder: For decoding, we will use cdec (Dyer et
al., 2010), a multi-pass decoder that supports syn-
tactic translation models and sparse features.
Optimizer: Optimization is performed using PRO
(Hopkins and May, 2011) as implemented by the
cdec decoder. We run PRO for 30 iterations as sug-
gested by Hopkins and May (2011). The PRO opti-
mizer internally uses a L-BFGS optimizer with the
default f2 regularization implemented in cdec. Any
additional regularization is explicitly noted.
</bodyText>
<footnote confidence="0.37002625">
Baseline Features: We use the baseline features
produced by Lopez’ suffix array grammar extrac-
tor (Lopez, 2008), which is distributed with cdec.
6Allcodeathttp://github.com/jhclark/cdec
</footnote>
<bodyText confidence="0.998727857142857">
Bidirectional lexical log-probabilities, the coher-
ent phrasal translation log-probability, target word
count, glue rule count, source OOV count, tar-
get OOV count, and target language model log-
probability. Note that these features may be sim-
plified or removed as specified in each experimental
condition.
</bodyText>
<table confidence="0.999316">
Zh→En Ar→En Cz→En
Train 303K 5.4M 1M
WeightTune 1664 1797 3000
HyperTune 1085 1056 2000
Test 1357 1313 2000
</table>
<tableCaption confidence="0.999979">
Table 1: Corpus statistics: number of parallel sentences.
</tableCaption>
<bodyText confidence="0.9993174">
Chinese Resources: For the Chinese→English ex-
periments, including the completed work presented
in this proposal, we train on the Foreign Broadcast
Information Service (FBIS) corpus of approximately
300,000 sentence pairs with about 9.4 million En-
glish words. We tune weights on the NIST MT 2006
dataset, tune hyperparameters on NIST MT05, and
test on NIST MT 2008.
Arabic Resources: We build an Arabic→English
system, training on the large NIST MT 2009 con-
strained training corpus of approximately 5 mil-
lion sentence pairs with about 181 million English
words. We tune weights on the NIST MT 2006
dataset, tune hyperparameters on NIST MT 2005,
and test on NIST MT 2008.
Czech resources: We also construct a
Czech→English system based on the CzEng
1.0 data (Bojar et al., 2012). First, we lowercased
and performed sentence-level deduplication of the
data.7 Then, we uniformly sampled a training set of
1M sentences (sections 1 – 97) along with a weight-
tuning set (section 98), hyperparameter-tuning
(section 99), and test set (section 99) from the
paraweb domain contained of CzEng.8 Sentences
less than 5 words were discarded due to noise.
Evaluation: We quantify increases in translation
quality using case-insensitive BLEU (Papineni et al.,
2002). We control for test set variation and opti-
mizer instability by averaging over multiple opti-
mizer replicas (Clark et al., 2011).9
</bodyText>
<footnote confidence="0.9934405">
7CzEng is distributed deduplicated at the document level,
leading to very high sentence-level overlap.
8The section splits recommended by Bojar et al. (2012).
9MultEval 0.5.1: github.com/jhclark/multeval
</footnote>
<page confidence="0.975326">
400
</page>
<table confidence="0.999325333333333">
Bits 4 8 12
Features 101 1302 12,910
Test BLEU 36.4 36.6 36.8
</table>
<tableCaption confidence="0.972469333333333">
Table 2: Translation quality for Cz-+En system with
varying bits for discretization. For all other experiments,
we tune the number of bits on held-out data.
</tableCaption>
<table confidence="0.99995125">
Condition Zh-+En Ar-+En Cz-+En
P 20.8* (-2.7) 44.3* (-3.6) 36.5* (-1.1)
log P 23.5† 47.9† 37.6†
Disc P 23.4† (-0.1) 47.2† (-0.7) 36.8* (-0.8)
Over. P 20.7* (-2.8) 44.6* (-3.3) 36.6* (-1.0)
LNR P 23.1*† (-0.4) 48.0† (+0.1) 37.3 (-0.3)
MNR P 23.8† (+0.3) 48.7*† (+0.8) 37.6† (±)
MNR C 23.6† (±) 48.7*† (+0.8) 37.4† (-0.2)
</table>
<tableCaption confidence="0.999397">
Table 3: Top: Translation quality for systems with and
</tableCaption>
<bodyText confidence="0.7877634">
without the typical log transform. Bottom: Transla-
tion quality for systems using discretization and struc-
tured regularization with probabilities P or counts C as
the input of discretization. MNR P consistently recovers
or outperforms a state-of-the-art system, but without any
assumptions about how to transform the initial features.
All scores are averaged over 3 end-to-end optimizer repli-
cations. * denotes significantly different than log probs
(row2) with p(CHANCE) &lt; 0.01 under Clark et al. (2011)
and † is likewise used with regard to P (row 1).
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999834">
6.1 Does Non-Linearity Matter?
</subsectionHeader>
<bodyText confidence="0.999839307692308">
In our first set of experiments, we seek to answer
“Does non-linearity matter?” by starting with our
baseline system of 7 typical features (the log Prob-
ability system) and we then remove the log trans-
form from all of the log probability features in our
grammar (the Probs. system). The results are shown
in Table 3 (rows 1, 2). If a naive feature engi-
neer were to remove the non-linear log transform,
the systems would degrade between 1.1 BLEU and
3.6 BLEU. From this, we conclude that non-linearity
does affect translation quality. This is a potential pit-
fall for any real-valued feature including probability
features, count features, similarity measures, etc.
</bodyText>
<subsectionHeader confidence="0.99991">
6.2 Learning Non-Linear Transformations
</subsectionHeader>
<bodyText confidence="0.999979104166667">
Next, we evaluate the effects of discretization
(Disc), overlapping bins (Over.), linear neighbor
regularization (LNR), and monotone neighbor reg-
ularization (MNR) on three language pairs: a small
Zh-+En system, a large Ar-+En system and a large
Cz-+En system. In the first row of Table 3, we use
raw probabilities rather than log probabilities for
pcoherent(t|s), plex(t|s), and plex(s|t). In rows 3 –
7, all translation model features (without the log-
transformed features) are then discretized into indi-
cator features.10 The number of bins and the struc-
tured regularization strength were tuned on the hy-
perparameter tuning set.
Discretization alone does not consistently recover
the performance of the log transformed features
(row 3). The naive overlap strategy in fact degrades
performance (row 4). Linear neighbor regularization
(row 5) behaves more consistently than discretiza-
tion alone, but is consistently outperformed by the
monotone neighbor regularizer (row 6), which is
able to meet or significantly exceed the performance
of the log transformed system. Importantly, this
is done without any knowledge of the correct non-
linear transformation. In the final row, we go a
step further by removing pcoherent(t|s) altogether and
replacing it with simple count features: c(s) and
c(s, t), with slight to no degradation in quality.11 We
take this as evidence that a feature engineer develop-
ing a new real-valued feature may find discretization
and monotone neighbor regularization useful.
We also observe that different data sets benefit
from non-linear feature transformation in to differ-
ent degrees (Table 3, rows 1, 2). We noticed that dis-
cretization with monotone neighbor regularization is
able to improve over a log transform (rows 2, 6) in
proportion to the improvement of a log transform
over probability-based features (rows 1, 2).
To provide insight into how translation quality can
be affected by the number of bits for discretization,
we offer Table 2.
In Figure 7, we present the weights learned by the
Ar-+En system for probability-based features. We
see that even without a bias toward a log transform,
a log-like shape still emerges for many SMT fea-
tures based only on the criteria of optimizing BLEU
and a preference for monotonicity. However, the op-
timizer chooses some important variations on the log
curve, especially for low probabilities, that lead to
</bodyText>
<footnote confidence="0.99012575">
10We also keep a real-valued copy of the word penalty to help
normalize the language model.
11These features can single-out rules with c(s) =
1, c(s, t) = 1, subsuming separate low-count features
</footnote>
<page confidence="0.995066">
401
</page>
<figureCaption confidence="0.9349004">
Original raw count feature value
Figure 7: Plots of weights learned for the discretized
pcoherent(e|f) (top) and c(f) (bottom) for the Ar→En sys-
tem with 4 bits and monotone neighbor regularization.
p(e|f) &gt; 0.11 is omitted for exposition as values were
constant after this point. The gray line fits a log curve to
the weights. The system learns a shape that deviates from
the log in several regions. Each non-monotonic segment
represents the learner choosing to better fit the data while
paying a strong regularization penalty.
</figureCaption>
<sectionHeader confidence="0.985741" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.8930878125">
Previous work on feature discretization in machine
learning has focused on the conversion of real-
valued features into discrete values for learners that
are either incapable of handling real-valued inputs
or perform suboptimally given real-valued inputs
(Dougherty et al., 1995; Kotsiantis and Kanellopou-
los, 2006). Decision trees and random forests have
been successfully used in language modeling (Je-
linek et al., 1994; Xu and Jelinek, 2004) and parsing
(Charniak, 2010; Magerman, 1995).
Kernel methods such as support vector machines
(SVMs) are often considered when non-linear inter-
actions between features are desired since they al-
low for easy usage of non-linear kernels. Wu et al.
(2004) showed improvements using non-linear ker-
nel PCA for word sense disambiguation. Taskar
et al. (2003) describes a method for incorporating
kernels into structured Markov networks. Tsochan-
taridis et al. (2004) then proposed a structured SVM
for grammar learning, named-entity recognition,
text classification, and sequence alignment. This
was followed by a structured SVM with inexact in-
ference (Finley and Joachims, 2008) and the latent
structured SVM (Yu and Joachims, 2009). Even
within kernel methods, learning non-linear map-
pings with kernels remains an open area of research;
For example, Cortes et al. (2009) investigated learn-
ing non-linear combinations of kernels. In MT,
Gim´enez and M`arquez (2007) used a SVM to an-
notate a phrase table with binary features indicating
whether or not a phrase translation was appropriate
in context. Nguyen et al. (2007) also applied non-
linear features for SMT n-best reranking.
Toutanova and Ahn (2013) use a form of regres-
sion decision trees to induce locally non-linear fea-
tures in a n-best reranking framework. He and Deng
(2012) directly optimize the lexical and phrasal fea-
tures using expected BLEU. Nelakanti et al. (2013)
use tree-structured ip regularizers to train language
models and improve perplexity over Kneser-Ney.
Learning parameters under weak order restric-
tions has also been studied for regression. Isotonic
regression (Barlow et al., 1972; Robertson et al.,
1988; Silvapulle and Sen, 2005) fits a curve to a set
of data points such that each point in the fitted curve
is greater than or equal to the previous point in the
curve. Nearly isotonic regression allows violations
in monotonicity (Tibshirani et al., 2011).
</bodyText>
<sectionHeader confidence="0.997389" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999435">
In the absence of highly refined knowledge about
a feature, discretization with structured regulariza-
tion enables higher quality impact of new feature
sets that contain non-linearities. In our experiments,
we observed that discretization out-performed naive
features lacking a good non-linear transformation by
up to 4.4 BLEU and that it can outperform a baseline
by up to 0.8 BLEU while dropping the log transform
of the lexical probabilities and removing the phrasal
probabilities in favor of counts. Looking beyond this
basic feature set, non-linear transformations could
be the difference between showing quality improve-
ments or not for novel features. As researchers in-
clude more real-valued features including counts,
similarity measures, and separately-trained models
with millions of features, we suspect this will be-
come an increasingly relevant issue. We conclude
that non-linear responses play an important role in
SMT, even for a commonly-used feature set, an ob-
servation that we hope will inform feature engineers.
</bodyText>
<figure confidence="0.992725357142857">
0.02 0.04 0.06 0.08 0.10
Original probability feature value
0.07 0.4
0.08
0.0 0.0 0.
0 50 10 150 200 250 30
0 6 0.8 1.0
Weight
Weight
70
ents
0.0 1.
0.02
0.6
</figure>
<page confidence="0.99622">
402
</page>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99982425">
This work was supported by Google Faculty Re-
search grants 2011 R2 705 and 2012 R2 10 and by
the NSF-sponsored XSEDE computing resources
program under grant TG-CCR110017.
</bodyText>
<sectionHeader confidence="0.997774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856268041238">
Michael Auli, Michel Galley, Chris Quirk, and Geoffrey
Zweig. 2013. Joint Language and Translation Mod-
eling with Recurrent Neural Networks. In Empirical
Methods in Natural Language Processing, number Oc-
tober, pages 1044–1054.
R. E. Barlow, D. Bartholomew, J. M. Bremner, and H. D.
Brunk. 1972. Statistical inference under order restric-
tions; the theory and application of isotonic regres-
sion. Wiley.
Ondej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ondej Du&amp;quot;sek, Pe-
tra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji´ı
Mar&amp;quot;s´ık, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tam-
chyna. 2012. The Joy of Parallelism with CzEng 1 .0.
In Proceedings of LREC2012, Istanbul, Turkey. Euro-
pean Language Resources Association.
Peter E Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 10598.
Eugene Charniak. 2010. Top-Down Nearly-Context-
Sensitive Parsing. In Empirical Methods in Natural
Language Processing, number October, pages 674–
683.
Colin Cherry and George Foster. 2012. Batch Tuning
Strategies for Statistical Machine Translation. In Pro-
ceedings of the North American Association for Com-
putational Linguistics, pages 427–436.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing - EMNLP ’08, pages 224–233, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201–228,
June.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 13:1159–1187.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statistical
Machine Translation: Controlling for Optimizer Insta-
bility. In Association for Computational Linguistics.
Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. Learning Non-Linear Combinations
of Kernels. In Advances in Neural Information Pro-
cessing Systems (NIPS 2009), pages 1–9, Vancouver,
Canada.
James Dougherty, Ron Kohavi, and Mehran Sahami.
1995. Supervised and Unsupervised Discretization of
Continuous Features. In Proceedings of the Twelfth
International Conference on Machine Learning, pages
194–202, San Francisco, CA.
Chris Dyer, Jonathan Weese, Adam Lopez, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models. In
Association for Computational Linguistics, number
July, pages 7–12.
Thomas Finley and Thorsten Joachims. 2008. Training
structural SVMs when exact inference is intractable.
In Proceedings of the International Conference on Ma-
chine Learning, pages 304–311, New York, New York,
USA. ACM Press.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-Scale Discriminative Training for Statistical
Machine Translation Using Held-Out Line Search. In
North American Association for Computational Lin-
guistics, number June, pages 248–258.
Michel Galley, Chris Quirk, Colin Cherry, and Kristina
Toutanova. 2013. Regularized Minimum Error Rate
Training. In Empirical Methods in Natural Language
Processing.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, PRO, and Paraphrases. In Workshop on Statistical
Machine Translation, pages 283–291.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Context-
aware Discriminative Phrase Selection for Statistical
Machine Translation. In Workshop on Statistical Ma-
chine Translation, number June, pages 159–166.
Kevin Gimpel and Noah A Smith. 2009. Feature-Rich
Translation by Quasi-Synchronous Lattice Parsing. In
Empirical Methods in Natural Language Processing.
Kevin Gimpel and Noah A Smith. 2012. Structured
Ramp Loss Minimization for Machine Translation. In
North American Association for Computational Lin-
guistics.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the Association for Com-
putational Linguistics, Jeju Island, Korea. Microsoft
Research.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. Computational Linguistics, pages 1352–
1362.
</reference>
<page confidence="0.990493">
403
</page>
<reference confidence="0.999772245098039">
Frederick Jelinek, John Lafferty, David Magerman,
Robert Mercer, Adwait Ratnaparkhi, and Salim
Roukos. 1994. Decision Tree Parsing using a Hidden
Derivation Model. In Workshop on Human Language
Technologies (HLT).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Empirical Meth-
ods in Natural Language Processing.
Sotiris Kotsiantis and Dimitris Kanellopoulos. 2006.
Discretization Techniques : A recent survey. In
GESTS International Transactions on Computer Sci-
ence and Engineering, volume 32, pages 47–58.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun
Zhao. 2013a. Additive Neural Networks for Statistical
Machine Translation. In Proceedings of the Associa-
tion for Computational Linguistics.
Lemao Liu, Tiejun Zhao, Taro Watanabe, and Eiichiro
Sumita. 2013b. Tuning SMT with A Large Number
of Features via Online Feature Grouping. In Proceed-
ings of the International Joint Conference on Natural
Language Processing.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Association for Computa-
tional Linguistics Computational Linguistics, number
August, pages 505–512.
David M Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Association for Computational
Linguistics, pages 276–283.
Anil Nelakanti, Cedric Archambeau, Julien Mairal, Fran-
cis Bach, and Guillaume Bouchard. 2013. Structured
Penalties for Log-linear Language Models. In Empiri-
cal Methods in Natural Language Processing, Seattle,
WA.
Patrick Nguyen, Milind Mahajan, Xiaodong He, and Mi-
crosoft Way. 2007. Training Non-Parametric Features
for Statistical Machine Translation. In Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
Association for Computational Linguistics, number
July, page 295, Morristown, NJ, USA. Association for
Computational Linguistics.
Franz J Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Association for Com-
putational Linguistics, number July, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU : a Method for Automatic
Evaluation of Machine Translation. In Computational
Linguistics, number July, pages 311–318.
Tim Robertson, F. T. Wright, and R. L. Dykstra. 1988.
Order Restricted Statistical Inference. Wiley.
S Ted Sandler. 2010. Regularized Learning with Feature
Networks. Ph.D. thesis, University of Pennsylvania.
Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Transla-
tion. In International Conference on Computational
Linguistics (COLING), number December 2012, pages
1071–1080, Mumbai, India.
Mervyn J. Silvapulle and Pranab K. Sen. 2005. Con-
strained Statistical Inference: Order, Inequality, and
Shape Constraints. Wiley.
Artem Sokolov, Guillaume Wisniewski, and Franc¸ois
Yvon. 2012. Non-linear N-best List Reranking with
Few Features. In Association for Machine Translation
in the Americas.
Jun Suzuki and Masaaki Nagata. 2013. Supervised
Model Learning with Feature Grouping based on a
Discrete Constraint. In Proceedings of the Association
for Computational Linguistics, pages 18–23.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-Margin Markov Networks. In Neural Informa-
tion Processing Systems.
Ryan J Tibshirani, Holger Hoefling, and Robert Tibshi-
rani. 2011. Nearly-Isotonic Regression. Technomet-
rics, 53(1):54–61.
Kristina Toutanova and Byung-Gyu Ahn. 2013. Learn-
ing Non-linear Features for Machine Translation Us-
ing Gradient Boosting Machines. In Proceedings of
the Association for Computational Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support Vector
Machine Learning for Interdependent and Structured
Output Spaces. In International Conference on Ma-
chine Learning (ICML).
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel Regression Based Machine Trans-
lation. In North American Association for Compu-
tational Linguistics, number April, pages 185–188,
Rochester, N.
Dekai Wu, Weifeng Su, and Marine Carpuat. 2004. A
Kernel PCA Method for Superior Word Sense Disam-
biguation. In Association for Computational Linguis-
tics, Barcelona.
Peng Xu and Frederick Jelinek. 2004. Random Forests in
Language Modeling. In Empirical Methods in Natural
Language Processing.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of the 26th Annual International Confer-
ence on Machine Learning - ICML ’09, pages 1–8,
New York, New York, USA. ACM Press.
</reference>
<page confidence="0.999012">
404
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986676">
<title confidence="0.9979885">Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization</title>
<author confidence="0.999982">Jonathan H Chris</author>
<affiliation confidence="0.998695">Research Mellon University</affiliation>
<address confidence="0.99984">Redmond, WA 98052, USA Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.999614428571429">Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, nonlinear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Language and Translation Modeling with Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing, number October,</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="4211" citStr="Auli et al., 2013" startWordPosition="642" endWordPosition="645">ational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that disc</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint Language and Translation Modeling with Recurrent Neural Networks. In Empirical Methods in Natural Language Processing, number October, pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Barlow</author>
<author>D Bartholomew</author>
<author>J M Bremner</author>
<author>H D Brunk</author>
</authors>
<title>Statistical inference under order restrictions; the theory and application of isotonic regression.</title>
<date>1972</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="36450" citStr="Barlow et al., 1972" startWordPosition="5983" endWordPosition="5986">her or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization with structured regularization enables higher quality impact of new feature sets that contain non-linearities. In our experiments, we observed that discretization out-performed naive features lacking a good non-linear transformation </context>
</contexts>
<marker>Barlow, Bartholomew, Bremner, Brunk, 1972</marker>
<rawString>R. E. Barlow, D. Bartholomew, J. M. Bremner, and H. D. Brunk. 1972. Statistical inference under order restrictions; the theory and application of isotonic regression. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondej Bojar</author>
<author>Zdenek Zabokrtsk´y</author>
<author>Ondej Dusek</author>
<author>Petra Galusc´akov´a</author>
<author>Martin Majlis</author>
<author>David Marecek</author>
<author>Ji´ı Mars´ık</author>
<author>Michal Nov´ak</author>
<author>Martin Popel</author>
<author>Ales Tamchyna</author>
</authors>
<date>2012</date>
<journal>The Joy of Parallelism with CzEng</journal>
<booktitle>In Proceedings of LREC2012,</booktitle>
<volume>1</volume>
<location>Istanbul,</location>
<marker>Bojar, Zabokrtsk´y, Dusek, Galusc´akov´a, Majlis, Marecek, Mars´ık, Nov´ak, Popel, Tamchyna, 2012</marker>
<rawString>Ondej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ondej Du&amp;quot;sek, Petra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji´ı Mar&amp;quot;s´ık, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tamchyna. 2012. The Joy of Parallelism with CzEng 1 .0. In Proceedings of LREC2012, Istanbul, Turkey. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>10598</pages>
<contexts>
<context position="1421" citStr="Brown et al., 1993" startWordPosition="205" endWordPosition="208">tomary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers. 1 Introduction Linear models using log-transformed probabilities as features have emerged as the dominant model in MT systems. This practice can be traced back to the IBM noisy channel models (Brown et al., 1993), which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes’ Rule. When Och and Ney (2002) introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities. This ∗This work was conducted as part of the first author’s Ph.D. work at Carnegie Mellon University. same formulation persisted even after the introduction of MERT (Och, 2003), which optimizes a linear model; again, using two log probability features (TM and LM) with</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 10598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Top-Down Nearly-ContextSensitive Parsing.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing, number October,</booktitle>
<pages>674--683</pages>
<contexts>
<context position="34826" citStr="Charniak, 2010" startWordPosition="5736" endWordPosition="5737"> Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with in</context>
</contexts>
<marker>Charniak, 2010</marker>
<rawString>Eugene Charniak. 2010. Top-Down Nearly-ContextSensitive Parsing. In Empirical Methods in Natural Language Processing, number October, pages 674– 683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics,</booktitle>
<pages>427--436</pages>
<contexts>
<context position="9915" citStr="Cherry and Foster, 2012" startWordPosition="1557" endWordPosition="1560"> MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the Bl-renormalized variant of regularized MERT. 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produces a fe</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the North American Association for Computational Linguistics, pages 427–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9318" citStr="Chiang et al., 2008" startWordPosition="1452" endWordPosition="1455">parability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), A</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08, pages 224–233, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="26244" citStr="Chiang, 2007" startWordPosition="4391" endWordPosition="4392">notone arrangement of weights. We augment RLNR with a smooth damping term D(w, j), which has the shape of a bathtub curve with steepness γ: D(w,j) = tanh2γ 2 (1jw1 + wj+1) − wj (12) 2 (wj−1 − wj+1) |h|−1 RMNR(w) = 0 E D(w,j)RLNR(w,j) (13) j=2 D is nearly zero while wj E [wj−1, wj+1] and nearly one otherwise. Briefly, the numerator measures how far wj is from the midpoint of wj−1 and wj+1 while the denominator scales that distance by the radius from the midpoint to the neighboring weight. 5 Experimental Setup6 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implem</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--1159</pages>
<contexts>
<context position="9936" citStr="Chiang, 2012" startWordPosition="1562" endWordPosition="1563">pion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the Bl-renormalized variant of regularized MERT. 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produces a feature set that is les</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. In Association for Computational Linguistics.</title>
<date>2011</date>
<contexts>
<context position="29132" citStr="Clark et al., 2011" startWordPosition="4832" endWordPosition="4835">Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8The section splits recommended by Bojar et al. (2012). 9MultEval 0.5.1: github.com/jhclark/multeval 400 Bits 4 8 12 Features 101 1302 12,910 Test BLEU 36.4 36.6 36.8 Table 2: Translation quality for Cz-+En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition Zh-+En Ar-+En Cz-+En P 20.8* (-2.7) 44.3* (-3.6) 36.5* (-1.1) log P 23.5† 47.9† 37.6† Disc P 23.4† (-0.1) 47.2† (-0.7) 36.8* (-0.8) Over. P 20.7* (-2.8) 44.6* (</context>
<context position="30440" citStr="Clark et al. (2011)" startWordPosition="5037" endWordPosition="5040"> (+0.8) 37.6† (±) MNR C 23.6† (±) 48.7*† (+0.8) 37.4† (-0.2) Table 3: Top: Translation quality for systems with and without the typical log transform. Bottom: Translation quality for systems using discretization and structured regularization with probabilities P or counts C as the input of discretization. MNR P consistently recovers or outperforms a state-of-the-art system, but without any assumptions about how to transform the initial features. All scores are averaged over 3 end-to-end optimizer replications. * denotes significantly different than log probs (row2) with p(CHANCE) &lt; 0.01 under Clark et al. (2011) and † is likewise used with regard to P (row 1). 6 Results 6.1 Does Non-Linearity Matter? In our first set of experiments, we seek to answer “Does non-linearity matter?” by starting with our baseline system of 7 typical features (the log Probability system) and we then remove the log transform from all of the log probability features in our grammar (the Probs. system). The results are shown in Table 3 (rows 1, 2). If a naive feature engineer were to remove the non-linear log transform, the systems would degrade between 1.1 BLEU and 3.6 BLEU. From this, we conclude that non-linearity does affe</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
<author>Afshin Rostamizadeh</author>
</authors>
<title>Learning Non-Linear Combinations of Kernels.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>1--9</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="35662" citStr="Cortes et al. (2009)" startWordPosition="5860" endWordPosition="5863"> (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to tr</context>
</contexts>
<marker>Cortes, Mohri, Rostamizadeh, 2009</marker>
<rawString>Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Learning Non-Linear Combinations of Kernels. In Advances in Neural Information Processing Systems (NIPS 2009), pages 1–9, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Dougherty</author>
<author>Ron Kohavi</author>
<author>Mehran Sahami</author>
</authors>
<title>Supervised and Unsupervised Discretization of Continuous Features.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>194--202</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="34632" citStr="Dougherty et al., 1995" startWordPosition="5704" endWordPosition="5707"> p(e|f) &gt; 0.11 is omitted for exposition as values were constant after this point. The gray line fits a log curve to the weights. The system learns a shape that deviates from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsoc</context>
</contexts>
<marker>Dougherty, Kohavi, Sahami, 1995</marker>
<rawString>James Dougherty, Ron Kohavi, and Mehran Sahami. 1995. Supervised and Unsupervised Discretization of Continuous Features. In Proceedings of the Twelfth International Conference on Machine Learning, pages 194–202, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Weese</author>
<author>Adam Lopez</author>
<author>Vladimir Eidelman</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics, number July,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="26677" citStr="Dyer et al., 2010" startWordPosition="4454" endWordPosition="4457">tance by the radius from the midpoint to the neighboring weight. 5 Experimental Setup6 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default f2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6Allcodeathttp://github.com/jhcl</context>
</contexts>
<marker>Dyer, Weese, Lopez, Eidelman, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Jonathan Weese, Adam Lopez, Vladimir Eidelman, Phil Blunsom, and Philip Resnik. 2010. cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models. In Association for Computational Linguistics, number July, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Training structural SVMs when exact inference is intractable.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>304--311</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="35469" citStr="Finley and Joachims, 2008" startWordPosition="5829" endWordPosition="5832">. Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear featu</context>
</contexts>
<marker>Finley, Joachims, 2008</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2008. Training structural SVMs when exact inference is intractable. In Proceedings of the International Conference on Machine Learning, pages 304–311, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Chris Dyer</author>
<author>Jaime Carbonell</author>
</authors>
<title>Large-Scale Discriminative Training for Statistical Machine Translation Using Held-Out Line Search.</title>
<date>2013</date>
<booktitle>In North American Association for Computational Linguistics,</booktitle>
<pages>248--258</pages>
<location>number</location>
<contexts>
<context position="9386" citStr="Flanigan et al., 2013" startWordPosition="1463" endWordPosition="1466"> in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are al</context>
</contexts>
<marker>Flanigan, Dyer, Carbonell, 2013</marker>
<rawString>Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013. Large-Scale Discriminative Training for Statistical Machine Translation Using Held-Out Line Search. In North American Association for Computational Linguistics, number June, pages 248–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Regularized Minimum Error Rate Training.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9979" citStr="Galley et al., 2013" startWordPosition="1568" endWordPosition="1571">OLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the Bl-renormalized variant of regularized MERT. 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produces a feature set that is less prone to non-linearities (see §2.2). We d</context>
</contexts>
<marker>Galley, Quirk, Cherry, Toutanova, 2013</marker>
<rawString>Michel Galley, Chris Quirk, Colin Cherry, and Kristina Toutanova. 2013. Regularized Minimum Error Rate Training. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Yuan Cao</author>
<author>Jonathan Weese</author>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, PRO, and Paraphrases.</title>
<date>2012</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>283--291</pages>
<contexts>
<context position="9822" citStr="Ganitkevitch et al., 2012" startWordPosition="1540" endWordPosition="1544">, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the Bl-renormalized variant of regularized MERT. 3 Discretization and Feature Induction In this</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch. 2012. Joshua 4.0: Packing, PRO, and Paraphrases. In Workshop on Statistical Machine Translation, pages 283–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Contextaware Discriminative Phrase Selection for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>159--166</pages>
<location>number</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Contextaware Discriminative Phrase Selection for Statistical Machine Translation. In Workshop on Statistical Machine Translation, number June, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Feature-Rich Translation by Quasi-Synchronous Lattice Parsing.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8949" citStr="Gimpel and Smith, 2009" startWordPosition="1393" endWordPosition="1396">ence translations, we can always recover the reference. In practice, both of these conditions are typically violated to a certain degree. However, if we modify our feature set such that some lower-ranked higher-quality hypothesis can be separated from all higher-ranked lower-quality hypotheses, then we can improve translation quality. For this reason, we believe that separability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we u</context>
</contexts>
<marker>Gimpel, Smith, 2009</marker>
<rawString>Kevin Gimpel and Noah A Smith. 2009. Feature-Rich Translation by Quasi-Synchronous Lattice Parsing. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured Ramp Loss Minimization for Machine Translation. In North American Association for Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="9352" citStr="Gimpel and Smith, 2012" startWordPosition="1457" endWordPosition="1460">ve tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A Smith. 2012. Structured Ramp Loss Minimization for Machine Translation. In North American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum Expected BLEU Training of Phrase and Lexicon Translation Models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics, Jeju Island,</booktitle>
<tech>Microsoft Research.</tech>
<contexts>
<context position="36124" citStr="He and Deng (2012)" startWordPosition="5936" endWordPosition="5939">d Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 20</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models. In Proceedings of the Association for Computational Linguistics, Jeju Island, Korea. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking. Computational Linguistics,</title>
<date>2011</date>
<pages>1352--1362</pages>
<contexts>
<context position="9593" citStr="Hopkins and May, 2011" startWordPosition="1499" endWordPosition="1502">etizing features (e.g. indicator features for count=N). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4Sinc</context>
<context position="26834" citStr="Hopkins and May, 2011" startWordPosition="4476" endWordPosition="4479">translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default f2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6Allcodeathttp://github.com/jhclark/cdec Bidirectional lexical log-probabilities, the coherent phrasal translation log-probability, target word count, glue rule count, source OOV count, tar</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. Computational Linguistics, pages 1352– 1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
</authors>
<title>Decision Tree Parsing using a Hidden Derivation Model.</title>
<date>1994</date>
<booktitle>In Workshop on Human Language Technologies (HLT).</booktitle>
<contexts>
<context position="34775" citStr="Jelinek et al., 1994" startWordPosition="5725" endWordPosition="5729">ns a shape that deviates from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence align</context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>Frederick Jelinek, John Lafferty, David Magerman, Robert Mercer, Adwait Ratnaparkhi, and Salim Roukos. 1994. Decision Tree Parsing using a Hidden Derivation Model. In Workshop on Human Language Technologies (HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent Continuous Translation Models.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4244" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="646" endWordPosition="649">, 2 (2014) 393–404. Action Editor: Robert C. Moore. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that discretization can be used to recover</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sotiris Kotsiantis</author>
<author>Dimitris Kanellopoulos</author>
</authors>
<title>Discretization Techniques : A recent survey.</title>
<date>2006</date>
<booktitle>In GESTS International Transactions on Computer Science and Engineering,</booktitle>
<volume>32</volume>
<pages>47--58</pages>
<contexts>
<context position="34669" citStr="Kotsiantis and Kanellopoulos, 2006" startWordPosition="5708" endWordPosition="5712">d for exposition as values were constant after this point. The gray line fits a log curve to the weights. The system learns a shape that deviates from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then propose</context>
</contexts>
<marker>Kotsiantis, Kanellopoulos, 2006</marker>
<rawString>Sotiris Kotsiantis and Dimitris Kanellopoulos. 2006. Discretization Techniques : A recent survey. In GESTS International Transactions on Computer Science and Engineering, volume 32, pages 47–58.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lemao Liu</author>
</authors>
<title>Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013a. Additive Neural Networks for Statistical Machine Translation.</title>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<marker>Liu, </marker>
<rawString>Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013a. Additive Neural Networks for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lemao Liu</author>
</authors>
<title>Tiejun Zhao, Taro Watanabe, and Eiichiro Sumita. 2013b. Tuning SMT with A Large Number of Features via Online Feature Grouping.</title>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing.</booktitle>
<marker>Liu, </marker>
<rawString>Lemao Liu, Tiejun Zhao, Taro Watanabe, and Eiichiro Sumita. 2013b. Tuning SMT with A Large Number of Features via Online Feature Grouping. In Proceedings of the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-Scale Translation Models via Pattern Matching.</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics Computational Linguistics, number August,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="27211" citStr="Lopez, 2008" startWordPosition="4538" endWordPosition="4539">syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default f2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6Allcodeathttp://github.com/jhclark/cdec Bidirectional lexical log-probabilities, the coherent phrasal translation log-probability, target word count, glue rule count, source OOV count, target OOV count, and target language model logprobability. Note that these features may be simplified or removed as specified in each experimental condition. Zh→En Ar→En Cz→En Train 303K 5.4M 1M WeightTune 1664 1797 3000 HyperTune 1085 1056 2000 Test 1357 1313 2000 Table 1: Corpus statistics: number of parallel sentences. Chinese Resources: For the Chinese→English experiments,</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-Scale Translation Models via Pattern Matching. In Association for Computational Linguistics Computational Linguistics, number August, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="34843" citStr="Magerman, 1995" startWordPosition="5738" endWordPosition="5739">nic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M Magerman. 1995. Statistical Decision-Tree Models for Parsing. In Association for Computational Linguistics, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Nelakanti</author>
<author>Cedric Archambeau</author>
<author>Julien Mairal</author>
<author>Francis Bach</author>
<author>Guillaume Bouchard</author>
</authors>
<title>Structured Penalties for Log-linear Language Models.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="36220" citStr="Nelakanti et al. (2013)" startWordPosition="5951" endWordPosition="5954">mains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization wit</context>
</contexts>
<marker>Nelakanti, Archambeau, Mairal, Bach, Bouchard, 2013</marker>
<rawString>Anil Nelakanti, Cedric Archambeau, Julien Mairal, Francis Bach, and Guillaume Bouchard. 2013. Structured Penalties for Log-linear Language Models. In Empirical Methods in Natural Language Processing, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Milind Mahajan</author>
<author>Xiaodong He</author>
<author>Microsoft Way</author>
</authors>
<title>Training Non-Parametric Features for Statistical Machine Translation. In Association for Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="35911" citStr="Nguyen et al. (2007)" startWordPosition="5900" endWordPosition="5903"> grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a cu</context>
</contexts>
<marker>Nguyen, Mahajan, He, Way, 2007</marker>
<rawString>Patrick Nguyen, Milind Mahajan, Xiaodong He, and Microsoft Way. 2007. Training Non-Parametric Features for Statistical Machine Translation. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics, number July,</booktitle>
<pages>295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1569" citStr="Och and Ney (2002)" startWordPosition="230" endWordPosition="233">ns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers. 1 Introduction Linear models using log-transformed probabilities as features have emerged as the dominant model in MT systems. This practice can be traced back to the IBM noisy channel models (Brown et al., 1993), which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes’ Rule. When Och and Ney (2002) introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities. This ∗This work was conducted as part of the first author’s Ph.D. work at Carnegie Mellon University. same formulation persisted even after the introduction of MERT (Och, 2003), which optimizes a linear model; again, using two log probability features (TM and LM) with equal weight recovered the noisy channel model. Yet systems now use many more features, some of which are not even probabilities. We no longer beli</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the Association for Computational Linguistics, number July, page 295, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Association for Computational Linguistics, number July,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1929" citStr="Och, 2003" startWordPosition="291" endWordPosition="292">l in MT systems. This practice can be traced back to the IBM noisy channel models (Brown et al., 1993), which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes’ Rule. When Och and Ney (2002) introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities. This ∗This work was conducted as part of the first author’s Ph.D. work at Carnegie Mellon University. same formulation persisted even after the introduction of MERT (Och, 2003), which optimizes a linear model; again, using two log probability features (TM and LM) with equal weight recovered the noisy channel model. Yet systems now use many more features, some of which are not even probabilities. We no longer believe that equal weights between the TM and LM provides optimal translation quality; the probabilities in the TM do not obey the chain rule nor Bayes’ rule, nullifying several theoretical mathematical justifications for multiplying probabilities. The story of multiplying probabilities may just amount to heavily penalizing small values. The community has abando</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Association for Computational Linguistics, number July, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU : a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Computational Linguistics, number July,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="29004" citStr="Papineni et al., 2002" startWordPosition="4811" endWordPosition="4814"> on NIST MT 2005, and test on NIST MT 2008. Czech resources: We also construct a Czech→English system based on the CzEng 1.0 data (Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8The section splits recommended by Bojar et al. (2012). 9MultEval 0.5.1: github.com/jhclark/multeval 400 Bits 4 8 12 Features 101 1302 12,910 Test BLEU 36.4 36.6 36.8 Table 2: Translation quality for Cz-+En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition Zh-+En Ar-+En Cz-+En P 20.8* (-2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU : a Method for Automatic Evaluation of Machine Translation. In Computational Linguistics, number July, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Robertson</author>
<author>F T Wright</author>
<author>R L Dykstra</author>
</authors>
<title>Order Restricted Statistical Inference.</title>
<date>1988</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="36474" citStr="Robertson et al., 1988" startWordPosition="5987" endWordPosition="5990">ranslation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization with structured regularization enables higher quality impact of new feature sets that contain non-linearities. In our experiments, we observed that discretization out-performed naive features lacking a good non-linear transformation by up to 4.4 BLEU and th</context>
</contexts>
<marker>Robertson, Wright, Dykstra, 1988</marker>
<rawString>Tim Robertson, F. T. Wright, and R. L. Dykstra. 1988. Order Restricted Statistical Inference. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ted Sandler</author>
</authors>
<title>Regularized Learning with Feature Networks.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="25093" citStr="Sandler (2010)" startWordPosition="4188" endWordPosition="4189">he discretization of a single real-valued feature are spatially related. We expect similar weights to be given to the indicator features that represent neighboring values of the original real-valued feature such that the resulting transformation appears somewhat smooth. To incorporate this knowledge of nearby bins, the linear neighbor regularizer RLNR penalizes each feature’s weight by the squared amount it differs from its neighbors’ midpoint: �j 1 2 RLNR (w, ✓) = C 1 (wj−1 + wj+1) − wj I (9) |h|−1 RLNR(w) = β E RLNR(w, j) (10) j=2 This is a special case of the feature network regularizer of Sandler (2010). Unlike traditional regularizers, we do not hope to reduce the active feature count. With the PRO loss l and a `2 regularizater R2, our final loss function internal to each iteration of PRO is: L(w) = l(x, y; w) + R2(w) + RLNR(w) (11) 399 4.3 Monotone Neighbor Regularization However, as 0 → oc, the linear neighbor regularizer RLNR forces a linear arrangement of weights – this violates our premise that we should be agnostic to non-linear transformations. We now describe a structured regularizer RMNR whose limiting solution is any monotone arrangement of weights. We augment RLNR with a smooth d</context>
</contexts>
<marker>Sandler, 2010</marker>
<rawString>S Ted Sandler. 2010. Regularized Learning with Feature Networks. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics (COLING), number</booktitle>
<pages>1071--1080</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="4192" citStr="Schwenk, 2012" startWordPosition="640" endWordPosition="641">tion for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and de</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation. In International Conference on Computational Linguistics (COLING), number December 2012, pages 1071–1080, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mervyn J Silvapulle</author>
<author>Pranab K Sen</author>
</authors>
<title>Constrained Statistical Inference: Order, Inequality, and Shape Constraints.</title>
<date>2005</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="36501" citStr="Silvapulle and Sen, 2005" startWordPosition="5991" endWordPosition="5994">te in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization with structured regularization enables higher quality impact of new feature sets that contain non-linearities. In our experiments, we observed that discretization out-performed naive features lacking a good non-linear transformation by up to 4.4 BLEU and that it can outperform a base</context>
</contexts>
<marker>Silvapulle, Sen, 2005</marker>
<rawString>Mervyn J. Silvapulle and Pranab K. Sen. 2005. Constrained Statistical Inference: Order, Inequality, and Shape Constraints. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Non-linear N-best List Reranking with Few Features. In Association for Machine Translation in the Americas.</title>
<date>2012</date>
<contexts>
<context position="3382" citStr="Sokolov et al., 2012" startWordPosition="524" endWordPosition="527">er non-linear transformations? To answer these, we explore the issue of non-linearity in models for MT. In the process, we will discuss the impact of linearity on feature engineering and develop a general mechanism for learning a class of non-linear transformations of real-valued features. Applying a non-linear transformation such as log to features is one way of achieving a non-linear response function, even if those features are aggregated in a linear model. Alternatively, we could achieve a non-linear response using a natively nonlinear model such as a SVM (Wang et al., 2007) or RankBoost (Sokolov et al., 2012). However, MT is a structured prediction problem, in which a full hypothesis is composed of partial hypotheses. MT decoders take advantage of the fact that the model 393 Transactions of the Association for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypo</context>
</contexts>
<marker>Sokolov, Wisniewski, Yvon, 2012</marker>
<rawString>Artem Sokolov, Guillaume Wisniewski, and Franc¸ois Yvon. 2012. Non-linear N-best List Reranking with Few Features. In Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Masaaki Nagata</author>
</authors>
<title>Supervised Model Learning with Feature Grouping based on a Discrete Constraint.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>18--23</pages>
<contexts>
<context position="20752" citStr="Suzuki and Nagata (2013)" startWordPosition="3401" endWordPosition="3404">he optimal feature transformation and (2) no bin’s data should be so sparse that the optimizer cannot reliably estimate a weight for each bin. Therefore, we construct bins that are (i) populated uniformly subject to (ii) each bin containing no more than one feature value. We call this approach uniform population feature binning. While one could consider the predictive power of the features when determining bin boundaries, this would suggest that we should jointly optimize and determine bin boundaries, which is beyond the scope of this work. This problem has recently been considered for NLP by Suzuki and Nagata (2013) and for MT by Liu et al. (2013b), though the latter involves decoding the entire training data. Let X be the list of feature values to bin where i indexes feature values xi ∈ X and their associated frequencies fi. We want each bin to have a uniform size u. For the sake of simplifying our final algorithm, we first create adjusted frequencies f&apos; i so that very frequent feature values will not occupy more than 100% of a bin via the following algorithm, which iterates over k: fk (5) i fk+1 i = min(fki ,uk) (6) which returns u&apos; = uk when fki &lt; uk ∀i. Next, we solve for a binning B of N bins where </context>
</contexts>
<marker>Suzuki, Nagata, 2013</marker>
<rawString>Jun Suzuki and Masaaki Nagata. 2013. Supervised Model Learning with Feature Grouping based on a Discrete Constraint. In Proceedings of the Association for Computational Linguistics, pages 18–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-Margin Markov Networks.</title>
<date>2003</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="35149" citStr="Taskar et al. (2003)" startWordPosition="5784" endWordPosition="5787">of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-Margin Markov Networks. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan J Tibshirani</author>
<author>Holger Hoefling</author>
<author>Robert Tibshirani</author>
</authors>
<title>Nearly-Isotonic Regression.</title>
<date>2011</date>
<tech>Technometrics, 53(1):54–61.</tech>
<contexts>
<context position="36727" citStr="Tibshirani et al., 2011" startWordPosition="6031" endWordPosition="6034">k. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization with structured regularization enables higher quality impact of new feature sets that contain non-linearities. In our experiments, we observed that discretization out-performed naive features lacking a good non-linear transformation by up to 4.4 BLEU and that it can outperform a baseline by up to 0.8 BLEU while dropping the log transform of the lexical probabilities and removing the phrasal probabilities in favor of counts. Looking beyond this basic feature set, non-linear transformations could be the dif</context>
</contexts>
<marker>Tibshirani, Hoefling, Tibshirani, 2011</marker>
<rawString>Ryan J Tibshirani, Holger Hoefling, and Robert Tibshirani. 2011. Nearly-Isotonic Regression. Technometrics, 53(1):54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Byung-Gyu Ahn</author>
</authors>
<title>Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35994" citStr="Toutanova and Ahn (2013)" startWordPosition="5913" endWordPosition="5916">alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured ip regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater tha</context>
</contexts>
<marker>Toutanova, Ahn, 2013</marker>
<rawString>Kristina Toutanova and Byung-Gyu Ahn. 2013. Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support Vector Machine Learning for Interdependent and Structured Output Spaces.</title>
<date>2004</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="35256" citStr="Tsochantaridis et al. (2004)" startWordPosition="5798" endWordPosition="5802">1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase trans</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support Vector Machine Learning for Interdependent and Structured Output Spaces. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>John Shawe-Taylor</author>
<author>Sandor Szedmak</author>
</authors>
<title>Kernel Regression Based Machine Translation.</title>
<date>2007</date>
<booktitle>In North American Association for Computational Linguistics, number April,</booktitle>
<pages>185--188</pages>
<location>Rochester, N.</location>
<contexts>
<context position="3346" citStr="Wang et al., 2007" startWordPosition="518" endWordPosition="521"> to believe it is better than other non-linear transformations? To answer these, we explore the issue of non-linearity in models for MT. In the process, we will discuss the impact of linearity on feature engineering and develop a general mechanism for learning a class of non-linear transformations of real-valued features. Applying a non-linear transformation such as log to features is one way of achieving a non-linear response function, even if those features are aggregated in a linear model. Alternatively, we could achieve a non-linear response using a natively nonlinear model such as a SVM (Wang et al., 2007) or RankBoost (Sokolov et al., 2012). However, MT is a structured prediction problem, in which a full hypothesis is composed of partial hypotheses. MT decoders take advantage of the fact that the model 393 Transactions of the Association for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalab</context>
</contexts>
<marker>Wang, Shawe-Taylor, Szedmak, 2007</marker>
<rawString>Zhuoran Wang, John Shawe-Taylor, and Sandor Szedmak. 2007. Kernel Regression Based Machine Translation. In North American Association for Computational Linguistics, number April, pages 185–188, Rochester, N.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Weifeng Su</author>
<author>Marine Carpuat</author>
</authors>
<title>A Kernel PCA Method for Superior Word Sense Disambiguation.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="35049" citStr="Wu et al. (2004)" startWordPosition="5769" endWordPosition="5772">e conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes </context>
</contexts>
<marker>Wu, Su, Carpuat, 2004</marker>
<rawString>Dekai Wu, Weifeng Su, and Marine Carpuat. 2004. A Kernel PCA Method for Superior Word Sense Disambiguation. In Association for Computational Linguistics, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random Forests in Language Modeling.</title>
<date>2004</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="34798" citStr="Xu and Jelinek, 2004" startWordPosition="5730" endWordPosition="5733">es from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random Forests in Language Modeling. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09,</booktitle>
<pages>1--8</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="35523" citStr="Yu and Joachims, 2009" startWordPosition="5838" endWordPosition="5841">e often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09, pages 1–8, New York, New York, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
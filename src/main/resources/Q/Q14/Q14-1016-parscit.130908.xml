<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999035">
Discriminative Lexical Semantic Segmentation with Gaps:
Running the MWE Gamut
</title>
<author confidence="0.998893">
Nathan Schneider Emily Danchik Chris Dyer Noah A. Smith
</author>
<affiliation confidence="0.876967333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.99802">
{nschneid,emilydan,cdyer,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9956634375">
We present a novel representation, evaluation
measure, and supervised models for the task of
identifying the multiword expressions (MWEs)
in a sentence, resulting in a lexical seman-
tic segmentation. Our approach generalizes
a standard chunking representation to encode
MWEs containing gaps, thereby enabling effi-
cient sequence tagging algorithms for feature-
rich discriminative models. Experiments on a
new dataset of English web text offer the first
linguistically-driven evaluation of MWE iden-
tification with truly heterogeneous expression
types. Our statistical sequence model greatly
outperforms a lookup-based segmentation pro-
cedure, achieving nearly 60% F1 for MWE
identification.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987467533333333">
Language has a knack for defying expectations when
put under the microscope. For example, there is the
notion—sometimes referred to as compositionality—
that words will behave in predictable ways, with indi-
vidual meanings that combine to form complex mean-
ings according to general grammatical principles. Yet
language is awash with examples to the contrary:
in particular, idiomatic expressions such as awash
with NP, have a knack for VP-ing, to the contrary, and
defy expectations. Thanks to processes like metaphor
and grammaticalization, these are (to various degrees)
semantically opaque, structurally fossilized, and/or
statistically idiosyncratic. In other words, idiomatic
expressions may be exceptional in form, function,
or distribution. They are so diverse, so unruly, so
</bodyText>
<listItem confidence="0.9788916">
1. MW named entities: Prime Minister Tony Blair
2. MW compounds: hot air balloon, skinny dip
3. conventionally SW compounds: somewhere
4. verb-particle: pick up, dry out, take over, cut short
5. verb-preposition: refer to, depend on, look for
6. verb-noun(-preposition): pay attention (to)
7. support verb: make decisions, take pictures
8. other phrasal verb: put up with, get rid of
9. PP modifier: above board, at all, from time to time
10. coordinated phrase: cut and dry, more or less
11. connective: as well as, let alone, in spite of
12. semi-fixed VP: pick up where &lt;one&gt; left off
13. fixed phrase: scared to death, leave of absence
14. phatic: You’re welcome. Me neither!
15. proverb: Beggars can’t be choosers.
</listItem>
<figureCaption confidence="0.98634075">
Figure 1: Some of the classes of idioms in English.
The examples included here contain multiple lexicalized
words—with the exception of those in (3), if the conven-
tional single-word (SW) spelling is used.
</figureCaption>
<bodyText confidence="0.9996400625">
difficult to circumscribe, that entire theories of syn-
tax are predicated on the notion that constructions
with idiosyncratic form-meaning mappings (Fillmore
et al., 1988; Goldberg, 1995) or statistical properties
(Goldberg, 2006) offer crucial evidence about the
grammatical organization of language.
Here we focus on multiword expressions
(MWEs): lexicalized combinations of two or more
words that are exceptional enough to be considered
as single units in the lexicon. As figure 1 illus-
trates, MWEs occupy diverse syntactic and semantic
functions. Within MWEs, we distinguish (a) proper
names and (b) lexical idioms. The latter have proved
themselves a “pain in the neck for NLP” (Sag et al.,
2002). Automatic and efficient detection of MWEs,
though far from solved, would have diverse appli-
</bodyText>
<page confidence="0.99587">
193
</page>
<bodyText confidence="0.972519214285714">
Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre.
Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics.
cations including machine translation (Carpuat and
Diab, 2010), information retrieval (Newman et al.,
2012), opinion mining (Berend, 2011), and second
language learning (Ellis et al., 2008).
It is difficult to establish any comprehensive tax-
onomy of multiword idioms, let alone develop lin-
guistic criteria and corpus resources that cut across
these types. Consequently, the voluminous litera-
ture on MWEs in computational linguistics—see §7,
Baldwin and Kim (2010), and Ramisch (2012) for
surveys—has been fragmented, looking (for exam-
ple) at subclasses of phrasal verbs or nominal com-
pounds in isolation. To the extent that MWEs have
been annotated in existing corpora, it has usually
been as a secondary aspect of some other scheme.
Traditionally, such resources have prioritized certain
kinds of MWEs to the exclusion of others, so they
are not appropriate for evaluating general-purpose
identification systems.
In this article, we briefly review a shallow form
of analysis for MWEs that is neutral to expression
type, and that facilitates free text annotation with-
out requiring a prespecified MWE lexicon (§2). The
scheme applies to gappy (discontinuous) as well as
contiguous expressions, and allows for a qualitative
distinction of association strengths. In Schneider
et al. (2014) we have applied this scheme to fully an-
notate a 55,000-word corpus of English web reviews
(Bies et al., 2012a), a conversational genre in which
colloquial idioms are highly salient. This article’s
main contribution is to show that the representation—
constrained according to linguistically motivated as-
sumptions (§3)—can be transformed into a sequence
tagging scheme that resembles standard approaches
in named entity recognition and other text chunking
tasks (§4). Along these lines, we develop a discrim-
inative, structured model of MWEs in context (§5)
and train, evaluate, and examine it on the annotated
corpus (§6). Finally, in §7 and §8 we comment on
related work and future directions.
</bodyText>
<sectionHeader confidence="0.997457" genericHeader="introduction">
2 Annotated Corpus
</sectionHeader>
<bodyText confidence="0.999977770833334">
To build and evaluate a multiword expression ana-
lyzer, we use the MWE-annotated corpus of Schnei-
der et al. (2014). It consists of informal English web
text that has been specifically and completely anno-
tated for MWEs, without reference to any particular
lexicon. To the best of our knowledge, this corpus
is the first to be freely annotated for many kinds of
MWEs (without reference to a lexicon), and is also
the first dataset of social media text with MWE an-
notations beyond named entities. This section gives
a synopsis of the annotation conventions used to de-
velop that resource, as they are important to under-
standing our models and evaluation.
Rationale. The multiword expressions community
has lacked a canonical corpus resource comparable
to benchmark datasets used for problems such as
NER and parsing. Consequently, the MWE litera-
ture has been driven by lexicography: typically, the
goal is to acquire an MWE lexicon with little or no
supervision, or to apply such a lexicon to corpus
data. Studies of MWEs in context have focused on
various subclasses of constructions in isolation, ne-
cessitating special-purpose datasets and evaluation
schemes. By contrast, Schneider et al.’s (2014) cor-
pus creates an opportunity to tackle general-purpose
MWE identification, such as would be desirable for
use by high-coverage downstream NLP systems. It is
used to train and evaluate our models below. The cor-
pus is publicly available as a benchmark for further
research.1
Data. The documents in the corpus are online user
reviews of restaurants, medical providers, retailers,
automotive services, pet care services, etc. Marked
by conversational and opinionated language, this
genre is fertile ground for colloquial idioms (Nunberg
et al., 1994; Moon, 1998). The 723 reviews (55,000
words, 3,800 sentences) in the English Web Tree-
bank (WTB; Bies et al., 2012b) were collected by
Google, tokenized, and annotated with phrase struc-
ture trees in the style of the Penn Treebank (Marcus
et al., 1993). MWE annotators used the sentence and
word tokenizations supplied by the treebank.2
Annotation scheme. The annotation scheme itself
was designed to be as simple as possible. It consists
of grouping together the tokens in each sentence that
belong to the same MWE instance. While annotation
guidelines provide examples of MWE groupings in
a wide range of constructions, the annotator is not
</bodyText>
<footnote confidence="0.98490975">
1http://www.ark.cs.cmu.edu/LexSem/
2Because we use treebank data, syntactic parses are available
to assist in post hoc analysis. Syntactic information was not
shown to annotators.
</footnote>
<page confidence="0.994793">
194
</page>
<table confidence="0.9769624">
# of constituent tokens # of gaps
2 3 ≥4 total 0 1 2
strong 2257 595 172 3024 2626 394 4
weak 269 121 69 459 322 135 2
2526 716 241 3483 2948 529 6
</table>
<tableCaption confidence="0.999952">
Table 1: Counts in the MWE corpus.
</tableCaption>
<bodyText confidence="0.9977775">
tied to any particular taxonomy or syntactic structure.
This simplifies the number of decisions that have to
be made for each sentence, even if some are difficult.
Further instructions to annotators included:
</bodyText>
<listItem confidence="0.973962555555556">
• Groups should include only the lexically fixed parts
of an expression (modulo inflectional morphology);
this generally excludes determiners and pronouns:
made the mistake, pride themselves on.
• Multiword proper names count as MWEs.
• Misspelled or unconventionally spelled tokens are
interpreted according to the intended word if clear.
• Overtokenized words (spelled as two tokens, but
conventionally one word) are joined as multiwords.
</listItem>
<bodyText confidence="0.9865565">
Clitics separated by the tokenization in the corpus—
negative n’t, possessive ’s, etc.—are joined if func-
tioning as a fixed part of a multiword (e.g., T ’s
Cafe), but not if used productively.
Gaps. There are, broadly speaking, three reasons
to group together tokens that are not fully contigu-
ous. Most commonly, gaps contain internal modifiers,
such as good in make good decisions. Syntactic con-
structions such as the passive can result in gaps that
might not otherwise be present: in good decisions
were made, there is instead a gap filled by the pas-
sive auxiliary. Finally, some MWEs may take internal
arguments: they gave me a break. Figure 1 has addi-
tional examples. Multiple gaps can occur even within
the same expression, though it is rare: they agreed to
give Bob a well-deserved break.
Strength. The annotation scheme has two
“strength” levels for MWEs. Clearly idiomatic ex-
pressions are marked as strong MWEs, while mostly
compositional but especially frequent collocations/
phrases (e.g., abundantly clear and patently obvious)
are marked as weak MWEs. Weak multiword groups
are allowed to include strong MWEs as constituents
(but not vice versa). Strong groups are required to
cohere when used inside weak groups: that is, a weak
group cannot include only part of a strong group.
For purposes of annotation, there were no constraints
hinging on the ordering of tokens in the sentence.
Process. MWE annotation proceeded one sentence
at a time. The 6 annotators referred to and improved
the guidelines document on an ongoing basis. Every
sentence was seen independently by at least 2 an-
notators, and differences of opinion were discussed
and resolved (often by marking a weak MWE as a
compromise). See Schneider et al. (2014) for details.
Statistics. The annotated corpus consists of 723
documents (3,812 sentences). MWEs are frequent
in this domain: 57% of sentences (72% of sentences
over 10 words long) and 88% of documents contain
at least one MWE. 8,060~55,579=15% of tokens
belong to an MWE; in total, there are 3,483 MWE
instances. 544 (16%) are strong MWEs containing a
gold-tagged proper noun—most are proper names. A
breakdown appears in table 1.
</bodyText>
<sectionHeader confidence="0.989842" genericHeader="method">
3 Representation and Task Definition
</sectionHeader>
<bodyText confidence="0.996649285714286">
We define a lexical segmentation of a sentence as a
partitioning of its tokens into segments such that each
segment represents a single unit of lexical meaning.
A multiword lexical expression may contain gaps,
i.e. interruptions by other segments. We impose two
restrictions on gaps that appear to be well-motivated
linguistically:
</bodyText>
<listItem confidence="0.9997855">
• Projectivity: Every expression filling a gap must
be completely contained within that gap; gappy
expressions may not interleave.
• No nested gaps: A gap in an expression may be
</listItem>
<bodyText confidence="0.9204456">
filled by other single- or multiword expressions, so
long as those do not themselves contain gaps.
Formal grammar. Our scheme corresponds to the
following extended CFG (Thatcher, 1967), where S
is the full sentence and terminals w are word tokens:
</bodyText>
<equation confidence="0.999198">
S → X+
X →w+ (Y+ w+)∗
Y → w+
</equation>
<bodyText confidence="0.99164">
Each expression X or Y is lexicalized by the words in
one or more underlined variables on the right-hand
side. An X constituent may optionally contain one or
more gaps filled by Y constituents, which must not
contain gaps themselves.3
</bodyText>
<footnote confidence="0.85686025">
3MWEs with multiple gaps are rare but attested in data: e.g.,
putting me at my ease. We encountered one violation of the gap
nesting constraint in the reviews data: I have21nothing21 but21
fantastic things2 to21say21 . Additionally, the interrupted phrase
</footnote>
<page confidence="0.998724">
195
</page>
<bodyText confidence="0.9458386">
Denoting multiword groupings with subscripts, My
wife had taken1 her ’072 Ford2 Fusion2 in1 for a
routine oil3 change3 contains 3 multiword groups—
{taken, in}, {’07, Ford, Fusion}, {oil, change}—and
7 single-word groups. The first MWE is gappy (ac-
centuated by the box); a single word and a contiguous
multiword group fall within the gap. The projectivity
constraint forbids an analysis like taken1 her ’072
Ford1 Fusion2, while the gap nesting constraint for-
her2 ’07 Ford2 Fusion2
</bodyText>
<subsectionHeader confidence="0.999306">
3.1 Two-level Scheme: Strong vs. Weak MWEs
</subsectionHeader>
<bodyText confidence="0.99993675">
Our annotated data distinguish two strengths of
MWEs as discussed in §2. Augmenting the gram-
mar of the previous section, we therefore designate
nonterminals as strong (X, Y) or weak (˜X, ˜Y):
</bodyText>
<equation confidence="0.999323">
S → ˜X+
X˜ → X+ ( ˜Y+ X+)∗
X →w+ ( ˜Y+ w+)∗
Y˜ → Y+
Y → w+
</equation>
<bodyText confidence="0.999943714285714">
A weak MWE may be lexicalized by single words
and/or strong multiwords. Strong multiwords cannot
contain weak multiwords except in gaps. Further, the
contents of a gap cannot be part of any multiword
that extends outside the gap.4
For example, consider the segmentation: he was
willing to budge1 a2 little2 on1 the price which
means4 a43 lot43 to4 me4. Subscripts denote strong
MW groups and superscripts weak MW groups; un-
marked tokens serve as single-word expressions. The
MW groups are thus {budge, on}, {a, little}, {a, lot},
and {means, {a, lot}, to, me}. As should be evident
from the grammar, the projectivity and gap-nesting
constraints apply here just as in the 1-level scheme.
</bodyText>
<subsectionHeader confidence="0.998213">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.965430090909091">
Matching criteria. Given that most tokens do not
belong to an MWE, to evaluate MWE identification
we adopt a precision/recall-based measure from the
coreference resolution literature. The MUC criterion
(Vilain et al., 1995) measures precision and recall
great gateways never1 before1 , so23 far23 as23 Hudson knew2 ,
seen1 by Europeans was annotated in another corpus.
4This was violated 6 times in our annotated data: modifiers
within gaps are sometimes collocated with the gappy expression,
as in on12 a12 tight1 budget12 and have12 little1 doubt12.
of links in terms of groups (units) implied by the
transitive closure over those links.5 It can be defined
as follows:
Let a Ð b denote a link between two elements
in the gold standard, and a ˆÐb denote a link in the
system prediction. Let the ∗ operator denote the tran-
sitive closure over all links, such that QaÐ∗b� is 1 if
a and b belong to the same (gold) set, and 0 other-
wise. Assuming there are no redundant6 links within
any annotation (which in our case is guaranteed by
linking consecutive words in each MWE), we can
write the MUC precision and recall measures as:
</bodyText>
<equation confidence="0.984589666666667">
Ea,b:aÐb Qa ˆÐ∗b�
R=
Ea,b:aÐb 1
</equation>
<bodyText confidence="0.999921916666667">
This awards partial credit when predicted and gold
expressions overlap in part. Requiring full MWEs to
match exactly would arguably be too stringent, over-
penalizing larger MWEs for minor disagreements.
We combine precision and recall using the standard
F1 measure of their harmonic mean. This is the link-
based evaluation used for most of our experiments.
For comparison, we also report some results with
a more stringent exact match evaluation where the
span of the predicted MWE must be identical to the
span of the gold MWE for it to count as correct.
Strength averaging. Recall that the 2-level
scheme (§3.1) distinguishes strong vs. weak links/
groups, where the latter category applies to reason-
ably compositional collocations as well as ambigu-
ous or difficult cases. If where one annotation uses
a weak link the other has a strong link or no link at
all, we want to penalize the disagreement less than
if one had a strong link and the other had no link.
To accommodate the 2-level scheme, we therefore
average F↑1 , in which all weak links have been con-
verted to strong links, and F↓1 , in which they have
been removed: F1= 1 2(F↑ 1 +F↓ 1 ).7 If neither annota-
tion contains any weak links, this equals the MUC
</bodyText>
<footnote confidence="0.9897658">
5As a criterion for coreference resolution, the MUC measure
has perceived shortcomings which have prompted several other
measures (see Recasens and Hovy, 2011 for a review). It is not
clear, however, whether any of these criticisms are relevant to
MWE identification.
6A link between a and b is redundant if the other links already
imply that a and b belong to the same set. A set of N elements is
expressed non-redundantly with exactly N −1 links.
7Overall precision and recall are likewise computed by aver-
aging “strengthened” and “weakened” measurements.
</footnote>
<figure confidence="0.607823">
bids taken1
in1.
Ea,b:a ˆÐb QaÐ∗b�
P=
Ea,b:a ˆÐb 1
</figure>
<page confidence="0.896485">
196
</page>
<figureCaption confidence="0.9912115">
Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and
weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications.
</figureCaption>
<figure confidence="0.9998434">
a
I
means
B
a
I˜
means
B
gappy,
1-level
gappy,
2-level
willing
O
willing
O
the
O
on the
I¯ O
lot to
I I
lot to me
I¯ I˜ I˜
. (OIB(oIb[ii]+I[
O
¯I˜I])∗[
budge
B
budge
B
a
b
a
b
little
i
little
i
. (OIB(oIbi+II)∗I+)+
O
on
I
me
I
lot to
I I
me
I
a
I
a
B
little
I
budge
O
on
O
was
O
to
O
price
O
which
O
the
O
he
O
means
B
willing
O
. (OIBI+)+
O
no gaps,
1-level
no gaps,
2-level
lot to me
I¯ I˜ I˜
. (OIB[
O
little
I¯
a
B
was
O
willing
O
budge
O
which
O
price
O
the
O
on
O
he
O
to
O
a
I˜
means
B
¯I
˜I]+)+
he
O
was
O
he
O
was
O
price
O
which
O
price
O
which
O
¯I
˜I]+)+
to
O
to
O
</figure>
<bodyText confidence="0.863568333333333">
score because F1 = F↑1 = F↓1 . This method applies
to both the link-based and exact match evaluation
criteria.
</bodyText>
<sectionHeader confidence="0.969729" genericHeader="method">
4 Tagging Schemes
</sectionHeader>
<bodyText confidence="0.9999702">
Following (Ramshaw and Marcus, 1995), shallow an-
alysis is often modeled as a sequence-chunking task,
with tags containing chunk-positional information.
The BIO scheme and variants (e.g., BILOU; Ratinov
and Roth, 2009) are standard for tasks like named
entity recognition, supersense tagging, and shallow
parsing.
The language of derivations licensed by the gram-
mars in §3 allows for a tag-based encoding of MWE
analyses with only bigram constraints. We describe
4 tagging schemes for MWE identification, starting
with BIO and working up to more expressive variants.
They are depicted in figure 2.
No gaps, 1-level (3 tags). This is the standard con-
tiguous chunking representation from Ramshaw and
Marcus (1995) using the tags {O B I}. O is for to-
kens outside any chunk; B marks tokens beginning
a chunk; and I marks other tokens inside a chunk.
Multiword chunks will thus start with B and then I.
B must always be followed by I; I is not allowed at
the beginning of the sentence or following O.
No gaps, 2-level (4 tags). We can distinguish
strength levels by splitting I into two tags: I¯ for
strong expressions and I˜ for weak expressions. To
express strong and weak contiguous chunks requires
4 tags: {O B I¯ i}. (Marking B with a strength as well
would be redundant because MWEs are never length-
one chunks.) The constraints on I¯ and I˜ are the same
as the constraints on I in previous schemes. If I¯ and
I˜ occur next to each other, the strong attachment will
receive higher precedence, resulting in analysis of
strong MWEs as nested within weak MWEs.
Gappy, 1-level (6 tags). Because gaps cannot
themselves contain gappy expressions (we do not
support full recursivity), a finite number of additional
tags are sufficient to encode gappy chunks. We there-
fore add lowercase tag variants representing tokens
within a gap: {O o B b I i}. In addition to the con-
straints stated above, no within-gap tag may occur at
the beginning or end of the sentence or immediately
following or preceding O. Within a gap, b, i, and o
behave like their out-of-gap counterparts.
Gappy, 2-level (8 tags). 8 tags are required to en-
code the 2-level scheme with gaps: {O o B b I¯ i I˜ i}.
Variants of the inside tag are marked for strength of
the incoming link—this applies gap-externally (capi-
talized tags) and gap-internally (lowercase tags). If I¯
or I˜ immediately follows a gap, its diacritic reflects
the strength of the gappy expression, not the gap’s
contents.
</bodyText>
<sectionHeader confidence="0.996409" genericHeader="method">
5 Model
</sectionHeader>
<bodyText confidence="0.999935285714286">
With the above representations we model MWE iden-
tification as sequence tagging, one of the paradigms
that has been used previously for identifying con-
tiguous MWEs (Constant and Sigogne, 2011, see
§7).8 Constraints on legal tag bigrams are sufficient
to ensure the full tagging is well-formed subject to
the regular expressions in figure 2; we enforce these
</bodyText>
<footnote confidence="0.9687115">
8Hierarchical modeling based on our representations is left
to future work.
</footnote>
<page confidence="0.99519">
197
</page>
<bodyText confidence="0.99942925">
constraints in our experiments.9
In NLP, conditional random fields (Lafferty et al.,
2001) and the structured perceptron (Collins, 2002)
are popular techniques for discriminative sequence
modeling with a convex loss function. We choose
the second approach for its speed: learning and in-
ference depend mainly on the runtime of the Viterbi
algorithm, whose asymptotic complexity is linear in
the length of the input and (with a first-order Markov
assumption) quadratic in the number of tags. Below,
we review the structured perceptron and discuss our
cost function, features, and experimental setup.
</bodyText>
<subsectionHeader confidence="0.997336">
5.1 Cost-Augmented Structured Perceptron
</subsectionHeader>
<bodyText confidence="0.999948105263158">
The structured perceptron’s (Collins, 2002) learn-
ing procedure, algorithm 1, generalizes the classic
perceptron algorithm (Freund and Schapire, 1999) to
incorporate a structured decoding step (for sequences,
the Viterbi algorithm) in the inner loop. Thus, train-
ing requires only max inference, which is fast with a
first-order Markov assumption. In training, features
are adjusted where a tagging error is made; the pro-
cedure can be viewed as optimizing the structured
hinge loss. The output of learning is a weight vector
that parametrizes a feature-rich scoring function over
candidate labelings of a sequence.
To better align the learning algorithm with our
F-score–based MWE evaluation (§3.2), we use a
cost-augmented version of the structured perceptron
that is sensitive to different kinds of errors during
training. When recall is the bigger obstacle, we can
adopt the following cost function: given a sentence
x, its gold labeling y∗, and a candidate labeling y′,
</bodyText>
<equation confidence="0.99727225">
ly∗�
cost(y∗,y′,x) = E c(y∗j,y′j) where
j=1
c(y∗,y′) = Qy∗ ≠ y′�+pQy∗ ∈ {B,b}∧y′ ∈ {O,o}�
</equation>
<bodyText confidence="0.999818285714286">
A single nonnegative hyperparameter, p, controls
the tradeoff between recall and accuracy; higher p
biases the model in favor of recall (possibly hurt-
ing accuracy and precision). This is a slight variant
of the recall-oriented cost function of Mohit et al.
(2012). The difference is that we only penalize
beginning-of-expression recall errors. Preliminary
</bodyText>
<footnote confidence="0.744978666666667">
9The 8-tag scheme licenses 42 tag bigrams: sequences such
as B O and o 1 are prohibited. There are also constraints on the
allowed tags at the beginning and end of the sequence.
</footnote>
<equation confidence="0.965497153846154">
Input: data ((x(n),y(n)))Nn=1; number of iterations M
w ← 0
w ← 0
t ← 1
for m = 1 to M do
for n = 1 to N do
(x,y) ← (x(n),y(n))
yˆ ← argmaxy′ (wTg(x,y′)+cost(y,y′,x))
if yˆ≠ y then
w ← w+g(x,y)−g(x,ˆy)
w ← w+tg(x,y)−tg(x, ˆy)
end
t ← t +1
</equation>
<table confidence="0.3676476">
end
end
Output: w−(wlt)
Algorithm 1: Training with the averaged perceptron.
(Adapted from Daumé, 2006, p. 19.)
</table>
<bodyText confidence="0.999863857142857">
experiments showed that a cost function penalizing
all recall errors—i.e., with pQy∗ ≠ O ∧y′ = O� as the
second term, as in Mohit et al.—tended to append
additional tokens to high-confidence MWEs (such
as proper names) rather than encourage new MWEs,
which would require positing at least two new non-
outside tags.
</bodyText>
<subsectionHeader confidence="0.897187">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.999910375">
Basic features. These are largely based on those
of Constant et al. (2012): they look at word unigrams
and bigrams, character prefixes and suffixes, and POS
tags, as well as lexicon entries that match lemmas10
of multiple words in the sentence. Appendix A lists
the basic features in detail.
Some of the basic features make use of lexicons.
We use or construct 10 lists of English MWEs: all
multiword entries in WordNet (Fellbaum, 1998); all
multiword chunks in SemCor (Miller et al., 1993);
all multiword entries in English Wiktionary;
WikiMwe dataset mined from English Wikipedia
(Hartmann et al., 2012); the SAID database of
phrasal lexical idioms (Kuiper et al., 2003); the
named entities and other MWEs in the WSJ corpus
on the English side of the CEDT (Hajiˇc et al., 2012);
</bodyText>
<footnote confidence="0.915808">
10The WordNet API in NLTK (Bird et al., 2009) was used for
lemmatization.
11http://en.wiktionary.org; data obtained from
https://toolserver.org/~enwikt/definitions/
enwikt-defs-20130814-en.tsv.gz
11 the
</footnote>
<page confidence="0.868388">
198
</page>
<table confidence="0.9124732">
LOOKUP SUPERVISED MODEL
entries max gap P R F1 σ P R F1 σ
0 length 74.39 44.43 55.57 2.19
71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.90
420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.67
437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30
1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.65
2 lexicons + MWtypes(train)&gt;1 6 lexicons + MWtypes(train)&gt;2
preexising lexicons
none
WordNet + SemCor
6lexicons
10 lexicons
best configuration with
in-domain lexicon
</table>
<tableCaption confidence="0.950841142857143">
Table 2: Use of lexicons for lookup-based vs. statistical segmentation. Supervised learning used only basic features
and the structured perceptron, with the 8-tag scheme. Results are with the link-based matching criterion for evaluation.
Top: Comparison of preexisting lexicons. “6 lexicons” refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,
and English Wiktionary; “10 lexicons” adds MWEs from CEDT, VNC, LVC, and Oyz. (In these lookup-based
configurations, allowing gappy MWEs never helps performance.)
Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of each
cross-validation fold at least once (lookup) or twice (model).
</tableCaption>
<bodyText confidence="0.975536740740741">
All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviations
are shown for the F1 score. In each column, the highest value using only preexisting lexicons is underlined, and the
highest overall value is bolded. The boxed row indicates the configuration used as the basis for subsequent experiments.
the verb-particle constructions (VPCs) dataset of
(Baldwin, 2008); a list of light verb constructions
(LVCs) provided by Claire Bonial; and two idioms
websites.12 After preprocessing, each lexical entry
consists of an ordered sequence of word lemmas,
some of which may be variables like &lt;something&gt;.
Given a sentence and one or more of the lexicons,
lookup proceeds as follows: we enumerate entries
whose lemma sequences match a sequence of lemma-
tized tokens, and build a lattice of possible analyses
over the sentence. We find the shortest path (i.e.,
using as few expressions as possible) with dynamic
programming, allowing gaps of up to length 2.13
Unsupervised word clusters. Distributional clus-
tering on large (unlabeled) corpora can produce lexi-
cal generalizations that are useful for syntactic and
semantic analysis tasks (e.g.: Miller et al., 2004; Koo
et al., 2008; Turian et al., 2010; Owoputi et al., 2013;
Grave et al., 2013). We were interested to see whether
a similar pattern would hold for MWE identification,
given that MWEs are concerned with what is lexi-
cally idiosyncratic—i.e., backing off from specific
lexemes to word classes may lose the MWE-relevant
information. Brown clustering14 (Brown et al., 1992)
</bodyText>
<footnote confidence="0.978819">
12http://www.phrases.net/ and http://home.
postech.ac.kr/~oyz/doc/idiom.html
13Each top-level lexical expression (single- or multiword)
incurs a cost of 1; each expression within a gap has cost 1.25.
14With Liang’s (2005) implementation: https://github.
com/percyliang/brown-cluster. We obtain 1,000 clusters
</footnote>
<bodyText confidence="0.999846789473684">
on the 21-million-word Yelp Academic Dataset15
(which is similar in genre to the annotated web re-
views data) gives us a hard clustering of word types.
To our tagger, we add features mapping the previ-
ous, current, and next token to Brown cluster IDs.
The feature for the current token conjoins the word
lemma with the cluster ID.
Part-of-speech tags. We compared three PTB-
style POS taggers on the full REVIEWS subcor-
pus (train+test). The Stanford CoreNLP tagger16
(Toutanova et al., 2003) yields an accuracy of 90.4%.
The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al.,
2013) achieves 90.1% with the model17 trained on the
Twitter corpus of Ritter et al. (2011), and 94.9% when
trained on the ANSWERS, EMAIL, NEWSGROUP, and
WEBLOG subcorpora of WTB. We use this third con-
figuration to produce automatic POS tags for training
and testing our MWE tagger. (A comparison condi-
tion in §6.3 uses oracle POS tags.)
</bodyText>
<subsectionHeader confidence="0.991555">
5.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.982391">
The corpus of web reviews described in §2 is used
for training and evaluation. 101 arbitrarily chosen
documents (500 sentences, 7,171 words) were held
from words appearing at least 25 times.
</bodyText>
<footnote confidence="0.9975615">
15https://www.yelp.com/academic_dataset
16v. 3.2.0, with english-bidirectional-distsim
17http://www.ark.cs.cmu.edu/TweetNLP/model.
ritter_ptb_alldata_fixed.20130723
</footnote>
<page confidence="0.993637">
199
</page>
<table confidence="0.999354">
LINK-BASED EXACT MATCH
configuration M ρ JwJ P R F1 P R F1
base model 5 — 1,765k 69.27 50.49 58.35 60.99 48.27 53.85
+ recall cost 4 150 1,765k 61.09 57.94 59.41 53.09 55.38 54.17
+ clusters 3 100 2,146k 63.98 55.51 59.39 56.34 53.24 54.70
+ oracle POS 4 100 2,145k 66.19 59.35 62.53 58.51 57.00 57.71
</table>
<tableCaption confidence="0.984492666666667">
Table 3: Comparison of supervised models on test (using the 8-tag scheme). The base model corresponds to the boxed
result in table table 2, but here evaluated on test. For each configuration, the number of training iterations M and (except
for the base model) the recall-oriented hyperparameter ρ were tuned by cross-validation on train.
</tableCaption>
<bodyText confidence="0.99992645">
out as a final test set. This left 3,312 sentences/
48,408 words for training/development (train). Fea-
ture engineering and hyperparameter tuning were
conducted with 8-fold cross-validation on train. The
8-tag scheme is used except where otherwise noted.
In learning with the structured perceptron (algo-
rithm 1), we employ two well-known techniques that
can both be viewed as regularization. First, we use
the average of parameters over all timesteps of learn-
ing. Second, within each cross-validation fold, we de-
termine the number of training iterations (epochs) M
by early stopping—that is, after each iteration, we use
the model to decode the held-out data, and when that
accuracy ceases to improve, use the previous model.
The two hyperparameters are the number of iterations
and the value of the recall cost hyperparameter (ρ).
Both are tuned via cross-validation on train; we use
the multiple of 50 that maximizes average link-based
F1. The chosen values are shown in table 3. Experi-
ments were managed with the ducttape tool.18
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999865">
We experimentally address the following questions
to probe and justify our modeling approach.
</bodyText>
<subsectionHeader confidence="0.998328">
6.1 Is supervised learning necessary?
</subsectionHeader>
<bodyText confidence="0.9998057">
Previous MWE identification studies have found
benefit to statistical learning over heuristic lexicon
lookup (Constant and Sigogne, 2011; Green et al.,
2012). Our first experiment tests whether this holds
for comprehensive MWE identification: it compares
our supervised tagging approach with baselines of
heuristic lookup on preexisting lexicons. The base-
lines construct a lattice for each sentence using the
same method as lexicon-based model features (§5.2).
If multiple lexicons are used, the union of their en-
</bodyText>
<footnote confidence="0.436079">
18https://github.com/jhclark/ducttape/
</footnote>
<bodyText confidence="0.980094230769231">
tries is used to construct the lattice. The resulting
segmentation—which does not encode a strength
distinction—is evaluated against the gold standard.
Table 2 shows the results. Even with just the la-
beled training set as input, the supervised approach
beats the strongest heuristic baseline (that incorpo-
rates in-domain lexicon entries extracted from the
training data) by 30 precision points, while achieving
comparable recall. For example, the baseline (but not
the statistical model) incorrectly predicts an MWE in
places to eat in Baltimore (because eat in, meaning
‘eat at home,’ is listed in WordNet). The supervised
approach has learned not to trust WordNet too much
due to this sort of ambiguity. Downstream applica-
tions that currently use lexicon matching for MWE
identification (e.g., Ghoneim and Diab, 2013) likely
stand to benefit from our statistical approach.
6.2 How best to exploit MWE lexicons
(type-level information)?
For statistical tagging (right portion of table 2), using
more preexisting (out-of-domain) lexicons generally
improves recall; precision also improves a bit.
A lexicon of MWEs occurring in the non-held-out
training data at least twice19 (table 2, bottom right) is
marginally worse (better precision/worse recall) than
the best result using only preexisting lexicons.
</bodyText>
<subsectionHeader confidence="0.99856">
6.3 Variations on the base model
</subsectionHeader>
<bodyText confidence="0.985277555555556">
We experiment with some of the modeling alterna-
tives discussed in §5. Results appear in table 3 under
both the link-based and exact match evaluation cri-
teria. We note that the exact match scores are (as
expected) several points lower.
19If we train with access to the full lexicon of training
set MWEs, the learner credulously overfits to relying on that
lexicon—after all, it has perfect coverage of the training data!—
which proves fatal for the model at test time.
</bodyText>
<page confidence="0.986576">
200
</page>
<bodyText confidence="0.991705">
Recall-oriented cost. The recall-oriented cost
adds about 1 link-based F1 point, sacrificing precision
in favor of recall.
Unsupervised word clusters. When combined
with the recall-oriented cost, these produce a slight
improvement to precision/degradation to recall, im-
proving exact match F1 but not affecting link-based
F1. Only a few clusters receive high positive weight;
one of these consists of matter, joke, biggie, pun,
avail, clue, corkage, frills, worries, etc. These words
are diverse semantically, but all occur in collocations
with no, which is what makes the cluster coherent
and useful to the MWE model.
Oracle part-of-speech tags. Using human-
annotated rather than automatic POS tags improves
MWE identification by about 3 F1 points on test
(similar differences were observed in development).
</bodyText>
<subsectionHeader confidence="0.992589">
6.4 What are the highest-weighted features?
</subsectionHeader>
<bodyText confidence="0.99998208">
An advantage of the linear modeling framework is
that we can examine learned feature weights to gain
some insight into the model’s behavior.
In general, the highest-weighted features are the
lexicon matching features and features indicative of
proper names (POS tag of proper noun, capitalized
word not at the beginning of the sentence, etc.).
Despite the occasional cluster capturing colloca-
tional or idiomatic groupings, as described in the
previous section, the clusters appear to be mostly
useful for identifying words that tend to belong (or
not) to proper names. For example, the cluster with
street, road, freeway, highway, airport, etc., as well
as words outside of the cluster vocabulary, weigh
in favor of an MWE. A cluster with everyday desti-
nations (neighborhood, doctor, hotel, bank, dentist)
prefers non-MWEs, presumably because these words
are not typically part of proper names in this corpus.
This was from the best model using non-oracle POS
tags, so the clusters are perhaps useful in correct-
ing for proper nouns that were mistakenly tagged
as common nouns. One caveat, though, is that it is
hard to discern the impact of these specific features
where others may be capturing essentially the same
information.
</bodyText>
<subsectionHeader confidence="0.996388">
6.5 How heterogeneous are learned MWEs?
</subsectionHeader>
<bodyText confidence="0.986852">
On test, the final model (with automatic POS tags)
predicts 365 MWE instances (31 are gappy; 23 are
</bodyText>
<table confidence="0.991083133333333">
POS pattern # examples (lowercased lemmas)
NOUN NOUN 53 customer service, oil change
VERB PREP 36 work with, deal with, yell at
PROPN PROPN 29 eagle transmission, comfort zone
ADJ NOUN 21 major award, top notch, mental health
VERB PART 20 move out, end up, pick up, pass up
VERB ADV 17 come back, come in, come by, stay away
PREP NOUN 12 on time, in fact, in cash, for instance
VERB NOUN 10 take care, make money, give crap
VERB PRON 10 thank you, get it
PREP PREP 8 out of, due to, out ta, in between
ADV ADV 6 no matter, upfront, at all, early on
DET NOUN 6 a lot, a little, a bit, a deal
VERB DET NOUN 6 answer the phone, take a chance
NOUN PREP 5 kind of, care for, tip on, answer to
</table>
<tableCaption confidence="0.999953">
Table 4: Top predicted POS patterns and frequencies.
</tableCaption>
<bodyText confidence="0.99948444">
weak). There are 298 unique MWE types.
Organizing the predicted MWEs by their coarse
POS sequence reveals that the model is not too preju-
diced in the kinds of expressions it recognizes: the
298 types fall under 89 unique POS+strength patterns.
Table 4 shows the 14 POS sequences predicted 5 or
more times as strong MWEs. Some of the examples
(major award, a deal, tip on) are false positives, but
most are correct. Singleton patterns include PROPN
VERB (god forbid), PREP DET (at that), ADJ PRON
(worth it), and PREP VERB PREP (to die for).
True positive MWEs mostly consist of (a) named
entities, and (b) lexical idioms seen in training and/or
listed in one of the lexicons. Occasionally the sys-
tem correctly guesses an unseen and OOV idiom
based on features such as hyphenation (walk - in) and
capitalization/OOV words (Chili Relleno, BIG MIS-
TAKE). On test, 244 gold MWE types were unseen
in training; the system found 93 true positives (where
the type was predicted at least once), 109 false posi-
tives, and 151 false negatives—an unseen type recall
rate of 38%. Removing types that occurred in lexi-
cons leaves 35 true positives, 61 false positives, and
111 false negatives—a unseen and OOV type recall
rate of 24%.
</bodyText>
<subsectionHeader confidence="0.995636">
6.6 What kinds of mismatches occur?
</subsectionHeader>
<bodyText confidence="0.9960232">
Inspection of the output turns up false positives due
to ambiguity (e.g., Spongy and sweet bread); false
negatives (top to bottom); and overlap (get high qual-
ity service, gold get high quality service; live up to,
gold live up to). A number of the mismatches turn
</bodyText>
<page confidence="0.995595">
201
</page>
<table confidence="0.2295424">
scheme
no gaps, 1-level
no gaps, 2-level
gappy, 1-level
gappy, 2-level
</table>
<tableCaption confidence="0.982801666666667">
Table 5: Training with different tagging schemes. Results
are cross-validation averages on train. All schemes are
evaluated against the full gold standard (8 tags).
</tableCaption>
<bodyText confidence="0.9946964">
out to be problems with the gold standard, like hav-
ing our water shut off (gold having our water shut
off). This suggests that even noisy automatic taggers
might help identify annotation inconsistencies and
errors for manual correction.
</bodyText>
<subsectionHeader confidence="0.8899905">
6.7 Are gappiness and the strength distinction
learned in practice?
</subsectionHeader>
<bodyText confidence="0.999986352941176">
Three quarters of MWEs are strong and contain no
gaps. To see whether our model is actually sensi-
tive to the phenomena of gappiness and strength,
we train on data simplified to remove one or both
distinctions—as in the first 3 labelings in figure 2—
and evaluate against the full 8-tag scheme. For the
model with the recall cost, clusters, and oracle POS
tags, we evaluate each of these simplifications of
the training data in table 5. The gold standard for
evaluation remains the same across all conditions.
If the model was unable to recover gappy expres-
sions or the strong/weak distinction, we would expect
it to do no better when trained with the full tagset than
with the simplified tagset. However, there is some
loss in performance as the tagset for learning is sim-
plified, which suggests that gappiness and strength
are being learned to an extent.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999939423728814">
Our annotated corpus (Schneider et al., 2014) joins
several resources that indicate certain varieties of
MWEs: lexicons such as WordNet (Fellbaum, 1998),
SAID (Kuiper et al., 2003), and WikiMwe (Hartmann
et al., 2012); targeted lists (Baldwin, 2005, 2008;
Cook et al., 2008; Tu and Roth, 2011, 2012); web-
sites like Wiktionary and Phrases.net; and large-scale
corpora such as SemCor (Miller et al., 1993), the
French Treebank (Abeillé et al., 2003), the Szeged-
ParalellFX corpus (Vincze, 2012), and the Prague
Czech-English Dependency Treebank ( ˇCmejrek et al.,
2005). The difference is that Schneider et al. (2014)
pursued a comprehensive annotation approach rather
than targeting specific varieties of MWEs or relying
on a preexisting lexical resource. The annotations
are shallow, not relying explicitly on syntax (though
in principle they could be mapped onto the parses in
the Web Treebank).
In terms of modeling, the use of machine learn-
ing classification (Hashimoto and Kawahara, 2008;
Shigeto et al., 2013) and specifically BIO sequence
tagging (Diab and Bhutada, 2009; Constant and Si-
gogne, 2011; Constant et al., 2012; Vincze et al.,
2013) for contextual recognition of MWEs is not
new. Lexical semantic classification tasks like named
entity recognition (e.g., Ratinov and Roth, 2009), su-
persense tagging (Ciaramita and Altun, 2006; Paaß
and Reichartz, 2009), and index term identification
(Newman et al., 2012) also involve chunking of cer-
tain MWEs. But our discriminative models, facili-
tated by the new corpus, broaden the scope of the
MWE identification task to include many varieties of
MWEs at once, including explicit marking of gaps
and a strength distinction. By contrast, the afore-
mentioned identification systems, as well as some
MWE-enhanced syntactic parsers (e.g., Green et al.,
2012), have been restricted to contiguous MWEs.
However, Green et al. (2011) allow gaps to be de-
scribed as constituents in a syntax tree. Gimpel and
Smith’s (2011) shallow, gappy language model al-
lows arbitrary token groupings within a sentence,
whereas our model imposes projectivity and nest-
ing constraints (§3). Blunsom and Baldwin (2006)
present a sequence model for HPSG supertagging,
and evaluate performance on discontinuous MWEs,
though the sequence model treats the non-adjacent
component supertags like other labels—it cannot en-
force that they mutually require one another, as we
do via the gappy tagging scheme (§3.1). The lexicon
lookup procedures of Bejˇcek et al. (2013) can match
gappy MWEs, but are nonstatistical and extremely
error-prone when tuned for high oracle recall.
Another major thread of research has pursued un-
supervised discovery of multiword types from raw
corpora, such as with statistical association measures
(Church et al., 1991; Pecina, 2010; Ramisch et al.,
2012, inter alia), parallel corpora (Melamed, 1997;
Moirón and Tiedemann, 2006; Tsvetkov and Wint-
ner, 2010), or a combination thereof (Tsvetkov and
</bodyText>
<figure confidence="0.9117252">
JYJ ρ M JwJ P R F1
3 100 2.1 733k 73.33 55.72 63.20
4 150 3.3 977k 72.60 59.11 65.09
6 200 1.6 1,466k 66.48 61.26 63.65
8 100 3.5 1,954k 73.27 60.44 66.15
</figure>
<page confidence="0.995492">
202
</page>
<bodyText confidence="0.999908571428571">
Wintner, 2011); this may be followed by a lookup-
and-classify approach to contextual identification
(Ramisch et al., 2010). Though preliminary experi-
ments with our models did not show benefit to incor-
porating such automatically constructed lexicons, we
hope these two perspectives can be brought together
in future work.
</bodyText>
<sectionHeader confidence="0.986364" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999970958333333">
This article has presented the first supervised model
for identifying heterogeneous multiword expressions
in English text. Our feature-rich discriminative se-
quence tagger performs shallow chunking with a
novel scheme that allows for MWEs containing gaps,
and includes a strength distinction to separate highly
idiomatic expressions from collocations. It is trained
and evaluated on a corpus of English web reviews
that are comprehensively annotated for multiword
expressions. Beyond the training data, its features in-
corporate evidence from external resources—several
lexicons as well as unsupervised word clusters; we
show experimentally that this statistical approach is
far superior to identifying MWEs by heuristic lexicon
lookup alone. Future extensions might integrate addi-
tional features (e.g., exploiting statistical association
measures computed over large corpora), enhance the
lexical representation (e.g., by adding semantic tags),
improve the expressiveness of the model (e.g., with
higher-order features and inference), or integrate the
model with other tasks (such as parsing and transla-
tion).
Our data and open source software are released at
http://www.ark.cs.cmu.edu/LexSem/.
</bodyText>
<sectionHeader confidence="0.984411" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999580666666667">
This research was supported in part by NSF CA-
REER grant IIS-1054319, Google through the Read-
ing is Believing project at CMU, and DARPA grant
FA8750-12-2-0342 funded under the DEFT program.
We are grateful to Kevin Knight, Martha Palmer,
Claire Bonial, Lori Levin, Ed Hovy, Tim Baldwin,
Omri Abend, members of JHU CLSP, the NLP group
at Berkeley, and the Noah’s ARK group at CMU, and
anonymous reviewers for valuable feedback.
</bodyText>
<figure confidence="0.970169972972973">
A Basic Features
All are conjoined with the current label, yi.
Label Features
1. previous label (the only first-order feature)
Token Features
Original token
2. i = {1,2}
3. i = JwJ−{0,1}
4. capitalized ∧ Qi = 0]
5. word shape
Lowercased token
4
6. prefix: [wi]k1 Ik=1
7. suffix: [wi]SwS
j ISwS
j=SwS−3
8. has digit
9. has non-alphanumeric c
10. context word: w =
1 lj=i−2
11. context word bigram: wj+1 Ii+1
j j=i−2
Lemma Features
12. lemma + context lemma if one of them is a verb and the other
is a noun, verb, adjective, adverb, preposition, or particle: λi ∧
λj i+2
Ij=i−2
Part-of-speech Features
13. context POS: posj Ii+2
j=i−2
14. context POS bigram: posj+1Ii+1
j j=i−2
15. word + context POS: wi ∧posi±1
16. context word + POS: wi±1 ∧posi
Lexicon Features (unlexicalized)
WordNet only
17. OOV: λi is not in WordNet as a unigram lemma ∧ posi
</figure>
<reference confidence="0.989532772727273">
18. compound: non-punctuation lemma λi and the {previous,
next} lemma in the sentence (if it is non-punctuation; an inter-
vening hyphen is allowed) form an entry in WordNet, possibly
separated by a hyphen or space
19. compound-hyphen: posi = HYPH ∧ previous and next tokens
form an entry in WordNet, possibly separated by a hyphen or
space
20. ambiguity class: if content word unigram λi is in WordNet,
the set of POS categories it can belong to; else posi if not a
content POS ∧ the POS of the longest MW match to which λi
belongs (if any) ∧ the position in that match (B or I)
For each multiword lexicon
21. lexicon name ∧ status of token i in the shortest path segmen-
tation (O, B, or I) ∧ subcategory of lexical entry whose match
includes token i, if matched ∧ whether the match is gappy
22. the above ∧ POS tags of the first and last matched tokens in
the expression
Over all multiword lexicons
23. at least k lexicons contain a match that includes this token (if
n ≥ 1 matches, n active features)
24. at least k lexicons contain a match that includes this token,
starts with a given POS, and ends with a given POS
</reference>
<page confidence="0.999619">
203
</page>
<sectionHeader confidence="0.99017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989324852941176">
Anne Abeillé, Lionel Clément, and François Toussenel.
2003. Building a treebank for French. In Anne Abeillé
and Nancy Ide, editors, Treebanks, volume 20 of Text,
Speech and Language Technology, pages 165–187.
Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
Timothy Baldwin. 2005. Looking for prepositional verbs
in corpus data. In Proc. of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics Formalisms
and Applications, pages 115–126. Colchester, UK.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proc. of MWE, pages 1–2. Marrakech,
Morocco.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, Florida, USA.
Eduard Bejˇcek, Pavel Straˇnák, and Pavel Pecina. 2013.
Syntactic identification of occurrences of multiword
expressions in text using a lexicon with dependency
structures. In Proc. of the 9th Workshop on Multiword
Expressions, pages 106–115. Atlanta, Georgia, USA.
Gábor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proc. of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1162–1170. Chiang Mai, Thailand.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012a. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012b. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natu-
ral Language Processing with Python: Analyzing Text
with the Natural Language Toolkit. O’Reilly Media,
Inc., Sebastopol, California, USA.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proc. of EMNLP, pages 164–171. Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in sta-
tistical machine translation. In Proc. of NAACL-HLT,
pages 242–245. Los Angeles, California, USA.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical acquisition: exploiting
on-line resources to build a lexicon, pages 115–164.
Lawrence Erlbaum Associates, Hillsdale, New Jersey,
USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extrac-
tion with a supersense sequence tagger. In Proc. of
EMNLP, pages 594–602. Sydney, Australia.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments
with perceptron algorithms. In Proc. of EMNLP, pages
1–8. Philadelphia, Pennsylvania, USA.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World, pages 49–56. Portland, Oregon, USA.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin.
2012. Discriminative strategies to integrate multiword
expression recognition and parsing. In Proc. of ACL,
pages 204–212. Jeju Island, Korea.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.
The VNC-Tokens dataset. In Proc. of MWE, pages
19–22. Marrakech, Morocco.
Hal Daumé, III. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. disserta-
tion, University of Southern California, Los Angeles,
California, USA. URL http://hal3.name/docs/
daume06thesis.pdf.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction MWE token classification. In Proc. of MWE,
pages 17–22. Suntec, Singapore.
Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard.
2008. Formulaic language in native and second lan-
guage speakers: psycholinguistics, corpus linguistics,
and TESOL. TESOL Quarterly, 42(3):375–396.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Paul Kay, and Mary Catherine
O’Connor. 1988. Regularity and idiomaticity in gram-
matical constructions: the case of ‘let alone’. Language,
64(3):501–538.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
Mahmoud Ghoneim and Mona Diab. 2013. Multiword
expressions in the context of statistical machine trans-
</reference>
<page confidence="0.994411">
204
</page>
<reference confidence="0.999058336538462">
lation. In Proc. of IJCNLP, pages 1181–1187. Nagoya,
Japan.
Kevin Gimpel and Noah A. Smith. 2011. Generative
models of monolingual and bilingual gappy patterns.
In Proc. of WMT, pages 512–522. Edinburgh, Scotland,
UK.
Adele E. Goldberg. 1995. Constructions: a construction
grammar approach to argument structure. University
of Chicago Press, Chicago, Illinois, USA.
Adele E. Goldberg. 2006. Constructions at work: the
nature of generalization in language. Oxford University
Press, Oxford, UK.
Edouard Grave, Guillaume Obozinski, and Francis Bach.
2013. Hidden Markov tree models for semantic class
induction. In Proc. of CoNLL, pages 94–103. Sofia,
Bulgaria.
Spence Green, Marie-Catherine de Marneffe, John Bauer,
and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: a
parsing tour de force with French. In Proc. of EMNLP,
pages 725–735. Edinburgh, Scotland, UK.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2012. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195–227.
Jan Hajiˇc, Eva Hajiˇcová, Jarmila Panevová, Petr Sgall,
Silvie Cinková, Eva Fuˇcíková, Marie Mikulová, Petr
Pajas, Jan Popelka, Jiˇrí Semecký, Jana Šindlerová, Jan
Štˇepánek, Josef Toman, Zdeˇnka Urešová, and Zdenˇek
Žabokrtský. 2012. Prague Czech-English Dependency
Treebank 2.0. Technical Report LDC2012T08, Linguis-
tic Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
Silvana Hartmann, György Szarvas, and Iryna Gurevych.
2012. Mining multiword terms from Wikipedia. In
Maria Teresa Pazienza and Armando Stellato, editors,
Semi-Automatic Ontology Development. IGI Global,
Hershey, Pennsylvania, USA.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proc. of EMNLP, pages 992–1001.
Honolulu, Hawaii, USA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-08: HLT, pages 595–603. Columbus, Ohio.
Koenraad Kuiper, Heather McCann, Heidi Quinn,
Therese Aitchison, and Kees van der Veer. 2003.
SAID. Technical Report LDC2003T10, Linguistic
Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In
Proc. of ICML, pages 282–289.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts In-
stitute of Technology, Cambridge, Massachusetts,
USA. URL http://people.csail.mit.edu/
pliang/papers/meng-thesis.pdf.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc.
of EMNLP, pages 97–108. Providence, Rhode Island,
USA.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303–308. Plainsboro, New Jersey,
USA.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proc. of HLT-NAACL, pages 337–342.
Boston, Massachusetts, USA.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Ke-
mal Oflazer, and Noah A. Smith. 2012. Recall-oriented
learning of named entities in Arabic Wikipedia. In
Proc. of EACL, pages 162–173. Avignon, France.
Begona Villada Moirón and Jörg Tiedemann. 2006. Iden-
tifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL 2006 Workshop
on Multi-word Expressions in a Multilingual Context,
pages 33–40. Trento, Italy.
Rosamund Moon. 1998. Fixed expressions and idioms
in English: a corpus-based approach. Oxford Stud-
ies in Lexicography and Lexicology. Clarendon Press,
Oxford, UK.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmentation
for index term identification and keyphrase extraction.
In Proc. of COLING 2012, pages 2077–2092. Mumbai,
India.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow. 1994.
Idioms. Language, 70(3):491–538.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL-HLT,
pages 380–390. Atlanta, Georgia, USA.
Gerhard Paaß and Frank Reichartz. 2009. Exploiting
</reference>
<page confidence="0.980321">
205
</page>
<reference confidence="0.999608848484849">
semantic constraints for estimating supersenses with
CRFs. In Proc. of the Ninth SIAM International Confer-
ence on Data Mining, pages 485–496. Sparks, Nevada,
USA.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Evalu-
ation, 44(1):137–158.
Carlos Ramisch. 2012. A generic and open
framework for multiword expressions treatment:
from acquisition to applications. Ph.D. disser-
tation, University of Grenoble and Federal Uni-
versity of Rio Grande do Sul, Grenoble, France.
URL http://www.inf.ufrgs.br/~ceramisch/
download_files/thesis-getalp.pdf.
Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.
2012. A broad evaluation of techniques for automatic
acquisition of multiword expressions. In Proc. of ACL
2012 Student Research Workshop, pages 1–6. Jeju Is-
land, Korea.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. mwetoolkit: a framework for multiword expres-
sion identification. In Proc. of LREC, pages 662–669.
Valletta, Malta.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Proc.
of the Third ACL Workshop on Very Large Corpora,
pages 82–94. Cambridge, Massachusetts, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL, pages 147–155. Boulder, Colorado, USA.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485–510.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proc. of EMNLP, pages 1524–1534. Edin-
burgh, Scotland, UK.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
a pain in the neck for NLP. In Alexander Gelbukh,
editor, Computational Linguistics and Intelligent Text
Processing, volume 2276 of Lecture Notes in Computer
Science, pages 189–206. Springer, Berlin, Germany.
Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily
Danchik, Michael T. Mordowanec, Henrietta Conrad,
and Noah A. Smith. 2014. Comprehensive annotation
of multiword expressions in a social web corpus. In
Proc. of LREC. Reykjavík, Iceland.
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei
Kondo, Tomoya Kouse, Keisuke Sakaguchi, Akifumi
Yoshimoto, Frances Yung, and Yuji Matsumoto. 2013.
Construction of English MWE dictionary and its appli-
cation to POS tagging. In Proc. of the 9th Workshop
on Multiword Expressions, pages 139–144. Atlanta,
Georgia, USA.
James W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization of
finite automata theory. Journal of Computer and System
Sciences, 1(4):317–322.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173–180. Edmonton, Alberta,
Canada.
Yulia Tsvetkov and Shuly Wintner. 2010. Extraction of
multi-word expressions from small parallel corpora.
In Coling 2010: Posters, pages 1256–1264. Beijing,
China.
Yulia Tsvetkov and Shuly Wintner. 2011. Identification
of multi-word expressions by combining multiple lin-
guistic information sources. In Proc. of EMNLP, pages
836–845. Edinburgh, Scotland, UK.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
Proc. of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages 31–39.
Portland, Oregon, USA.
Yuancheng Tu and Dan Roth. 2012. Sorting out the most
confusing English phrasal verbs. In Proc. of *SEM,
pages 65–69. Montréal, Quebec, Canada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of ACL,
pages 384–394. Uppsala, Sweden.
Martin ˇCmejrek, Jan Cuˇrín, Jan Hajiˇc, and Jiˇrí Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73–78. Budapest, Hungary.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proc. of MUC-6, pages
45–52. Columbia, Maryland, USA.
Veronika Vincze. 2012. Light verb constructions in the
SzegedParalellFX English-Hungarian parallel corpus.
In Proc. of LREC. Istanbul, Turkey.
Veronika Vincze, István Nagy T., and János Zsibrita. 2013.
Learning to detect English and Hungarian light verb
constructions. ACM Transactions on Speech and Lan-
guage Processing, 10(2):6:1–6:25.
</reference>
<page confidence="0.998892">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898485">
<title confidence="0.9979465">Discriminative Lexical Semantic Segmentation with Running the MWE Gamut</title>
<author confidence="0.999986">Nathan Schneider Emily Danchik Chris Dyer Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997864">Pittsburgh, PA 15213, USA</address>
<email confidence="0.99979">nschneid@cs.cmu.edu</email>
<email confidence="0.99979">emilydan@cs.cmu.edu</email>
<email confidence="0.99979">cdyer@cs.cmu.edu</email>
<email confidence="0.99979">nasmith@cs.cmu.edu</email>
<abstract confidence="0.994141941176471">We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) a sentence, resulting in a seman- Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for featurerich discriminative models. Experiments on a new dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types. Our statistical sequence model greatly outperforms a lookup-based segmentation proachieving nearly 60% MWE identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>compound: non-punctuation lemma λi and the {previous, next} lemma in the sentence (if it is non-punctuation; an intervening hyphen is allowed) form an entry in WordNet, possibly separated by a hyphen or space 19. compound-hyphen: posi = HYPH ∧ previous and next tokens form an entry in WordNet, possibly separated by a hyphen or space</title>
<marker></marker>
<rawString>18. compound: non-punctuation lemma λi and the {previous, next} lemma in the sentence (if it is non-punctuation; an intervening hyphen is allowed) form an entry in WordNet, possibly separated by a hyphen or space 19. compound-hyphen: posi = HYPH ∧ previous and next tokens form an entry in WordNet, possibly separated by a hyphen or space</rawString>
</citation>
<citation valid="false">
<title>ambiguity class: if content word unigram λi is in WordNet, the set of POS categories it can belong to; else posi if not a content POS ∧ the POS of the longest MW match to which λi belongs (if any) ∧ the position in that match (B or I) For each multiword lexicon 21. lexicon name ∧ status of token i in the shortest path segmentation (O, B, or I) ∧ subcategory of lexical entry whose match includes token i, if matched ∧ whether the match is gappy 22. the above ∧ POS tags of the first and last matched tokens in the expression Over all multiword lexicons 23. at least k lexicons contain a match that includes this token (if n ≥ 1 matches, n active features) 24. at least k lexicons contain a match that includes this token, starts with a given POS, and ends with a given POS</title>
<marker></marker>
<rawString>20. ambiguity class: if content word unigram λi is in WordNet, the set of POS categories it can belong to; else posi if not a content POS ∧ the POS of the longest MW match to which λi belongs (if any) ∧ the position in that match (B or I) For each multiword lexicon 21. lexicon name ∧ status of token i in the shortest path segmentation (O, B, or I) ∧ subcategory of lexical entry whose match includes token i, if matched ∧ whether the match is gappy 22. the above ∧ POS tags of the first and last matched tokens in the expression Over all multiword lexicons 23. at least k lexicons contain a match that includes this token (if n ≥ 1 matches, n active features) 24. at least k lexicons contain a match that includes this token, starts with a given POS, and ends with a given POS</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Abeillé</author>
<author>Lionel Clément</author>
<author>François Toussenel</author>
</authors>
<title>Building a treebank for French.</title>
<date>2003</date>
<booktitle>In Anne Abeillé and</booktitle>
<volume>20</volume>
<pages>165--187</pages>
<editor>Nancy Ide, editors, Treebanks,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="39628" citStr="Abeillé et al., 2003" startWordPosition="6426" endWordPosition="6429">owever, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tag</context>
</contexts>
<marker>Abeillé, Clément, Toussenel, 2003</marker>
<rawString>Anne Abeillé, Lionel Clément, and François Toussenel. 2003. Building a treebank for French. In Anne Abeillé and Nancy Ide, editors, Treebanks, volume 20 of Text, Speech and Language Technology, pages 165–187. Kluwer Academic Publishers, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>Looking for prepositional verbs in corpus data.</title>
<date>2005</date>
<booktitle>In Proc. of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications,</booktitle>
<pages>115--126</pages>
<publisher>Colchester, UK.</publisher>
<contexts>
<context position="39429" citStr="Baldwin, 2005" startWordPosition="6395" endWordPosition="6396">s. If the model was unable to recover gappy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could </context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005. Looking for prepositional verbs in corpus data. In Proc. of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications, pages 115–126. Colchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>A resource for evaluating the deep lexical acquisition of English verb-particle constructions.</title>
<date>2008</date>
<booktitle>In Proc. of MWE,</booktitle>
<pages>1--2</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="26350" citStr="Baldwin, 2008" startWordPosition="4298" endWordPosition="4299">performance.) Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of each cross-validation fold at least once (lookup) or twice (model). All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviations are shown for the F1 score. In each column, the highest value using only preexisting lexicons is underlined, and the highest overall value is bolded. The boxed row indicates the configuration used as the basis for subsequent experiments. the verb-particle constructions (VPCs) dataset of (Baldwin, 2008); a list of light verb constructions (LVCs) provided by Claire Bonial; and two idioms websites.12 After preprocessing, each lexical entry consists of an ordered sequence of word lemmas, some of which may be variables like &lt;something&gt;. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised </context>
</contexts>
<marker>Baldwin, 2008</marker>
<rawString>Timothy Baldwin. 2008. A resource for evaluating the deep lexical acquisition of English verb-particle constructions. In Proc. of MWE, pages 1–2. Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su Nam Kim</author>
</authors>
<title>Multiword expressions.</title>
<date>2010</date>
<booktitle>In Nitin Indurkhya</booktitle>
<editor>and Fred J. Damerau, editors,</editor>
<publisher>CRC Press, Taylor and Francis Group,</publisher>
<location>Boca Raton, Florida, USA.</location>
<contexts>
<context position="4149" citStr="Baldwin and Kim (2010)" startWordPosition="604" endWordPosition="607">uistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation </context>
</contexts>
<marker>Baldwin, Kim, 2010</marker>
<rawString>Timothy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group, Boca Raton, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Bejˇcek</author>
<author>Pavel Straˇnák</author>
<author>Pavel Pecina</author>
</authors>
<title>Syntactic identification of occurrences of multiword expressions in text using a lexicon with dependency structures.</title>
<date>2013</date>
<booktitle>In Proc. of the 9th Workshop on Multiword Expressions,</booktitle>
<pages>106--115</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Bejˇcek, Straˇnák, Pecina, 2013</marker>
<rawString>Eduard Bejˇcek, Pavel Straˇnák, and Pavel Pecina. 2013. Syntactic identification of occurrences of multiword expressions in text using a lexicon with dependency structures. In Proc. of the 9th Workshop on Multiword Expressions, pages 106–115. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gábor Berend</author>
</authors>
<title>Opinion expression mining by exploiting keyphrase extraction.</title>
<date>2011</date>
<booktitle>In Proc. of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1162--1170</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="3824" citStr="Berend, 2011" startWordPosition="557" endWordPosition="558">tic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Trad</context>
</contexts>
<marker>Berend, 2011</marker>
<rawString>Gábor Berend. 2011. Opinion expression mining by exploiting keyphrase extraction. In Proc. of 5th International Joint Conference on Natural Language Processing, pages 1162–1170. Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<title>English Web Treebank.</title>
<date>2012</date>
<tech>Technical Report LDC2012T13,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="5081" citStr="Bies et al., 2012" startWordPosition="751" endWordPosition="754">ed certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the annotated corpus (§6). Finally, in §7 and §8 we comment on related work and future directions. 2 Annotated Co</context>
<context position="7542" citStr="Bies et al., 2012" startWordPosition="1139" endWordPosition="1142">eral-purpose MWE identification, such as would be desirable for use by high-coverage downstream NLP systems. It is used to train and evaluate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1http://www.ark.cs.cmu.edu/LexSem/ 2Because we use treebank data, syntac</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012a. English Web Treebank. Technical Report LDC2012T13, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<title>English Web Treebank.</title>
<date>2012</date>
<tech>Technical Report LDC2012T13,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="5081" citStr="Bies et al., 2012" startWordPosition="751" endWordPosition="754">ed certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the annotated corpus (§6). Finally, in §7 and §8 we comment on related work and future directions. 2 Annotated Co</context>
<context position="7542" citStr="Bies et al., 2012" startWordPosition="1139" endWordPosition="1142">eral-purpose MWE identification, such as would be desirable for use by high-coverage downstream NLP systems. It is used to train and evaluate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1http://www.ark.cs.cmu.edu/LexSem/ 2Because we use treebank data, syntac</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012b. English Web Treebank. Technical Report LDC2012T13, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc.,</publisher>
<location>Sebastopol, California, USA.</location>
<contexts>
<context position="24609" citStr="Bird et al., 2009" startWordPosition="4041" endWordPosition="4044">0 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary; WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); the named entities and other MWEs in the WSJ corpus on the English side of the CEDT (Hajiˇc et al., 2012); 10The WordNet API in NLTK (Bird et al., 2009) was used for lemmatization. 11http://en.wiktionary.org; data obtained from https://toolserver.org/~enwikt/definitions/ enwikt-defs-20130814-en.tsv.gz 11 the 198 LOOKUP SUPERVISED MODEL entries max gap P R F1 σ P R F1 σ 0 length 74.39 44.43 55.57 2.19 71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.90 420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.67 437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30 1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.65 2 lexicons + MWtypes(train)&gt;1 6 lexicons + MWtypes(train)&gt;2 preexising lexicons none WordNet + SemCor 6lexicons 10 lexicons best configuration</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly Media, Inc., Sebastopol, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Timothy Baldwin</author>
</authors>
<title>Multilingual deep lexical acquisition for HPSGs via supertagging. In</title>
<date>2006</date>
<booktitle>Proc. of EMNLP,</booktitle>
<pages>164--171</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="41324" citStr="Blunsom and Baldwin (2006)" startWordPosition="6688" endWordPosition="6691">rpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) shallow, gappy language model allows arbitrary token groupings within a sentence, whereas our model imposes projectivity and nesting constraints (§3). Blunsom and Baldwin (2006) present a sequence model for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures</context>
</contexts>
<marker>Blunsom, Baldwin, 2006</marker>
<rawString>Phil Blunsom and Timothy Baldwin. 2006. Multilingual deep lexical acquisition for HPSGs via supertagging. In Proc. of EMNLP, pages 164–171. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="27512" citStr="Brown et al., 1992" startWordPosition="4480" endWordPosition="4483">ramming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12http://www.phrases.net/ and http://home. postech.ac.kr/~oyz/doc/idiom.html 13Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters on the 21-million-word Yelp Academic Dataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token conjoi</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Mona Diab</author>
</authors>
<title>Task-based evaluation of multiword expressions: a pilot study in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>242--245</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="3748" citStr="Carpuat and Diab, 2010" startWordPosition="545" endWordPosition="548">units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing c</context>
</contexts>
<marker>Carpuat, Diab, 2010</marker>
<rawString>Marine Carpuat and Mona Diab. 2010. Task-based evaluation of multiword expressions: a pilot study in statistical machine translation. In Proc. of NAACL-HLT, pages 242–245. Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
</authors>
<title>Using statistics in lexical analysis.</title>
<date>1991</date>
<booktitle>Lawrence Erlbaum Associates,</booktitle>
<pages>115--164</pages>
<editor>In Uri Zernik, editor,</editor>
<location>Hillsdale, New Jersey, USA.</location>
<contexts>
<context position="41945" citStr="Church et al., 1991" startWordPosition="6782" endWordPosition="6785">resent a sequence model for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope thes</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1991</marker>
<rawString>Kenneth Church, William Gale, Patrick Hanks, and Donald Hindle. 1991. Using statistics in lexical analysis. In Uri Zernik, editor, Lexical acquisition: exploiting on-line resources to build a lexicon, pages 115–164. Lawrence Erlbaum Associates, Hillsdale, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="40522" citStr="Ciaramita and Altun, 2006" startWordPosition="6562" endWordPosition="6565">ng on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. G</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger. In Proc. of EMNLP, pages 594–602. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="21007" citStr="Collins, 2002" startWordPosition="3452" endWordPosition="3453">ression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8Hierarchical modeling based on our representations is left to future work. 197 constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learning procedure, algorithm 1, generalizes the clas</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: theory and experiments with perceptron algorithms. In Proc. of EMNLP, pages 1–8. Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Anthony Sigogne</author>
</authors>
<title>MWUaware part-of-speech tagging with a CRF model and lexical resources.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,</booktitle>
<pages>49--56</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="20624" citStr="Constant and Sigogne, 2011" startWordPosition="3392" endWordPosition="3395">ehave like their out-of-gap counterparts. Gappy, 2-level (8 tags). 8 tags are required to encode the 2-level scheme with gaps: {O o B b I¯ i I˜ i}. Variants of the inside tag are marked for strength of the incoming link—this applies gap-externally (capitalized tags) and gap-internally (lowercase tags). If I¯ or I˜ immediately follows a gap, its diacritic reflects the strength of the gappy expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8Hierarchical modeling based on our representations is left to future work. 197 constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whos</context>
<context position="31060" citStr="Constant and Sigogne, 2011" startWordPosition="5025" endWordPosition="5028">ases to improve, use the previous model. The two hyperparameters are the number of iterations and the value of the recall cost hyperparameter (ρ). Both are tuned via cross-validation on train; we use the multiple of 50 that maximizes average link-based F1. The chosen values are shown in table 3. Experiments were managed with the ducttape tool.18 6 Results We experimentally address the following questions to probe and justify our modeling approach. 6.1 Is supervised learning necessary? Previous MWE identification studies have found benefit to statistical learning over heuristic lexicon lookup (Constant and Sigogne, 2011; Green et al., 2012). Our first experiment tests whether this holds for comprehensive MWE identification: it compares our supervised tagging approach with baselines of heuristic lookup on preexisting lexicons. The baselines construct a lattice for each sentence using the same method as lexicon-based model features (§5.2). If multiple lexicons are used, the union of their en18https://github.com/jhclark/ducttape/ tries is used to construct the lattice. The resulting segmentation—which does not encode a strength distinction—is evaluated against the gold standard. Table 2 shows the results. Even </context>
<context position="40284" citStr="Constant and Sigogne, 2011" startWordPosition="6525" endWordPosition="6529">(Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforemen</context>
</contexts>
<marker>Constant, Sigogne, 2011</marker>
<rawString>Matthieu Constant and Anthony Sigogne. 2011. MWUaware part-of-speech tagging with a CRF model and lexical resources. In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World, pages 49–56. Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Anthony Sigogne</author>
<author>Patrick Watrin</author>
</authors>
<title>Discriminative strategies to integrate multiword expression recognition and parsing.</title>
<date>2012</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>204--212</pages>
<location>Jeju Island,</location>
<contexts>
<context position="23857" citStr="Constant et al. (2012)" startWordPosition="3914" endWordPosition="3917">ˆ ← argmaxy′ (wTg(x,y′)+cost(y,y′,x)) if yˆ≠ y then w ← w+g(x,y)−g(x,ˆy) w ← w+tg(x,y)−tg(x, ˆy) end t ← t +1 end end Output: w−(wlt) Algorithm 1: Training with the averaged perceptron. (Adapted from Daumé, 2006, p. 19.) experiments showed that a cost function penalizing all recall errors—i.e., with pQy∗ ≠ O ∧y′ = O� as the second term, as in Mohit et al.—tended to append additional tokens to high-confidence MWEs (such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary; WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); </context>
<context position="40307" citStr="Constant et al., 2012" startWordPosition="6530" endWordPosition="6533">ue Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification s</context>
</contexts>
<marker>Constant, Sigogne, Watrin, 2012</marker>
<rawString>Matthieu Constant, Anthony Sigogne, and Patrick Watrin. 2012. Discriminative strategies to integrate multiword expression recognition and parsing. In Proc. of ACL, pages 204–212. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>The VNC-Tokens dataset.</title>
<date>2008</date>
<booktitle>In Proc. of MWE,</booktitle>
<pages>19--22</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="39454" citStr="Cook et al., 2008" startWordPosition="6398" endWordPosition="6401">nable to recover gappy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2008</marker>
<rawString>Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008. The VNC-Tokens dataset. In Proc. of MWE, pages 19–22. Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
</authors>
<title>Practical structured learning techniques for natural language processing.</title>
<date>2006</date>
<institution>University of Southern</institution>
<location>California, Los Angeles, California, USA.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="23446" citStr="Daumé, 2006" startWordPosition="3846" endWordPosition="3847">. The difference is that we only penalize beginning-of-expression recall errors. Preliminary 9The 8-tag scheme licenses 42 tag bigrams: sequences such as B O and o 1 are prohibited. There are also constraints on the allowed tags at the beginning and end of the sequence. Input: data ((x(n),y(n)))Nn=1; number of iterations M w ← 0 w ← 0 t ← 1 for m = 1 to M do for n = 1 to N do (x,y) ← (x(n),y(n)) yˆ ← argmaxy′ (wTg(x,y′)+cost(y,y′,x)) if yˆ≠ y then w ← w+g(x,y)−g(x,ˆy) w ← w+tg(x,y)−tg(x, ˆy) end t ← t +1 end end Output: w−(wlt) Algorithm 1: Training with the averaged perceptron. (Adapted from Daumé, 2006, p. 19.) experiments showed that a cost function penalizing all recall errors—i.e., with pQy∗ ≠ O ∧y′ = O� as the second term, as in Mohit et al.—tended to append additional tokens to high-confidence MWEs (such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists t</context>
</contexts>
<marker>Daumé, 2006</marker>
<rawString>Hal Daumé, III. 2006. Practical structured learning techniques for natural language processing. Ph.D. dissertation, University of Southern California, Los Angeles, California, USA. URL http://hal3.name/docs/ daume06thesis.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Pravin Bhutada</author>
</authors>
<title>Verb noun construction MWE token classification.</title>
<date>2009</date>
<booktitle>In Proc. of MWE,</booktitle>
<pages>17--22</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="40256" citStr="Diab and Bhutada, 2009" startWordPosition="6521" endWordPosition="6524">SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinctio</context>
</contexts>
<marker>Diab, Bhutada, 2009</marker>
<rawString>Mona Diab and Pravin Bhutada. 2009. Verb noun construction MWE token classification. In Proc. of MWE, pages 17–22. Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick C Ellis</author>
<author>Rita Simpson-Vlach</author>
<author>Carson Maynard</author>
</authors>
<title>Formulaic language in native and second language speakers: psycholinguistics, corpus linguistics, and TESOL.</title>
<date>2008</date>
<journal>TESOL Quarterly,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="3875" citStr="Ellis et al., 2008" startWordPosition="563" endWordPosition="566">) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain </context>
</contexts>
<marker>Ellis, Simpson-Vlach, Maynard, 2008</marker>
<rawString>Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard. 2008. Formulaic language in native and second language speakers: psycholinguistics, corpus linguistics, and TESOL. TESOL Quarterly, 42(3):375–396.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Paul Kay</author>
<author>Mary Catherine O’Connor</author>
</authors>
<title>Regularity and idiomaticity in grammatical constructions: the case of ‘let alone’.</title>
<date>1988</date>
<journal>Language,</journal>
<volume>64</volume>
<issue>3</issue>
<marker>Fillmore, Kay, O’Connor, 1988</marker>
<rawString>Charles J. Fillmore, Paul Kay, and Mary Catherine O’Connor. 1988. Regularity and idiomaticity in grammatical constructions: the case of ‘let alone’. Language, 64(3):501–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="21659" citStr="Freund and Schapire, 1999" startWordPosition="3544" endWordPosition="3547">r discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learning procedure, algorithm 1, generalizes the classic perceptron algorithm (Freund and Schapire, 1999) to incorporate a structured decoding step (for sequences, the Viterbi algorithm) in the inner loop. Thus, training requires only max inference, which is fast with a first-order Markov assumption. In training, features are adjusted where a tagging error is made; the procedure can be viewed as optimizing the structured hinge loss. The output of learning is a weight vector that parametrizes a feature-rich scoring function over candidate labelings of a sequence. To better align the learning algorithm with our F-score–based MWE evaluation (§3.2), we use a cost-augmented version of the structured p</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahmoud Ghoneim</author>
<author>Mona Diab</author>
</authors>
<title>Multiword expressions in the context of statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>1181--1187</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="32295" citStr="Ghoneim and Diab, 2013" startWordPosition="5209" endWordPosition="5212">e labeled training set as input, the supervised approach beats the strongest heuristic baseline (that incorporates in-domain lexicon entries extracted from the training data) by 30 precision points, while achieving comparable recall. For example, the baseline (but not the statistical model) incorrectly predicts an MWE in places to eat in Baltimore (because eat in, meaning ‘eat at home,’ is listed in WordNet). The supervised approach has learned not to trust WordNet too much due to this sort of ambiguity. Downstream applications that currently use lexicon matching for MWE identification (e.g., Ghoneim and Diab, 2013) likely stand to benefit from our statistical approach. 6.2 How best to exploit MWE lexicons (type-level information)? For statistical tagging (right portion of table 2), using more preexisting (out-of-domain) lexicons generally improves recall; precision also improves a bit. A lexicon of MWEs occurring in the non-held-out training data at least twice19 (table 2, bottom right) is marginally worse (better precision/worse recall) than the best result using only preexisting lexicons. 6.3 Variations on the base model We experiment with some of the modeling alternatives discussed in §5. Results app</context>
</contexts>
<marker>Ghoneim, Diab, 2013</marker>
<rawString>Mahmoud Ghoneim and Mona Diab. 2013. Multiword expressions in the context of statistical machine translation. In Proc. of IJCNLP, pages 1181–1187. Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Generative models of monolingual and bilingual gappy patterns.</title>
<date>2011</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>512--522</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Gimpel, Smith, 2011</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2011. Generative models of monolingual and bilingual gappy patterns. In Proc. of WMT, pages 512–522. Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions: a construction grammar approach to argument structure.</title>
<date>1995</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="2862" citStr="Goldberg, 1995" startWordPosition="419" endWordPosition="420">nective: as well as, let alone, in spite of 12. semi-fixed VP: pick up where &lt;one&gt; left off 13. fixed phrase: scared to death, leave of absence 14. phatic: You’re welcome. Me neither! 15. proverb: Beggars can’t be choosers. Figure 1: Some of the classes of idioms in English. The examples included here contain multiple lexicalized words—with the exception of those in (3), if the conventional single-word (SW) spelling is used. difficult to circumscribe, that entire theories of syntax are predicated on the notion that constructions with idiosyncratic form-meaning mappings (Fillmore et al., 1988; Goldberg, 1995) or statistical properties (Goldberg, 2006) offer crucial evidence about the grammatical organization of language. Here we focus on multiword expressions (MWEs): lexicalized combinations of two or more words that are exceptional enough to be considered as single units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have divers</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Adele E. Goldberg. 1995. Constructions: a construction grammar approach to argument structure. University of Chicago Press, Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions at work: the nature of generalization in language.</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="2905" citStr="Goldberg, 2006" startWordPosition="424" endWordPosition="425"> 12. semi-fixed VP: pick up where &lt;one&gt; left off 13. fixed phrase: scared to death, leave of absence 14. phatic: You’re welcome. Me neither! 15. proverb: Beggars can’t be choosers. Figure 1: Some of the classes of idioms in English. The examples included here contain multiple lexicalized words—with the exception of those in (3), if the conventional single-word (SW) spelling is used. difficult to circumscribe, that entire theories of syntax are predicated on the notion that constructions with idiosyncratic form-meaning mappings (Fillmore et al., 1988; Goldberg, 1995) or statistical properties (Goldberg, 2006) offer crucial evidence about the grammatical organization of language. Here we focus on multiword expressions (MWEs): lexicalized combinations of two or more words that are exceptional enough to be considered as single units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association </context>
</contexts>
<marker>Goldberg, 2006</marker>
<rawString>Adele E. Goldberg. 2006. Constructions at work: the nature of generalization in language. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edouard Grave</author>
<author>Guillaume Obozinski</author>
<author>Francis Bach</author>
</authors>
<title>Hidden Markov tree models for semantic class induction.</title>
<date>2013</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>94--103</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="27222" citStr="Grave et al., 2013" startWordPosition="4436" endWordPosition="4439">nd one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12http://www.phrases.net/ and http://home. postech.ac.kr/~oyz/doc/idiom.html 13Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters </context>
</contexts>
<marker>Grave, Obozinski, Bach, 2013</marker>
<rawString>Edouard Grave, Guillaume Obozinski, and Francis Bach. 2013. Hidden Markov tree models for semantic class induction. In Proc. of CoNLL, pages 94–103. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
</authors>
<title>Multiword expression identification with tree substitution grammars: a parsing tour de force with French.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>725--735</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Green, de Marneffe, Bauer, Manning, 2011</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christopher D. Manning. 2011. Multiword expression identification with tree substitution grammars: a parsing tour de force with French. In Proc. of EMNLP, pages 725–735. Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing models for identifying multiword expressions.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Green, de Marneffe, Manning, 2012</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, and Christopher D. Manning. 2012. Parsing models for identifying multiword expressions. Computational Linguistics, 39(1):195–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcová</author>
<author>Jarmila Panevová</author>
<author>Petr Sgall</author>
<author>Silvie Cinková</author>
<author>Eva Fuˇcíková</author>
<author>Marie Mikulová</author>
<author>Petr Pajas</author>
<author>Jan Popelka</author>
<author>Jiˇrí Semecký</author>
<author>Jana Šindlerová</author>
<author>Jan Štˇepánek</author>
<author>Josef Toman</author>
</authors>
<title>Zdeˇnka Urešová, and Zdenˇek Žabokrtský.</title>
<date>2012</date>
<tech>Technical Report LDC2012T08,</tech>
<institution>Linguistic Data Consortium,</institution>
<location>Philadelphia, Pennsylvania, USA.</location>
<marker>Hajiˇc, Hajiˇcová, Panevová, Sgall, Cinková, Fuˇcíková, Mikulová, Pajas, Popelka, Semecký, Šindlerová, Štˇepánek, Toman, 2012</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcová, Jarmila Panevová, Petr Sgall, Silvie Cinková, Eva Fuˇcíková, Marie Mikulová, Petr Pajas, Jan Popelka, Jiˇrí Semecký, Jana Šindlerová, Jan Štˇepánek, Josef Toman, Zdeˇnka Urešová, and Zdenˇek Žabokrtský. 2012. Prague Czech-English Dependency Treebank 2.0. Technical Report LDC2012T08, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="false">
<note>URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T10.</note>
<marker></marker>
<rawString>URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvana Hartmann</author>
</authors>
<title>György Szarvas, and Iryna Gurevych.</title>
<date>2012</date>
<booktitle>In Maria Teresa Pazienza and Armando Stellato, editors, Semi-Automatic Ontology Development. IGI Global,</booktitle>
<location>Hershey, Pennsylvania, USA.</location>
<marker>Hartmann, 2012</marker>
<rawString>Silvana Hartmann, György Szarvas, and Iryna Gurevych. 2012. Mining multiword terms from Wikipedia. In Maria Teresa Pazienza and Armando Stellato, editors, Semi-Automatic Ontology Development. IGI Global, Hershey, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Construction of an idiom corpus and its application to idiom identification based on WSD incorporating idiomspecific features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>992--1001</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="40171" citStr="Hashimoto and Kawahara, 2008" startWordPosition="6508" endWordPosition="6511">pora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many va</context>
</contexts>
<marker>Hashimoto, Kawahara, 2008</marker>
<rawString>Chikara Hashimoto and Daisuke Kawahara. 2008. Construction of an idiom corpus and its application to idiom identification based on WSD incorporating idiomspecific features. In Proc. of EMNLP, pages 992–1001. Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="27158" citStr="Koo et al., 2008" startWordPosition="4424" endWordPosition="4427">f which may be variables like &lt;something&gt;. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12http://www.phrases.net/ and http://home. postech.ac.kr/~oyz/doc/idiom.html 13Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14With Liang’s (2005) implementation: https:/</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL-08: HLT, pages 595–603. Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenraad Kuiper</author>
<author>Heather McCann</author>
<author>Heidi Quinn</author>
<author>Therese Aitchison</author>
<author>Kees van der Veer</author>
</authors>
<date>2003</date>
<booktitle>SAID. Technical Report LDC2003T10, Linguistic Data Consortium,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<note>URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T10.</note>
<marker>Kuiper, McCann, Quinn, Aitchison, van der Veer, 2003</marker>
<rawString>Koenraad Kuiper, Heather McCann, Heidi Quinn, Therese Aitchison, and Kees van der Veer. 2003. SAID. Technical Report LDC2003T10, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="20961" citStr="Lafferty et al., 2001" startWordPosition="3444" endWordPosition="3447">, its diacritic reflects the strength of the gappy expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8Hierarchical modeling based on our representations is left to future work. 197 constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learnin</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, Massachusetts, USA.</location>
<note>URL http://people.csail.mit.edu/ pliang/papers/meng-thesis.pdf.</note>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA. URL http://people.csail.mit.edu/ pliang/papers/meng-thesis.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7679" citStr="Marcus et al., 1993" startWordPosition="1162" endWordPosition="1165">valuate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1http://www.ark.cs.cmu.edu/LexSem/ 2Because we use treebank data, syntactic parses are available to assist in post hoc analysis. Syntactic information was not shown to annotators. 194 # of constituent tokens #</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional compounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>97--108</pages>
<location>Providence, Rhode Island, USA.</location>
<contexts>
<context position="42027" citStr="Melamed, 1997" startWordPosition="6796" endWordPosition="6797"> MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This artic</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic discovery of noncompositional compounds in parallel data. In Proc. of EMNLP, pages 97–108. Providence, Rhode Island, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. of HLT,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, New Jersey, USA.</location>
<contexts>
<context position="24273" citStr="Miller et al., 1993" startWordPosition="3984" endWordPosition="3987">such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary; WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); the named entities and other MWEs in the WSJ corpus on the English side of the CEDT (Hajiˇc et al., 2012); 10The WordNet API in NLTK (Bird et al., 2009) was used for lemmatization. 11http://en.wiktionary.org; data obtained from https://toolserver.org/~enwikt/definitions/ enwikt-defs-20130814-en.tsv.gz 11 the 198 LOOKUP SUPERVISED MODEL entries max gap P R F1 σ P R F1 σ 0 length 74.39 44.43 55.57 2.19 71k 0 46.15 </context>
<context position="39584" citStr="Miller et al., 1993" startWordPosition="6419" endWordPosition="6422">l tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proc. of HLT, pages 303–308. Plainsboro, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>337--342</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="27140" citStr="Miller et al., 2004" startWordPosition="4420" endWordPosition="4423">f word lemmas, some of which may be variables like &lt;something&gt;. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12http://www.phrases.net/ and http://home. postech.ac.kr/~oyz/doc/idiom.html 13Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14With Liang’s (2005) imple</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proc. of HLT-NAACL, pages 337–342. Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Nathan Schneider</author>
<author>Rishav Bhowmick</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Recall-oriented learning of named entities in Arabic Wikipedia.</title>
<date>2012</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>162--173</pages>
<location>Avignon, France.</location>
<contexts>
<context position="22835" citStr="Mohit et al. (2012)" startWordPosition="3732" endWordPosition="3735"> cost-augmented version of the structured perceptron that is sensitive to different kinds of errors during training. When recall is the bigger obstacle, we can adopt the following cost function: given a sentence x, its gold labeling y∗, and a candidate labeling y′, ly∗� cost(y∗,y′,x) = E c(y∗j,y′j) where j=1 c(y∗,y′) = Qy∗ ≠ y′�+pQy∗ ∈ {B,b}∧y′ ∈ {O,o}� A single nonnegative hyperparameter, p, controls the tradeoff between recall and accuracy; higher p biases the model in favor of recall (possibly hurting accuracy and precision). This is a slight variant of the recall-oriented cost function of Mohit et al. (2012). The difference is that we only penalize beginning-of-expression recall errors. Preliminary 9The 8-tag scheme licenses 42 tag bigrams: sequences such as B O and o 1 are prohibited. There are also constraints on the allowed tags at the beginning and end of the sequence. Input: data ((x(n),y(n)))Nn=1; number of iterations M w ← 0 w ← 0 t ← 1 for m = 1 to M do for n = 1 to N do (x,y) ← (x(n),y(n)) yˆ ← argmaxy′ (wTg(x,y′)+cost(y,y′,x)) if yˆ≠ y then w ← w+g(x,y)−g(x,ˆy) w ← w+tg(x,y)−tg(x, ˆy) end t ← t +1 end end Output: w−(wlt) Algorithm 1: Training with the averaged perceptron. (Adapted from </context>
</contexts>
<marker>Mohit, Schneider, Bhowmick, Oflazer, Smith, 2012</marker>
<rawString>Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, and Noah A. Smith. 2012. Recall-oriented learning of named entities in Arabic Wikipedia. In Proc. of EACL, pages 162–173. Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Begona Villada Moirón</author>
<author>Jörg Tiedemann</author>
</authors>
<title>Identifying idiomatic expressions using automatic wordalignment.</title>
<date>2006</date>
<booktitle>In Proc. of the EACL 2006 Workshop on Multi-word Expressions in a Multilingual Context,</booktitle>
<pages>33--40</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="42055" citStr="Moirón and Tiedemann, 2006" startWordPosition="6798" endWordPosition="6801">he sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first s</context>
</contexts>
<marker>Moirón, Tiedemann, 2006</marker>
<rawString>Begona Villada Moirón and Jörg Tiedemann. 2006. Identifying idiomatic expressions using automatic wordalignment. In Proc. of the EACL 2006 Workshop on Multi-word Expressions in a Multilingual Context, pages 33–40. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosamund Moon</author>
</authors>
<title>Fixed expressions and idioms in English: a corpus-based approach.</title>
<date>1998</date>
<booktitle>Oxford Studies in Lexicography and Lexicology.</booktitle>
<publisher>Clarendon Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="7441" citStr="Moon, 1998" startWordPosition="1123" endWordPosition="1124">ion schemes. By contrast, Schneider et al.’s (2014) corpus creates an opportunity to tackle general-purpose MWE identification, such as would be desirable for use by high-coverage downstream NLP systems. It is used to train and evaluate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constru</context>
</contexts>
<marker>Moon, 1998</marker>
<rawString>Rosamund Moon. 1998. Fixed expressions and idioms in English: a corpus-based approach. Oxford Studies in Lexicography and Lexicology. Clarendon Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Nagendra Koilada</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Bayesian text segmentation for index term identification and keyphrase extraction.</title>
<date>2012</date>
<booktitle>In Proc. of COLING 2012,</booktitle>
<pages>2077--2092</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="3793" citStr="Newman et al., 2012" startWordPosition="551" endWordPosition="554">WEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary as</context>
<context position="40602" citStr="Newman et al., 2012" startWordPosition="6574" endWordPosition="6577">itly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) shallow, gappy language model allows arbitrary token gr</context>
</contexts>
<marker>Newman, Koilada, Lau, Baldwin, 2012</marker>
<rawString>David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin. 2012. Bayesian text segmentation for index term identification and keyphrase extraction. In Proc. of COLING 2012, pages 2077–2092. Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
</authors>
<date>1994</date>
<journal>Idioms. Language,</journal>
<volume>70</volume>
<issue>3</issue>
<contexts>
<context position="7428" citStr="Nunberg et al., 1994" startWordPosition="1119" endWordPosition="1122">e datasets and evaluation schemes. By contrast, Schneider et al.’s (2014) corpus creates an opportunity to tackle general-purpose MWE identification, such as would be desirable for use by high-coverage downstream NLP systems. It is used to train and evaluate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide ran</context>
</contexts>
<marker>Nunberg, Sag, Wasow, 1994</marker>
<rawString>Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow. 1994. Idioms. Language, 70(3):491–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proc. of NAACL-HLT, pages 380–390. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Paaß</author>
<author>Frank Reichartz</author>
</authors>
<title>Exploiting semantic constraints for estimating supersenses with CRFs.</title>
<date>2009</date>
<booktitle>In Proc. of the Ninth SIAM International Conference on Data Mining,</booktitle>
<pages>485--496</pages>
<location>Sparks, Nevada, USA.</location>
<contexts>
<context position="40549" citStr="Paaß and Reichartz, 2009" startWordPosition="6566" endWordPosition="6569"> resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) sh</context>
</contexts>
<marker>Paaß, Reichartz, 2009</marker>
<rawString>Gerhard Paaß and Frank Reichartz. 2009. Exploiting semantic constraints for estimating supersenses with CRFs. In Proc. of the Ninth SIAM International Conference on Data Mining, pages 485–496. Sparks, Nevada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>Lexical association measures and collocation extraction.</title>
<date>2010</date>
<journal>Language Resources and Evaluation,</journal>
<volume>44</volume>
<issue>1</issue>
<contexts>
<context position="41959" citStr="Pecina, 2010" startWordPosition="6786" endWordPosition="6787">el for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspect</context>
</contexts>
<marker>Pecina, 2010</marker>
<rawString>Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evaluation, 44(1):137–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
</authors>
<title>A generic and open framework for multiword expressions treatment: from acquisition to applications.</title>
<date>2012</date>
<institution>University of Grenoble and Federal University of Rio Grande do Sul,</institution>
<location>Grenoble, France.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="4169" citStr="Ramisch (2012)" startWordPosition="609" endWordPosition="610">ction Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a </context>
</contexts>
<marker>Ramisch, 2012</marker>
<rawString>Carlos Ramisch. 2012. A generic and open framework for multiword expressions treatment: from acquisition to applications. Ph.D. dissertation, University of Grenoble and Federal University of Rio Grande do Sul, Grenoble, France. URL http://www.inf.ufrgs.br/~ceramisch/ download_files/thesis-getalp.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Vitor De Araujo</author>
<author>Aline Villavicencio</author>
</authors>
<title>A broad evaluation of techniques for automatic acquisition of multiword expressions.</title>
<date>2012</date>
<booktitle>In Proc. of ACL 2012 Student Research Workshop,</booktitle>
<pages>1--6</pages>
<location>Jeju Island,</location>
<marker>Ramisch, De Araujo, Villavicencio, 2012</marker>
<rawString>Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio. 2012. A broad evaluation of techniques for automatic acquisition of multiword expressions. In Proc. of ACL 2012 Student Research Workshop, pages 1–6. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Christian Boitet</author>
</authors>
<title>mwetoolkit: a framework for multiword expression identification.</title>
<date>2010</date>
<booktitle>In Proc. of LREC,</booktitle>
<pages>662--669</pages>
<location>Valletta,</location>
<contexts>
<context position="42405" citStr="Ramisch et al., 2010" startWordPosition="6863" endWordPosition="6866">jor thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength distinction to separate highly idiomatic expressions from collocations. It is trained and evaluated on a corp</context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010. mwetoolkit: a framework for multiword expression identification. In Proc. of LREC, pages 662–669. Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of the Third ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="18013" citStr="Ramshaw and Marcus, 1995" startWordPosition="2945" endWordPosition="2948">I˜ I˜ . (OIB(oIb[ii]+I[ O ¯I˜I])∗[ budge B budge B a b a b little i little i . (OIB(oIbi+II)∗I+)+ O on I me I lot to I I me I a I a B little I budge O on O was O to O price O which O the O he O means B willing O . (OIBI+)+ O no gaps, 1-level no gaps, 2-level lot to me I¯ I˜ I˜ . (OIB[ O little I¯ a B was O willing O budge O which O price O the O on O he O to O a I˜ means B ¯I ˜I]+)+ he O was O he O was O price O which O price O which O ¯I ˜I]+)+ to O to O score because F1 = F↑1 = F↓1 . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of the Third ACL Workshop on Very Large Corpora, pages 82–94. Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>147--155</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="18194" citStr="Ratinov and Roth, 2009" startWordPosition="2971" endWordPosition="2974">the O he O means B willing O . (OIBI+)+ O no gaps, 1-level no gaps, 2-level lot to me I¯ I˜ I˜ . (OIB[ O little I¯ a B was O willing O budge O which O price O the O on O he O to O a I˜ means B ¯I ˜I]+)+ he O was O he O was O price O which O price O which O ¯I ˜I]+)+ to O to O score because F1 = F↑1 = F↓1 . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the standard contiguous chunking representation from Ramshaw and Marcus (1995) using the tags {O B I}. O is for tokens outside any chunk; B marks tokens beginning a chunk; and I marks o</context>
<context position="40475" citStr="Ratinov and Roth, 2009" startWordPosition="6555" endWordPosition="6558">targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to b</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc. of CoNLL, pages 147–155. Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>04</issue>
<contexts>
<context position="16584" citStr="Recasens and Hovy, 2011" startWordPosition="2646" endWordPosition="2649">s or difficult cases. If where one annotation uses a weak link the other has a strong link or no link at all, we want to penalize the disagreement less than if one had a strong link and the other had no link. To accommodate the 2-level scheme, we therefore average F↑1 , in which all weak links have been converted to strong links, and F↓1 , in which they have been removed: F1= 1 2(F↑ 1 +F↓ 1 ).7 If neither annotation contains any weak links, this equals the MUC 5As a criterion for coreference resolution, the MUC measure has perceived shortcomings which have prompted several other measures (see Recasens and Hovy, 2011 for a review). It is not clear, however, whether any of these criticisms are relevant to MWE identification. 6A link between a and b is redundant if the other links already imply that a and b belong to the same set. A set of N elements is expressed non-redundantly with exactly N −1 links. 7Overall precision and recall are likewise computed by averaging “strengthened” and “weakened” measurements. bids taken1 in1. Ea,b:a ˆÐb QaÐ∗b� P= Ea,b:a ˆÐb 1 196 Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs.</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(04):485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1524--1534</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="28480" citStr="Ritter et al. (2011)" startWordPosition="4626" endWordPosition="4629">ataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token conjoins the word lemma with the cluster ID. Part-of-speech tags. We compared three PTBstyle POS taggers on the full REVIEWS subcorpus (train+test). The Stanford CoreNLP tagger16 (Toutanova et al., 2003) yields an accuracy of 90.4%. The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al., 2013) achieves 90.1% with the model17 trained on the Twitter corpus of Ritter et al. (2011), and 94.9% when trained on the ANSWERS, EMAIL, NEWSGROUP, and WEBLOG subcorpora of WTB. We use this third configuration to produce automatic POS tags for training and testing our MWE tagger. (A comparison condition in §6.3 uses oracle POS tags.) 5.3 Experimental Setup The corpus of web reviews described in §2 is used for training and evaluation. 101 arbitrarily chosen documents (500 sentences, 7,171 words) were held from words appearing at least 25 times. 15https://www.yelp.com/academic_dataset 16v. 3.2.0, with english-bidirectional-distsim 17http://www.ark.cs.cmu.edu/TweetNLP/model. ritter_p</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proc. of EMNLP, pages 1524–1534. Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: a pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2276</volume>
<pages>189--206</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="3376" citStr="Sag et al., 2002" startWordPosition="497" endWordPosition="500">ion that constructions with idiosyncratic form-meaning mappings (Fillmore et al., 1988; Goldberg, 1995) or statistical properties (Goldberg, 2006) offer crucial evidence about the grammatical organization of language. Here we focus on multiword expressions (MWEs): lexicalized combinations of two or more words that are exceptional enough to be considered as single units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. Submitted 12/2013; Revised 1/2014; Published 4/2014. c�2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop ling</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: a pain in the neck for NLP. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 2276 of Lecture Notes in Computer Science, pages 189–206. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Spencer Onuffer</author>
<author>Nora Kazour</author>
<author>Emily Danchik</author>
<author>Michael T Mordowanec</author>
<author>Henrietta Conrad</author>
<author>Noah A Smith</author>
</authors>
<title>Comprehensive annotation of multiword expressions in a social web corpus.</title>
<date>2014</date>
<booktitle>In Proc. of LREC.</booktitle>
<location>Reykjavík, Iceland.</location>
<contexts>
<context position="4972" citStr="Schneider et al. (2014)" startWordPosition="731" endWordPosition="734">rpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the </context>
<context position="10822" citStr="Schneider et al. (2014)" startWordPosition="1672" endWordPosition="1675">ong MWEs as constituents (but not vice versa). Strong groups are required to cohere when used inside weak groups: that is, a weak group cannot include only part of a strong group. For purposes of annotation, there were no constraints hinging on the ordering of tokens in the sentence. Process. MWE annotation proceeded one sentence at a time. The 6 annotators referred to and improved the guidelines document on an ongoing basis. Every sentence was seen independently by at least 2 annotators, and differences of opinion were discussed and resolved (often by marking a weak MWE as a compromise). See Schneider et al. (2014) for details. Statistics. The annotated corpus consists of 723 documents (3,812 sentences). MWEs are frequent in this domain: 57% of sentences (72% of sentences over 10 words long) and 88% of documents contain at least one MWE. 8,060~55,579=15% of tokens belong to an MWE; in total, there are 3,483 MWE instances. 544 (16%) are strong MWEs containing a gold-tagged proper noun—most are proper names. A breakdown appears in table 1. 3 Representation and Task Definition We define a lexical segmentation of a sentence as a partitioning of its tokens into segments such that each segment represents a si</context>
<context position="39226" citStr="Schneider et al., 2014" startWordPosition="6363" endWordPosition="6366"> For the model with the recall cost, clusters, and oracle POS tags, we evaluate each of these simplifications of the training data in table 5. The gold standard for evaluation remains the same across all conditions. If the model was unable to recover gappy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annota</context>
</contexts>
<marker>Schneider, Onuffer, Kazour, Danchik, Mordowanec, Conrad, Smith, 2014</marker>
<rawString>Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily Danchik, Michael T. Mordowanec, Henrietta Conrad, and Noah A. Smith. 2014. Comprehensive annotation of multiword expressions in a social web corpus. In Proc. of LREC. Reykjavík, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaro Shigeto</author>
<author>Ai Azuma</author>
<author>Sorami Hisamoto</author>
<author>Shuhei Kondo</author>
<author>Tomoya Kouse</author>
<author>Keisuke Sakaguchi</author>
<author>Akifumi Yoshimoto</author>
<author>Frances Yung</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Construction of English MWE dictionary and its application to POS tagging.</title>
<date>2013</date>
<booktitle>In Proc. of the 9th Workshop on Multiword Expressions,</booktitle>
<pages>139--144</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="40194" citStr="Shigeto et al., 2013" startWordPosition="6512" endWordPosition="6515"> al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once</context>
</contexts>
<marker>Shigeto, Azuma, Hisamoto, Kondo, Kouse, Sakaguchi, Yoshimoto, Yung, Matsumoto, 2013</marker>
<rawString>Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei Kondo, Tomoya Kouse, Keisuke Sakaguchi, Akifumi Yoshimoto, Frances Yung, and Yuji Matsumoto. 2013. Construction of English MWE dictionary and its application to POS tagging. In Proc. of the 9th Workshop on Multiword Expressions, pages 139–144. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Thatcher</author>
</authors>
<title>Characterizing derivation trees of context-free grammars through a generalization of finite automata theory.</title>
<date>1967</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="11986" citStr="Thatcher, 1967" startWordPosition="1858" endWordPosition="1859">nto segments such that each segment represents a single unit of lexical meaning. A multiword lexical expression may contain gaps, i.e. interruptions by other segments. We impose two restrictions on gaps that appear to be well-motivated linguistically: • Projectivity: Every expression filling a gap must be completely contained within that gap; gappy expressions may not interleave. • No nested gaps: A gap in an expression may be filled by other single- or multiword expressions, so long as those do not themselves contain gaps. Formal grammar. Our scheme corresponds to the following extended CFG (Thatcher, 1967), where S is the full sentence and terminals w are word tokens: S → X+ X →w+ (Y+ w+)∗ Y → w+ Each expression X or Y is lexicalized by the words in one or more underlined variables on the right-hand side. An X constituent may optionally contain one or more gaps filled by Y constituents, which must not contain gaps themselves.3 3MWEs with multiple gaps are rare but attested in data: e.g., putting me at my ease. We encountered one violation of the gap nesting constraint in the reviews data: I have21nothing21 but21 fantastic things2 to21say21 . Additionally, the interrupted phrase 195 Denoting mul</context>
</contexts>
<marker>Thatcher, 1967</marker>
<rawString>James W. Thatcher. 1967. Characterizing derivation trees of context-free grammars through a generalization of finite automata theory. Journal of Computer and System Sciences, 1(4):317–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL,</booktitle>
<pages>173--180</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="28309" citStr="Toutanova et al., 2003" startWordPosition="4596" endWordPosition="4599">ithin a gap has cost 1.25. 14With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters on the 21-million-word Yelp Academic Dataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token conjoins the word lemma with the cluster ID. Part-of-speech tags. We compared three PTBstyle POS taggers on the full REVIEWS subcorpus (train+test). The Stanford CoreNLP tagger16 (Toutanova et al., 2003) yields an accuracy of 90.4%. The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al., 2013) achieves 90.1% with the model17 trained on the Twitter corpus of Ritter et al. (2011), and 94.9% when trained on the ANSWERS, EMAIL, NEWSGROUP, and WEBLOG subcorpora of WTB. We use this third configuration to produce automatic POS tags for training and testing our MWE tagger. (A comparison condition in §6.3 uses oracle POS tags.) 5.3 Experimental Setup The corpus of web reviews described in §2 is used for training and evaluation. 101 arbitrarily chosen documents (500 sentences, 7,171 words) were held from wor</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL, pages 173–180. Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Shuly Wintner</author>
</authors>
<title>Extraction of multi-word expressions from small parallel corpora.</title>
<date>2010</date>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1256--1264</pages>
<location>Beijing, China.</location>
<contexts>
<context position="42084" citStr="Tsvetkov and Wintner, 2010" startWordPosition="6802" endWordPosition="6806"> non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and JYJ ρ M JwJ P R F1 3 100 2.1 733k 73.33 55.72 63.20 4 150 3.3 977k 72.60 59.11 65.09 6 200 1.6 1,466k 66.48 61.26 63.65 8 100 3.5 1,954k 73.27 60.44 66.15 202 Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifyi</context>
</contexts>
<marker>Tsvetkov, Wintner, 2010</marker>
<rawString>Yulia Tsvetkov and Shuly Wintner. 2010. Extraction of multi-word expressions from small parallel corpora. In Coling 2010: Posters, pages 1256–1264. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Shuly Wintner</author>
</authors>
<title>Identification of multi-word expressions by combining multiple linguistic information sources.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>836--845</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Tsvetkov, Wintner, 2011</marker>
<rawString>Yulia Tsvetkov and Shuly Wintner. 2011. Identification of multi-word expressions by combining multiple linguistic information sources. In Proc. of EMNLP, pages 836–845. Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuancheng Tu</author>
<author>Dan Roth</author>
</authors>
<title>Learning English light verb constructions: contextual or statistical.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,</booktitle>
<pages>31--39</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="39473" citStr="Tu and Roth, 2011" startWordPosition="6402" endWordPosition="6405">ppy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treeban</context>
</contexts>
<marker>Tu, Roth, 2011</marker>
<rawString>Yuancheng Tu and Dan Roth. 2011. Learning English light verb constructions: contextual or statistical. In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World, pages 31–39. Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuancheng Tu</author>
<author>Dan Roth</author>
</authors>
<title>Sorting out the most confusing English phrasal verbs.</title>
<date>2012</date>
<booktitle>In Proc. of *SEM,</booktitle>
<pages>65--69</pages>
<location>Montréal, Quebec, Canada.</location>
<marker>Tu, Roth, 2012</marker>
<rawString>Yuancheng Tu and Dan Roth. 2012. Sorting out the most confusing English phrasal verbs. In Proc. of *SEM, pages 65–69. Montréal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>384--394</pages>
<location>Uppsala,</location>
<contexts>
<context position="27179" citStr="Turian et al., 2010" startWordPosition="4428" endWordPosition="4431">iables like &lt;something&gt;. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12http://www.phrases.net/ and http://home. postech.ac.kr/~oyz/doc/idiom.html 13Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14With Liang’s (2005) implementation: https://github. com/percylia</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proc. of ACL, pages 384–394. Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin ˇCmejrek</author>
<author>Jan Cuˇrín</author>
<author>Jan Hajiˇc</author>
<author>Jiˇrí Havelka</author>
</authors>
<title>Prague Czech-English Dependency Treebank: resource for structure-based MT.</title>
<date>2005</date>
<booktitle>In Proc. of EAMT,</booktitle>
<pages>73--78</pages>
<location>Budapest, Hungary.</location>
<marker>ˇCmejrek, Cuˇrín, Hajiˇc, Havelka, 2005</marker>
<rawString>Martin ˇCmejrek, Jan Cuˇrín, Jan Hajiˇc, and Jiˇrí Havelka. 2005. Prague Czech-English Dependency Treebank: resource for structure-based MT. In Proc. of EAMT, pages 73–78. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proc. of MUC-6,</booktitle>
<pages>45--52</pages>
<location>Columbia, Maryland, USA.</location>
<contexts>
<context position="14275" citStr="Vilain et al., 1995" startWordPosition="2243" endWordPosition="2246">2 on1 the price which means4 a43 lot43 to4 me4. Subscripts denote strong MW groups and superscripts weak MW groups; unmarked tokens serve as single-word expressions. The MW groups are thus {budge, on}, {a, little}, {a, lot}, and {means, {a, lot}, to, me}. As should be evident from the grammar, the projectivity and gap-nesting constraints apply here just as in the 1-level scheme. 3.2 Evaluation Matching criteria. Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature. The MUC criterion (Vilain et al., 1995) measures precision and recall great gateways never1 before1 , so23 far23 as23 Hudson knew2 , seen1 by Europeans was annotated in another corpus. 4This was violated 6 times in our annotated data: modifiers within gaps are sometimes collocated with the gappy expression, as in on12 a12 tight1 budget12 and have12 little1 doubt12. of links in terms of groups (units) implied by the transitive closure over those links.5 It can be defined as follows: Let a Ð b denote a link between two elements in the gold standard, and a ˆÐb denote a link in the system prediction. Let the ∗ operator denote the trans</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proc. of MUC-6, pages 45–52. Columbia, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
</authors>
<title>Light verb constructions in the SzegedParalellFX English-Hungarian parallel corpus.</title>
<date>2012</date>
<booktitle>In Proc. of LREC.</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="39672" citStr="Vincze, 2012" startWordPosition="6434" endWordPosition="6435">set for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague Czech-English Dependency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and S</context>
</contexts>
<marker>Vincze, 2012</marker>
<rawString>Veronika Vincze. 2012. Light verb constructions in the SzegedParalellFX English-Hungarian parallel corpus. In Proc. of LREC. Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>István Nagy T</author>
<author>János Zsibrita</author>
</authors>
<title>Learning to detect English and Hungarian light verb constructions.</title>
<date>2013</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="40329" citStr="Vincze et al., 2013" startWordPosition="6534" endWordPosition="6537">ency Treebank ( ˇCmejrek et al., 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as som</context>
</contexts>
<marker>Vincze, T, Zsibrita, 2013</marker>
<rawString>Veronika Vincze, István Nagy T., and János Zsibrita. 2013. Learning to detect English and Hungarian light verb constructions. ACM Transactions on Speech and Language Processing, 10(2):6:1–6:25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
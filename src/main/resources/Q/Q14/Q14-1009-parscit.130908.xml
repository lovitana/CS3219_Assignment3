<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.988709">
Parallel Algorithms for Unsupervised Tagging
</title>
<author confidence="0.825072">
Sujith Ravi Sergei Vassilivitskii Vibhor Rastogi∗
</author>
<affiliation confidence="0.794006">
Google Google Twitter
</affiliation>
<address confidence="0.893495">
Mountain View, CA 94043 Mountain View, CA 94043 San Francisco, CA
</address>
<email confidence="0.99634">
sravi@google.com sergeiv@google.com vibhor.rastogi@gmail.com
</email>
<sectionHeader confidence="0.996636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667107142857">
We propose a new method for unsupervised
tagging that finds minimal models which are
then further improved by Expectation Max-
imization training. In contrast to previous
approaches that rely on manually specified
and multi-step heuristics for model minimiza-
tion, our approach is a simple greedy approx-
imation algorithm DMLC (DISTRIBUTED-
MINIMUM-LABEL-COVER) that solves this
objective in a single step.
We extend the method and show how to ef-
ficiently parallelize the algorithm on modern
parallel computing platforms while preserving
approximation guarantees. The new method
easily scales to large data and grammar sizes,
overcoming the memory bottleneck in previ-
ous approaches. We demonstrate the power
of the new algorithm by evaluating on various
sequence labeling tasks: Part-of-Speech tag-
ging for multiple languages (including low-
resource languages), with complete and in-
complete dictionaries, and supertagging, a
complex sequence labeling task, where the
grammar size alone can grow to millions of
entries. Our results show that for all of these
settings, our method achieves state-of-the-art
scalable performance that yields high quality
tagging outputs.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993839315789474">
Supervised sequence labeling with large labeled
training datasets is considered a solved problem. For
∗The research described herein was conducted while the
author was working at Google.
instance, state of the art systems obtain tagging ac-
curacies over 97% for part-of-speech (POS) tagging
on the English Penn Treebank. However, learning
accurate taggers without labeled data remains a chal-
lenge. The accuracies quickly drop when faced with
data from a different domain, language, or when
there is very little labeled information available for
training (Banko and Moore, 2004).
Recently, there has been an increasing amount
of research tackling this problem using unsuper-
vised methods. A popular approach is to learn from
POS-tag dictionaries (Merialdo, 1994), where we
are given a raw word sequence and a dictionary of
legal tags for each word type. Learning from POS-
tag dictionaries is still challenging. Complete word-
tag dictionaries may not always be available for use
and in every setting. When they are available, the
dictionaries are often noisy, resulting in high tag-
ging ambiguity. Furthermore, when applying tag-
gers in new domains or different datasets, we may
encounter new words that are missing from the dic-
tionary. There have been some efforts to learn POS
taggers from incomplete dictionaries by extending
the dictionary to include these words using some
heuristics (Toutanova and Johnson, 2008) or using
other methods such as type-supervision (Garrette
and Baldridge, 2012).
In this work, we tackle the problem of unsuper-
vised sequence labeling using tag dictionaries. The
first reported work on this problem was on POS tag-
ging from Merialdo (1994). The approach involved
training a standard Hidden Markov Model (HMM)
using the Expectation Maximization (EM) algo-
rithm (Dempster et al., 1977), though EM does not
</bodyText>
<page confidence="0.99393">
105
</page>
<bodyText confidence="0.962283529411765">
Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics.
perform well on this task (Johnson, 2007). More re-
cent methods have yielded better performance than
EM (see (Ravi and Knight, 2009) for an overview).
One interesting line of research introduced by
Ravi and Knight (2009) explores the idea of per-
forming model minimization followed by EM train-
ing to learn taggers. Their idea is closely related
to the classic Minimum Description Length princi-
ple for model selection (Barron et al., 1998). They
(1) formulate an objective function to find the small-
est model that explains the text (model minimization
step), and then, (2) fit the minimized model to the
data (EM step). For POS tagging, this method (Ravi
and Knight, 2009) yields the best performance to
date; 91.6% tagging accuracy on a standard test
dataset from the English Penn Treebank. The orig-
inal work from (Ravi and Knight, 2009) uses an in-
teger linear programming (ILP) formulation to find
minimal models, an approach which does not scale
to large datasets. Ravi et al. (2010b) introduced a
two-step greedy approximation to the original ob-
jective function (called the MIN-GREEDY algo-
rithm) that runs much faster while maintaining the
high tagging performance. Garrette and Baldridge
(2012) showed how to use several heuristics to fur-
ther improve this algorithm (for instance, better
choice of tag bigrams when breaking ties) and stack
other techniques on top, such as careful initialization
of HMM emission models which results in further
performance gains. Their method also works un-
der incomplete dictionary scenarios and can be ap-
plied to certain low-resource scenarios (Garrette and
Baldridge, 2013) by combining model minimization
with supervised training.
In this work, we propose a new scalable algorithm
for performing model minimization for this task. By
making an assumption on the structure of the solu-
tion, we prove that a variant of the greedy set cover
algorithm always finds an approximately optimal la-
bel set. This is in contrast to previous methods that
employ heuristic approaches with no guarantee on
the quality of the solution. In addition, we do not
have to rely on ad hoc tie-breaking procedures or
careful initializations for unknown words. Finally,
not only is the proposed method approximately op-
timal, it is also easy to distribute, allowing it to eas-
ily scale to very large datasets. We show empirically
that our method, combined with an EM training step
outperforms existing state of the art systems.
</bodyText>
<subsectionHeader confidence="0.772531">
1.1 Our Contributions
</subsectionHeader>
<listItem confidence="0.7652245">
• We present a new method, DISTRIBUTED
MINIMUM LABEL COVER, DMLC, for model
minimization that uses a fast, greedy algorithm
with formal approximation guarantees to the
quality of the solution.
• We show how to efficiently parallelize the al-
gorithm while preserving approximation guar-
antees. In contrast, existing minimization ap-
proaches cannot match the new distributed al-
gorithm when scaling from thousands to mil-
lions or even billions of tokens.
• We show that our method easily scales to both
large data and grammar sizes, and does not re-
quire the corpus or label set to fit into memory.
</listItem>
<bodyText confidence="0.791102153846154">
This allows us to tackle complex tagging tasks,
where the tagset consists of several thousand
labels, which results in more than one million
entries in the grammar.
• We demonstrate the power of the new
method by evaluating under several differ-
ent scenarios—POS tagging for multiple lan-
guages (including low-resource languages),
with complete and incomplete dictionaries, as
well as a complex sequence labeling task of su-
pertagging. Our results show that for all these
settings, our method achieves state-of-the-art
performance yielding high quality taggings.
</bodyText>
<sectionHeader confidence="0.999571" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999765">
Recently, there has been an increasing amount of
research tackling this problem from multiple di-
rections. Some efforts have focused on inducing
POS tag clusters without any tags (Christodoulopou-
los et al., 2010; Reichart et al., 2010; Moon et
al., 2010), but evaluating such systems proves dif-
ficult since it is not straightforward to map the clus-
ter labels onto gold standard tags. A more pop-
ular approach is to learn from POS-tag dictionar-
ies (Merialdo, 1994; Ravi and Knight, 2009), incom-
plete dictionaries (Hasan and Ng, 2009; Garrette and
Baldridge, 2012) and human-constructed dictionar-
ies (Goldberg et al., 2008).
</bodyText>
<page confidence="0.997352">
106
</page>
<bodyText confidence="0.999987454545455">
Another direction that has been explored in the
past includes bootstrapping taggers for a new lan-
guage based on information acquired from other lan-
guages (Das and Petrov, 2011) or limited annota-
tion resources (Garrette and Baldridge, 2013). Ad-
ditional work focused on building supervised tag-
gers for noisy domains such as Twitter (Gimpel et
al., 2011). While most of the relevant work in this
area centers on POS tagging, there has been some
work done for building taggers for more complex
sequence labeling tasks such as supertagging (Ravi
et al., 2010a).
Other related work include alternative methods
for learning sparse models via priors in Bayesian in-
ference (Goldwater and Griffiths, 2007) and poste-
rior regularization (Ganchev et al., 2010). But these
methods only encourage sparsity and do not explic-
itly seek to minimize the model size, which is the ob-
jective function used in this work. Moreover, taggers
learned using model minimization have been shown
to produce state-of-the-art results for the problems
discussed here.
</bodyText>
<sectionHeader confidence="0.990485" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99984925">
Following Ravi and Knight (2009), we formulate the
problem as that of label selection on the sentence
graph. Formally, we are given a set of sequences,
S = {S1, S2, ... , Sn} where each Si is a sequence
of words, Si = wi1, wi2, ... , wi,|Si|. With each
word wij we associate a set of possible tags Tij. We
will denote by m the total number of (possibly du-
plicate) words (tokens) in the corpus.
Additionally, we define two special words w0 and
w∞ with special tags start and end, and consider
the modified sequences S0 i = w0, Si, w∞. To sim-
plify notation, we will refer to w∞ = w|Si|+1. The
sequence label problem asks us to select a valid tag
tij ∈ Tij for each word wij in the input to minimize
a specific objective function.
We will refer to a tag pair (ti,j−1, tij) as a label.
Our aim is to minimize the number of distinct labels
used to cover the full input. Formally, given a se-
quence S0 i and a tag tij for each word wij in S0i, let
the induced set of labels for sequence S0 i be
</bodyText>
<equation confidence="0.91541125">
|S&apos;|
Li = U {(ti,j−1, tij)}.
j=1
The total number of distinct labels used over all se-
quences is then
�� �
φ = �� ∪i Li |=
i
</equation>
<bodyText confidence="0.999441571428571">
Note that the order of the tokens in the label makes
a difference as {(NN, VP)} and {(VP, NN)} are two
distinct labels.
Now we can define the problem formally, follow-
ing (Ravi and Knight, 2009).
Problem 1 (Minimum Label Cover). Given a set S
of sequences of words, where each word wij has a
set of valid tags Tij, the problem is to find a valid tag
assignment tij ∈ Tij for each word that minimizes
the number of distinct labels or tag pairs over all
sequences, φ = Ui U�sai+1{(ti,j−1, tij) }|.
The problem is closely related to the classical Set
Cover problem and is also NP-complete. To reduce
Set Cover to the label selection problem, map each
element i of the Set Cover instance to a single word
sentence Si = wi1, and let the valid tags Ti1 con-
tain the names of the sets that contain element i.
Consider a solution to the label selection problem;
every sentence Si is covered by two labels (w0, ki)
and (ki, w∞), for some ki ∈ Ti1, which corresponds
to an element i being covered by set ki in the Set
Cover instance. Thus any valid solution to the label
selection problem leads to a feasible solution to the
Set Cover problem ({k1, k2,...}) of exactly half the
size.
Finally, we will use {{...}} notation to denote a
multiset of elements, i.e. a set where an element may
appear multiple times.
</bodyText>
<sectionHeader confidence="0.994888" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.999712625">
In this Section, we describe the DISTRIBUTED-
MINIMUM-LABEL-COVER, DMLC, algorithm for
approximately solving the minimum label cover
problem. We describe the algorithm in a central-
ized setting, and defer the distributed implementa-
tion to Section 5. Before describing the algorithm,
we briefly explain the relationship of the minimum
label cover problem to set cover.
</bodyText>
<subsectionHeader confidence="0.999905">
4.1 Modification of Set Cover
</subsectionHeader>
<bodyText confidence="0.99771">
As we pointed out earlier, the minimum label cover
problem is at least as hard as the Set Cover prob-
</bodyText>
<equation confidence="0.989132333333333">
|Si|+1
U {(ti,j−1, tij)}|.
j=1
</equation>
<page confidence="0.991189">
107
</page>
<listItem confidence="0.959627351351351">
Algorithm 1: MLC Algorithm
1: Input: A set of sequences S with each words wij
having possible tags Tij.
2: Output: A tag assignment tij ∈ Tij for each word
wij approximately minimizing labels.
3: (Graph Creation) Initialize each vertex vij with the
set of possible tags Tij and its neighbors vi,j+1 and
vi,j−1.
4: repeat
5: (Message Passing) Each vertex vij sends its pos-
sibly tags Tij to its forward neighbor vij+1.
6: (Counter Update) Each vertex receives the
the tags Ti,j−1 and adds all possible labels
{(s, s0)|s ∈ Ti,j−1, s0 ∈ Tij} to a global counter
(M).
7: (MaxLabel Selection) Each vertex queries the
global counter M to find the maximum label
(t, t0).
8: (Tentative Assignment) Each vertex vij selects a
tag tentatively as follows: If one of the tags t, t0
is in the feasible set Tij, it tentatively selects the
tag.
9: (Random Assignment) If both are feasible it se-
lects one at random. The vertex communicates
its assignment to its neighbors.
10: (Confirmed Assignment) Each vertex receives
the tentative assignment from its neighbors. If
together with its neighbors it can match the se-
lected label, the assignment is finalized. If the
assigned tag is T, then the vertex vij sets the
valid tag set Tij to {t}.
11: until no unassigned vertices exist.
Algorithm 2: DMLC Implementation
1: Input: A set of sequences S with each
words wij having possible tags Tij.
2: Output: A tag assignment tij ∈ Tij for
each word wij approximately minimizing
</listItem>
<figure confidence="0.6745225">
labels.
3: Let M be the multi set of all possible labels
generated by choosing each possible tag t ∈
Tij.
M =U ⎛ |Si|+1 U {{(t0, t)}} ⎞
i ⎜ ⎜ ⎜ ⎝ U t&apos;∈Ti,j−1 ⎠ ⎟ ⎟ ⎟
j=1 t∈Tij
(1)
4: Let L = ∅ be the set of selected labels.
5: repeat
</figure>
<bodyText confidence="0.954101151515152">
6: Select the most frequent label not yet se-
lected: (t0, t) = arg max(s,,s)/∈L |M ∩
(s0, s)|.
7: For each bigram (wi,j−1, wij) where t0 ∈
Ti,j−1 and t ∈ Tij tentatively assign t0 to
wi,j−1 and t to wij. Add (t0, t) to L.
8: If a word gets two assignments, select
one at random with equal probability.
9: If a bigram (wi,j−1, wij) is consistent
with assignments in (t0, t), fix the tenta-
tive assignments, and set Ti,j−1 = {t0}
and Tij = {t}. Recompute M, the multi-
set of possible labels, with the updated
Ti,j−1 and Tij.
10: until there are no unassigned words
lem. An additional challenge comes from the fact
that labels are tags for a pair of words, and hence
are related. For example, if we label a word pair
(wi,j−1, wij) as (NN, VP), then the label for the next
word pair (wij, wi,j+1) has to be of the form (VP, *),
i.e., it has to start with VP.
Previous work (Ravi et al., 2010a; Ravi et al.,
2010b) recognized this challenge and employed two
phase heuristic approaches. Eschewing heuristics,
we will show that with one natural assumption, even
with this extra set of constraints, the standard greedy
algorithm for this problem results in a solution with
a provable approximation ratio of O(log m). In
practice, however, the algorithm performs far better
than the worst case ratio, and similar to the work
of (Gomes et al., 2006), we find that the greedy
approach selects a cover approximately 11% worse
than the optimum solution.
</bodyText>
<subsectionHeader confidence="0.977899">
4.2 MLC Algorithm
</subsectionHeader>
<bodyText confidence="0.999798333333333">
We present in Algorithm 1 our MINIMUM LABEL
COVER algorithm to approximately solve the mini-
mum label cover problem. The algorithm is simple,
efficient, and easy to distribute.
The algorithm chooses labels one at a time, select-
ing a label that covers as many words as possible in
</bodyText>
<page confidence="0.995948">
108
</page>
<bodyText confidence="0.993429308510638">
every iteration. For this, it generates and maintains
a multi-set of all possible labels M (Step 3). The
multi-set contains an occurrence of each valid label,
for example, if wi,j−1 has two possible valid tags
NN and VP, and wij has one possible valid tag VP,
then M will contain two labels, namely (NN, VP)
and (VP, VP). Since M is a multi-set it will contain
duplicates, e.g. the label (NN, VP) will appear for
each adjacent pair of words that have NN and VP as
valid tags, respectively.
In each iteration, the algorithm picks a label with
the most number of occurrences in M and adds it to
the set of chosen labels (Step 6). Intuitively, this is
a greedy step to select a label that covers the most
number of word pairs.
Once the algorithm picks a label (t0, t), it tries to
assign as many words to tags t or t0 as possible (Step
7). A word can be assigned t0 if t0 is a valid tag for
it, and t a valid tag for the next word in sequence.
Similarly, a word can be assigned t, if t is a valid
tag for it, and t0 a valid tag for the previous word.
Some words can get both assignments, in which case
we choose one tentatively at random (Step 8). If
a word’s tentative random tag, say t, is consistent
with the choices of its adjacent words (say t0 from
the previous word), then the tentative choice is fixed
as a permanent one. Whenever a tag is selected, the
set of valid tags Tij for the word is reduced to a sin-
gleton {t}. Once the set of valid tags Tij changes,
the multi-set M of all possible labels also changes,
as seen from Eq 1. The multi-set is then recom-
puted (Step 9) and the iterations repeated until all
of words have been tagged.
We can show that under a natural assumption this
simple algorithm is approximately optimal.
Assumption 1(c-feasibility). Let c ≥ 1 be any num-
ber, and k be the size of the optimal solution to the
original problem. In each iteration, the MLC algo-
rithm fixes the tags for some words. We say that the
algorithm is c-feasible, if after each iteration there
exists some solution to the remaining problem, con-
sistent with the chosen tags, with size at most ck .
The assumption encodes the fact that a single bad
greedy choice is not going to destroy the overall
structure of the solution, and a nearly optimal so-
lution remains. We note that this assumption of c-
feasibility is not only sufficient, as we will formally
show, but is also necessary. Indeed, without any as-
sumptions, once the algorithm fixes the tag for some
words, an optimal label may no longer be consis-
tent with the chosen tags, and it is not hard to find
contrived examples where the size of the optimal so-
lution doubles after each iteration of MLC.
Since the underlying problem is NP-complete, it
is computationally hard to give direct evidence ver-
ifying the assumption on natural language inputs.
However, on small examples we are able to show
that the greedy algorithm is within a small constant
factor of the optimum, specifically it is within 11%
of the optimum model size for the POS tagging
problem using the standard 24k dataset (Ravi and
Knight, 2009). Combined with the fact that the final
method outperforms state of the art approaches, this
leads us to conclude that the structural assumption is
well justified.
Lemma 1. Under the assumption of c-feasibility,
the MLC algorithm achieves a O(clog m) approx-
imation to the minimum label cover problem, where
m = Ei |Si |is the total number of tokens.
Proof. To prove the Lemma we will define an objec-
tive function ¯φ, counting the number of unlabeled
word pairs, as a function of possible labels, and
show that φ¯ decreases by a factor of (1− O(1/ck)) at
every iteration.
To define ¯φ, we first define φ, the number of la-
beled word pairs. Consider a particular set of la-
bels, L = {L1, L2, ... , Lk} where each label is a
pair (ti,tj). Call {tij} a valid assignment of to-
kens if for each wij, we have tij E Tij. Then the
score of L under an assignment t, which we denote
by φt, is the number of bigram labels that appear in
L. Formally, φt(L) =  |∪i,j {{(ti,j−1, tij) n L}}|.
Finally, we define φ(L) to be the best such assign-
ment, φ(L) = maxt φt(L), and ¯φ(L) = m − φ(L)
the number of uncovered labels.
Consider the label selected by the algorithm in ev-
ery step. By the c-feasibility assumption, there ex-
ists some solution having ck labels. Thus, some la-
bel from that solution covers at least a 1/ck fraction
of the remaining words. The selected label (t, t0)
maximizes the intersection with the remaining fea-
sible labels. The conflict resolution step ensures that
in expectation the realized benefit is at least a half
of the maximum, thereby reducing φ¯ by at least a
</bodyText>
<page confidence="0.993978">
109
</page>
<bodyText confidence="0.7369055">
(1 − 1/2ck) fraction. Therefore, after O(kclogm)
operations all of the labels are covered.
</bodyText>
<subsectionHeader confidence="0.993349">
4.3 Fitting the Model Using EM
</subsectionHeader>
<bodyText confidence="0.99999275">
Once the greedy algorithm terminates and returns a
minimized grammar of tag bigrams, we follow the
approach of Ravi and Knight (2009) and fit the min-
imized model to the data using the alternating EM
strategy.
In this step, we run an alternating optimization
procedure iteratively in phases. In each phase,
we initialize (and prune away) parameters within
the two HMM components (transition or emission
model) using the output from the previous phase.
We initialize this procedure by restricting the tran-
sition parameters to only those tag bigrams selected
in the model minimization step. We train in con-
junction with the original emission model using EM
algorithm which prunes away some of the emission
parameters. In the next phase, we alternate the ini-
tialization by choosing the pruned emission model
along with the original transition model (with full
set of tag bigrams) and retrain using EM. The alter-
nating EM iterations are terminated when the change
in the size of the observed grammar (i.e., the number
of unique bigrams in the tagging output) is ≤ 5%.1
We refer to our entire approach using greedy mini-
mization followed by EM training as DMLC + EM.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="method">
5 Distributed Implementation
</sectionHeader>
<bodyText confidence="0.999992857142857">
The DMLC algorithm is directly suited towards
parallelization across many machines. We turn to
Pregel (Malewicz et al., 2010), and its open source
version Giraph (Apa, 2013). In these systems the
computation proceeds in rounds. In every round, ev-
ery machine does some local processing and then
sends arbitrary messages to other machines. Se-
mantically, we think of the communication graph as
fixed, and in each round each vertex performs some
local computation and then sends messages to its
neighbors. This mode of parallel programming di-
rects the programmers to “Think like a vertex.”
The specific systems like Pregel and Giraph build
infrastructure that ensures that the overall system
</bodyText>
<footnote confidence="0.909761666666667">
1For more details on the alternating EM strategy and how
initialization with minimized models improve EM performance
in alternating iterations, refer to (Ravi and Knight, 2009).
</footnote>
<bodyText confidence="0.999726882352941">
is fault tolerant, efficient, and fast. In addition,
they provide implementation of commonly used dis-
tributed data structures, such as, for example global
counters. The programmer’s job is simply to specify
the code that each vertex will run at every round.
We implemented the DMLC algorithm in Pregel.
The implementation is straightforward and given in
Algorithm 2. The multi-set M of Algorithm 1 is
represented as a global counter in Algorithm 2. The
message passing (Step 3) and counter update (Step
4) steps update this global counter and hence per-
form the role of Step 3 of Algorithm 1. Step 5 se-
lects the label with largest count, which is equivalent
to the greedy label picking step 6 of Algorithm 1. Fi-
nally steps 6, 7, and 8 update the tag assignment of
each vertex performing the roles of steps 7, 8, and 9,
respectively, of Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.99109">
5.1 Speeding up the Algorithm
</subsectionHeader>
<bodyText confidence="0.9995322">
The implementation described above directly copies
the sequential algorithm. Here we describe addi-
tional steps we took to further improve the parallel
running times.
Singleton Sets: As the parallel algorithm pro-
ceeds, the set of feasible sets associated with a node
slowly decreases. At some point there is only one
tag that a node can take on, however this tag is rare,
and so it takes a while for it to be selected using the
greedy strategy. Nevertheless, if a node and one of
its neighbors have only a single tag left, then it is
safe to assign the unique label 2.
Modifying the Graph: As is often the case, the
bottleneck in parallel computations is the commu-
nication. To reduce the amount of communication
we reduce the graph on the fly, removing nodes and
edges once they no longer play a role in the compu-
tation. This simple modification decreases the com-
munication time in later rounds as the total size of
the problem shrinks.
</bodyText>
<sectionHeader confidence="0.998396" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998949333333333">
In this Section, we describe the experimental setup
for various tasks, settings and compare empirical
performance of our method against several existing
</bodyText>
<footnote confidence="0.94436">
2We must judiciously initialize the global counter to take
care of this assignment, but this is easily accomplished.
</footnote>
<page confidence="0.995605">
110
</page>
<bodyText confidence="0.99998525">
baselines. The performance results for all systems
(on all tasks) are measured in terms of tagging accu-
racy, i.e. % of tokens from the test corpus that were
labeled correctly by the system.
</bodyText>
<subsectionHeader confidence="0.9172015">
6.1 Part-of-Speech Tagging Task
6.1.1 Tagging Using a Complete Dictionary
</subsectionHeader>
<bodyText confidence="0.982361071428572">
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task. The tagset consists of 45 dis-
tinct tag labels and the dictionary contains 57,388
word/tag pairs derived from the entire Penn Tree-
bank. Per-token ambiguity for the test data is about
1.5 tags/token. In addition to the standard 24k
dataset, we also train and test on larger data sets—
973k tokens from the Penn Treebank, 3M tokens
from PTB+Europarl (Koehn, 2005) data.
Methods: We evaluate and compare performance
for POS tagging using four different methods that
employ the model minimization idea combined with
EM training:
</bodyText>
<listItem confidence="0.999913111111111">
• EM: Training a bigram HMM model using EM
algorithm (Merialdo, 1994).
• ILP + EM: Minimizing grammar size using
integer linear programming, followed by EM
training (Ravi and Knight, 2009).
• MIN-GREEDY + EM: Minimizing grammar
size using the two-step greedy method (Ravi et
al., 2010b).
• DMLC + EM: This work.
</listItem>
<bodyText confidence="0.999714219512195">
Results: Table 1 shows the results for POS tag-
ging on English Penn Treebank data. On the smaller
test datasets, all of the model minimization strate-
gies (methods 2, 3, 4) tend to perform equally well,
yielding state-of-the-art results and large improve-
ment over standard EM. When training (and testing)
on larger corpora sizes, DMLC yields the best re-
ported performance on this task to date. A major
advantage of the new method is that it can easily
scale to large corpora sizes and the distributed na-
ture of the algorithm still permits fast, efficient op-
timization of the global objective function. So, un-
like the earlier methods (such as MIN-GREEDY) it
is fast enough to run on several millions of tokens
to yield additional performance gains (shown in last
column).
Speedups: We also observe a significant speedup
when using the parallelized version of the DMLC
algorithm. Performing model minimization on the
24k tokens dataset takes 55 seconds on a single ma-
chine, whereas parallelization permits model mini-
mization to be feasible even on large datasets. Fig 1
shows the running time for DMLC when run on a
cluster of 100 machines. We vary the input data
size from 1M word tokens to about 8M word tokens,
while holding the resources constant. Both the algo-
rithm and its distributed implementation in DMLC
are linear time operations as evident by the plot.
In fact, for comparison, we also plot a straight line
passing through the first two runtimes. The straight
line essentially plots runtimes corresponding to a
linear speedup. DMLC clearly achieves better run-
times showing even better than linear speedup. The
reason for this is that distributed version has a con-
stant overhead for initialization, independent of the
data size. While the running time for rest of the im-
plementation is linear in data size. Thus, as the data
size becomes larger, the constant overhead becomes
less significant, and the distributed implementation
appears to complete slightly faster as data size in-
creases.
</bodyText>
<figureCaption confidence="0.9583045">
Figure 1: Runtime vs. data size (measured in # of word
tokens) on 100 machines. For comparison, we also plot a
straight line passing through the first two runtimes. The
straight line essentially plots runtimes corresponding to a
linear speedup. DMLC clearly achieves better runtimes
showing a better than linear speedup.
</figureCaption>
<subsectionHeader confidence="0.826343">
6.1.2 Tagging Using Incomplete Dictionaries
</subsectionHeader>
<bodyText confidence="0.978954">
We also evaluate our approach for POS tagging
under other resource-constrained scenarios. Obtain-
</bodyText>
<page confidence="0.996159">
111
</page>
<table confidence="0.866414">
Method Tagging accuracy (%)
te=24k te=973k
tr=24k tr=973k tr=3.7M
1. EM 81.7 82.3
2. ILP + EM (Ravi and Knight, 2009) 91.6
3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1
4. DMLC + EM (this work) 91.4 87.5 87.8
</table>
<tableCaption confidence="0.905524666666667">
Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for
different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents
the size of the raw text used to perform model minimization.
</tableCaption>
<bodyText confidence="0.999875071428572">
ing a complete dictionary is often difficult, espe-
cially for new domains. To verify the utility of our
method when the input dictionary is incomplete, we
evaluate against standard datasets used in previous
work (Garrette and Baldridge, 2012) and compare
against the previous best reported performance for
the same task. In all the experiments (described
here and in subsequent sections), we use the fol-
lowing terminology—raw data refers to unlabeled
text used by different methods (for model minimiza-
tion or other unsupervised training procedures such
as EM), dictionary consists of word/tag entries that
are legal, and test refers to data over which tagging
evaluation is performed.
English Data: For English POS tagging with in-
complete dictionary, we evaluate on the Penn Tree-
bank (Marcus et al., 1993) data. Following (Garrette
and Baldridge, 2012), we extracted a word-tag dic-
tionary from sections 00-15 (751,059 tokens) con-
sisting of 39,087 word types, 45,331 word/tag en-
tries, a per-type ambiguity of 1.16 yielding a per-
token ambiguity of 2.21 on the raw corpus (treating
unknown words as having all 45 possible tags). As
in their setup, we then use the first 47,996 tokens
of section 16 as raw data and perform final evalua-
tion on the sections 22-24. We use the raw corpus
along with the unlabeled test data to perform model
minimization and EM training. Unknown words are
allowed to have all possible tags in both these pro-
cedures.
Italian Data: The minimization strategy pre-
sented here is a general-purpose method that does
not require any specific tuning and works for other
languages as well. To demonstrate this, we also per-
form evaluation on a different language (Italian) us-
ing the TUT corpus (Bosco et al., 2000). Follow-
ing (Garrette and Baldridge, 2012), we use the same
data splits as their setting. We take the first half of
each of the five sections to build the word-tag dic-
tionary, the next quarter as raw data and the last
quarter as test data. The dictionary was constructed
from 41,000 tokens comprised of 7,814 word types,
8,370 word/tag pairs, per-type ambiguity of 1.07 and
a per-token ambiguity of 1.41 on the raw data. The
raw data consisted of 18,574 tokens and the test con-
tained 18,763 tokens. We use the unlabeled corpus
from the raw and test data to perform model mini-
mization followed by unsupervised EM training.
Other Languages: In order to test the effective-
ness of our method in other non-English settings, we
also report the performance of our method on sev-
eral other Indo-European languages using treebank
data from CoNLL-X and CoNLL-2007 shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al., 2007). The corpus statistics for the five
languages (Danish, Greek, Italian, Portuguese and
Spanish) are listed below. For each language, we
construct a dictionary from the raw training data.
The unlabeled corpus from the raw training and test
data is used to perform model minimization fol-
lowed by unsupervised EM training. As before, un-
known words are allowed to have all possible tags.
We report the final tagging performance on the test
data and compare it to baseline EM.
Garrette and Baldridge (2012) treat unknown
words (words that appear in the raw text but are
missing from the dictionary) in a special manner and
use several heuristics to perform better initialization
for such words (for example, the probability that an
unknown word is associated with a particular tag is
</bodyText>
<page confidence="0.996163">
112
</page>
<bodyText confidence="0.996574647058824">
conditioned on the openness of the tag). They also
use an auto-supervision technique to smooth counts
learnt from EM onto new words encountered dur-
ing testing. In contrast, we do not apply any such
technique for unknown words and allow them to be
mapped uniformly to all possible tags in the dictio-
nary. For this particular set of experiments, the only
difference from the Garrette and Baldridge (2012)
setup is that we include unlabeled text from the test
data (but without any dictionary tag labels or special
heuristics) to our existing word tokens from raw text
for performing model minimization. This is a stan-
dard practice used in unsupervised training scenar-
ios (for example, Bayesian inference methods) and
in general for scalable techniques where the goal is
to perform inference on the same data for which one
wishes to produce some structured prediction.
</bodyText>
<table confidence="0.940410375">
Language Train Dict Test
(tokens) (entries) (tokens)
DANISH 94386 18797 5852
GREEK 65419 12894 4804
ITALIAN 71199 14934 5096
PORTUGUESE 206678 30053 5867
SPANISH 89334 17176 5694
Results: Table 2 (column 2) compares previously
</table>
<bodyText confidence="0.995795681818182">
reported results against our approach for English.
We observe that our method obtains a huge improve-
ment over standard EM and gets comparable results
to the previous best reported scores for the same task
from (Garrette and Baldridge, 2012). It is encourag-
ing to note that the new system achieves this per-
formance without using any of the carefully-chosen
heuristics employed by the previous method. How-
ever, we do note that some of these techniques can
be easily combined with our method to produce fur-
ther improvements.
Table 2 (column 3) also shows results on Ital-
ian POS tagging. We observe that our method
achieves significant improvements in tagging accu-
racy over all the baseline systems including the pre-
vious best system (+2.9%). This demonstrates that
the method generalizes well to other languages and
produces consistent tagging improvements over ex-
isting methods for the same task.
Results for POS tagging on CoNLL data in five
different languages are displayed in Figure 2. Note
that the proportion of raw data in test versus train
</bodyText>
<figure confidence="0.424682">
DANISH GREEK ITALIAN PORTUGUESE SPANISH
</figure>
<figureCaption confidence="0.9954905">
Figure 2: Part-of-Speech tagging accuracy for different
languages on CoNLL data using incomplete dictionaries.
</figureCaption>
<bodyText confidence="0.999845333333333">
(from the standard CoNLL shared tasks) is much
smaller compared to the earlier experimental set-
tings. In general, we observe that adding more raw
data for EM training improves the tagging quality
(same trend observed earlier in Table 1: column 2
versus column 3). Despite this, DMLC + EM still
achieves significant improvements over the baseline
EM system on multiple languages (as shown in Fig-
ure 2). An additional advantage of the new method
is that it can easily scale to larger corpora and it pro-
duces a much more compact grammar that can be
efficiently incorporated for EM training.
</bodyText>
<subsectionHeader confidence="0.887577">
6.1.3 Tagging for Low-Resource Languages
</subsectionHeader>
<bodyText confidence="0.99999205">
Learning part-of-speech taggers for severely low-
resource languages (e.g., Malagasy) is very chal-
lenging. In addition to scarce (token-supervised)
labeled resources, the tag dictionaries avail-
able for training taggers are tiny compared to
other languages such as English. Garrette and
Baldridge (2013) combine various supervised and
semi-supervised learning algorithms into a common
POS tagger training pipeline to address some of
these challenges. They also report tagging accuracy
improvements on low-resource languages when us-
ing the combined system over any single algorithm.
Their system has four main parts, in order: (1) Tag
dictionary expansion using label propagation algo-
rithm, (2) Weighted model minimization, (3) Ex-
pectation maximization (EM) training of HMMs us-
ing auto-supervision, (4) MaxEnt Markov Model
(MEMM) training. The entire procedure results in
a trained tagger model that can then be applied to
tag any raw data.3 Step 2 in this procedure involves
</bodyText>
<footnote confidence="0.943775">
3For more details, refer (Garrette and Baldridge, 2013).
</footnote>
<figure confidence="0.976733411764706">
90
EM DMLC+EM
80
70
60
50
81.3
83.1
78.5
80.1
82
84.6
65.6
66.3
77.8
79.4
Tagging Accuracy (%)
</figure>
<page confidence="0.992753">
113
</page>
<table confidence="0.982824666666667">
Method Tagging accuracy (%)
English (PTB 00-15) Italian (TUT)
1. Random 63.53 62.81
2. EM 69.20 60.70
3. Type-supervision + HMM initialization (Garrette and Baldridge, 2012) 88.52 72.86
4. DMLC + EM (this work) 88.11 75.79
</table>
<tableCaption confidence="0.9942875">
Table 2: Part-of-Speech tagging accuracy using PTB sections 00-15 and TUT to build the tag dictionary. For compar-
ison, we also include the results for the previously reported state-of-the-art system (method 3) for the same task.
</tableCaption>
<table confidence="0.99785575">
Method Tagging accuracy (%)
Total Known Unknown
Low-resource tagging using (Garrette and Baldridge, 2013) 80.7 (70.2) 87.6 (90.3) 66.1 (45.1)
Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5)
</table>
<tableCaption confidence="0.99591">
Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens in
</tableCaption>
<bodyText confidence="0.95096035483871">
the test data. Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes:
(a) small, (b) tiny (shown in parentheses). The new method (row 2) significantly outperforms the existing method with
p &lt; 0.01 for small dictionary and p &lt; 0.05 for tiny dictionary.
a weighted version of model minimization which
uses the multi-step greedy approach from Ravi et
al. (2010b) enhanced with additional heuristics that
uses tag weights learnt via label propagation (in Step
1) within the minimization process.
We replace the model minimization procedure in
their Step 2 with our method (DMLC + EM) and di-
rectly compare this new system with their approach
in terms of tagging accuracy. Note for all other steps
in the pipeline we follow the same procedure (and
run the same code) as Garrette and Baldridge (2013),
including the same smoothing procedure for EM ini-
tialization in Step 3.
Data: We use the exact same setup as Garrette
and Baldridge (2013) and run experiments on Mala-
gasy, an Austronesian language spoken in Madagas-
car. We use the publicly available data4: 100k raw
tokens for training, a word-tag dictionary acquired
with 4 hours of human annotation effort (used for
type-supervision), and a held-out test dataset (5341
tokens). We provide the unlabeled corpus from the
raw training data along with the word-tag dictionary
as input to model minimization and evaluate on the
test corpus. We run multiple experiments for dif-
ferent (incomplete) dictionary scenarios: (a) small =
2773 word/tag pairs, (b) tiny = 329 word/tag pairs.
Results: Table 3 shows results on Malagasy
data comparing a system that employs (unweighted)
</bodyText>
<footnote confidence="0.548672">
4github.com/ dhgarrette/low-resource-pos-tagging-2013
</footnote>
<bodyText confidence="0.9998293">
DMLC against the existing state-of-the-art system
that incorporates a multi-step weighted model min-
imization combined with additional heuristics. We
observe that switching to the new model minimiza-
tion procedure alone yields significant improvement
in tagging accuracy under both dictionary scenarios.
It is encouraging that a better minimization proce-
dure also leads to higher tagging quality on the un-
known word tokens (column 4 in the table), even
when the input dictionary is tiny.
</bodyText>
<subsectionHeader confidence="0.997422">
6.2 Supertagging
</subsectionHeader>
<bodyText confidence="0.997801055555556">
Compared to POS tagging, a more challenging task
is learning supertaggers for lexicalized grammar
formalisms such as Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000). For example, CCG-
bank (Hockenmaier and Steedman, 2007) contains
1241 distinct supertags (lexical categories) and the
most ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point for
the semi-supervised methods typically applied to
the task. Yet, this is an important task since cre-
ating grammars and resources for CCG parsers for
new domains and languages is highly labor- and
knowledge-intensive.
As described earlier, our approach scales easily to
large datasets as well as label sizes. To evaluate it on
the supertagging task, we use the same dataset from
(Ravi et al., 2010a) and compare against their base-
line method that uses an modified (two-step) version
</bodyText>
<page confidence="0.994284">
114
</page>
<table confidence="0.9928236">
Method Supertagging accuracy (%)
Ambiguous Total
1. EM 38.7 45.6
2. ILP∗ + EM (Ravi et al., 2010a) 52.1 57.3
3. DMLC + EM (this work) 55.9 59.3
</table>
<tableCaption confidence="0.928404">
Table 4: Results for unsupervised supertagging with a dictionary. Here, we report the total accuracy as well as
accuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility). ∗The baseline
method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In
contrast, the new approach (DMLC) runs fast and also permits efficient parallelization.
</tableCaption>
<bodyText confidence="0.998585857142857">
of the ILP formulation for model minimization.
Data: We use the CCGbank data for this ex-
periment. This data was created by semi- auto-
matically converting the Penn Treebank to CCG
derivations (Hockenmaier and Steedman, 2007). We
use the standard splits of the data used in semi-
supervised tagging experiments (Banko and Moore,
2004)—sections 0-18 for training (i.e., to construct
the word-tag dictionary), and sections 22-24 for test.
Results: Table 4 compares the results for two
baseline systems—standard EM (method 1), and a
previously reported system using model minimiza-
tion (method 2) for the same task. We observe
that DMLC produces better taggings than either of
these and yields significant improvement in accu-
racy (+2% overall, +3.8% on ambiguous tokens).
Note that it is not feasible to run the ILP-based
baseline (method 2 in the table) directly since it is
very slow in practice, so Ravi et al. (2010a) use
a set of pre-processing steps to prune the original
grammar size (unique tag pairs) from &gt;1M to sev-
eral thousand entries followed by a modified two-
step ILP minimization strategy. This is required to
permit their model minimization step to be run in
a feasible manner. On the other hand, the new ap-
proach DMLC (method 3) scales better even when
the data/label sizes are large, hence it can be run with
the full data using the original model minimization
formulation (rather than a two-step heuristic).
Ravi et al. (2010a) also report further improve-
ments using an alternative approach involving an
ILP-based weighted minimization procedure. In
Section 7 we briefly discuss how the DMLC method
can be extended to this setting and combined with
other similar methods.
</bodyText>
<sectionHeader confidence="0.995032" genericHeader="conclusions">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999938028571429">
We present a fast, efficient model minimization
algorithm for unsupervised tagging that improves
upon previous two-step heuristics. We show that un-
der a fairly natural assumption of c-feasibility the
solution obtained by our minimization algorithm is
O(clog m)-approximate to the optimal. Although
in the case of two-step heuristics, the first step guar-
antees an O(log m)-approximation, the second step,
which is required to get a consistent solution, can
introduce many additional labels resulting in a so-
lution arbitrarily away from the optimal. Our one
step approach ensures consistency at each step of the
algorithm, while the c-feasibility assumption means
that the solution does not diverge too much from the
optimal in each iteration.
In addition to proving approximation guarantees
for the new algorithm, we show that it is paralleliz-
able, allowing us to easily scale to larger datasets
than previously explored. Our results show that
the algorithm achieves state-of-the-art performance,
outperforming existing methods on several differ-
ent tasks (both POS tagging and supertagging) and
works well even with incomplete dictionaries and
extremely low-resource languages like Malagasy.
For future work, it would be interesting to apply a
weighted version of the DMLC algorithm where la-
bels (i.e., tag pairs) can have different weight distri-
butions instead of uniform weights. Our algorithm
can be extended to allow an input weight distribu-
tion to be specified for minimization. In order to
initialize the weights we could use existing strate-
gies such as grammar-informed initialization (Ravi
et al., 2010a) or output distributions learnt via other
methods such as label propagation (Garrette and
Baldridge, 2013).
</bodyText>
<page confidence="0.998629">
115
</page>
<sectionHeader confidence="0.983809" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886721153846">
2013. Apache giraph. http://giraph.apache.
org/.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
pages 556–561.
Andrew R Barron, Jorma Rissanen, and Bin Yu. 1998.
The Minimum Description Length Principle in Cod-
ing and Modeling. IEEE Transactions of Information
Theory, 44(6):2743–2760.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a Treebank for
Italian: a data-driven annotation schema. In Proceed-
ings of the Second International Conference on Lan-
guage Resources and Evaluation LREC-2000, pages
99–105.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of CoNLL, pages 149–164.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 575–584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, pages 600–
609.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001–2049.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised Hidden Markov Models for part-of-speech
tagging with incomplete tag dictionaries. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 821–
831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 138–147.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42–47.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL, pages
746–754.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Fernando C. Gomes, Cludio N. Meneses, Panos M.
Pardalos, and Gerardo Valdisio R. Viana. 2006. Ex-
perimental analysis of approximation algorithms for
the vertex cover and set covering problems.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of the 12th
Conference on the European Chapter of the Associa-
tion for Computational Linguistics, pages 363–371.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 296–305.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Transla-
tion Summit X, pages 79–86.
Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik,
James C. Dehnert, Ilan Horn, Naty Leiser, and Grze-
gorz Czajkowski. 2010. Pregel: a system for large-
scale graph processing. In Proceedings of the 2010
ACM SIGMOD International Conference on Manage-
ment of data, pages 135–146.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Taesun Moon, Katrin Erk, and Jason Baldridge. 2010.
Crouching Dirichlet, Hidden Markov Model: Unsu-
pervised POS tagging with context local tag genera-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 196–
206.
</reference>
<page confidence="0.989688">
116
</page>
<reference confidence="0.999781114285714">
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL, pages 915–932.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conferenceof the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing (ACL-IJCNLP), pages 504–
512.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a.
Minimized models and grammar-informed initializa-
tion for supertagging with highly ambiguous lexicons.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
495–503.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010b. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING), pages 940–948.
Roi Reichart, Raanan Fattal, and Ari Rappoport. 2010.
Improved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 57–66.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Advances in Neural Information
Processing Systems (NIPS), pages 1521–1528.
</reference>
<page confidence="0.9993425">
117
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745124">
<title confidence="0.999966">Parallel Algorithms for Unsupervised Tagging</title>
<author confidence="0.988508">Ravi Sergei Vassilivitskii Vibhor</author>
<affiliation confidence="0.766979">Google Google Twitter</affiliation>
<address confidence="0.937294">Mountain View, CA 94043 Mountain View, CA 94043 San Francisco, CA</address>
<email confidence="0.994209">sravi@google.comsergeiv@google.comvibhor.rastogi@gmail.com</email>
<abstract confidence="0.999287">We propose a new method for unsupervised tagging that finds minimal models which are then further improved by Expectation Maximization training. In contrast to previous approaches that rely on manually specified and multi-step heuristics for model minimization, our approach is a simple greedy approxalgorithm DMLC that solves this objective in a single step. We extend the method and show how to efficiently parallelize the algorithm on modern parallel computing platforms while preserving approximation guarantees. The new method easily scales to large data and grammar sizes, overcoming the memory bottleneck in previous approaches. We demonstrate the power of the new algorithm by evaluating on various sequence labeling tasks: Part-of-Speech tagging for multiple languages (including lowresource languages), with complete and incomplete dictionaries, and supertagging, a complex sequence labeling task, where the grammar size alone can grow to millions of entries. Our results show that for all of these settings, our method achieves state-of-the-art scalable performance that yields high quality tagging outputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Apache giraph.</title>
<date>2013</date>
<note>http://giraph.apache. org/.</note>
<contexts>
<context position="34922" citStr="(2013)" startWordPosition="5897" endWordPosition="5897">ents over the baseline EM system on multiple languages (as shown in Figure 2). An additional advantage of the new method is that it can easily scale to larger corpora and it produces a much more compact grammar that can be efficiently incorporated for EM training. 6.1.3 Tagging for Low-Resource Languages Learning part-of-speech taggers for severely lowresource languages (e.g., Malagasy) is very challenging. In addition to scarce (token-supervised) labeled resources, the tag dictionaries available for training taggers are tiny compared to other languages such as English. Garrette and Baldridge (2013) combine various supervised and semi-supervised learning algorithms into a common POS tagger training pipeline to address some of these challenges. They also report tagging accuracy improvements on low-resource languages when using the combined system over any single algorithm. Their system has four main parts, in order: (1) Tag dictionary expansion using label propagation algorithm, (2) Weighted model minimization, (3) Expectation maximization (EM) training of HMMs using auto-supervision, (4) MaxEnt Markov Model (MEMM) training. The entire procedure results in a trained tagger model that can </context>
<context position="37379" citStr="(2013)" startWordPosition="6284" endWordPosition="6284"> p &lt; 0.01 for small dictionary and p &lt; 0.05 for tiny dictionary. a weighted version of model minimization which uses the multi-step greedy approach from Ravi et al. (2010b) enhanced with additional heuristics that uses tag weights learnt via label propagation (in Step 1) within the minimization process. We replace the model minimization procedure in their Step 2 with our method (DMLC + EM) and directly compare this new system with their approach in terms of tagging accuracy. Note for all other steps in the pipeline we follow the same procedure (and run the same code) as Garrette and Baldridge (2013), including the same smoothing procedure for EM initialization in Step 3. Data: We use the exact same setup as Garrette and Baldridge (2013) and run experiments on Malagasy, an Austronesian language spoken in Madagascar. We use the publicly available data4: 100k raw tokens for training, a word-tag dictionary acquired with 4 hours of human annotation effort (used for type-supervision), and a held-out test dataset (5341 tokens). We provide the unlabeled corpus from the raw training data along with the word-tag dictionary as input to model minimization and evaluate on the test corpus. We run mult</context>
</contexts>
<marker>2013</marker>
<rawString>2013. Apache giraph. http://giraph.apache. org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part-ofspeech tagging in context.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>556--561</pages>
<contexts>
<context position="2004" citStr="Banko and Moore, 2004" startWordPosition="284" endWordPosition="287"> high quality tagging outputs. 1 Introduction Supervised sequence labeling with large labeled training datasets is considered a solved problem. For ∗The research described herein was conducted while the author was working at Google. instance, state of the art systems obtain tagging accuracies over 97% for part-of-speech (POS) tagging on the English Penn Treebank. However, learning accurate taggers without labeled data remains a challenge. The accuracies quickly drop when faced with data from a different domain, language, or when there is very little labeled information available for training (Banko and Moore, 2004). Recently, there has been an increasing amount of research tackling this problem using unsupervised methods. A popular approach is to learn from POS-tag dictionaries (Merialdo, 1994), where we are given a raw word sequence and a dictionary of legal tags for each word type. Learning from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encou</context>
<context position="40532" citStr="Banko and Moore, 2004" startWordPosition="6772" endWordPosition="6775">uous tokens (i.e., tokens which have more than one tagging possibility). ∗The baseline method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In contrast, the new approach (DMLC) runs fast and also permits efficient parallelization. of the ILP formulation for model minimization. Data: We use the CCGbank data for this experiment. This data was created by semi- automatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semisupervised tagging experiments (Banko and Moore, 2004)—sections 0-18 for training (i.e., to construct the word-tag dictionary), and sections 22-24 for test. Results: Table 4 compares the results for two baseline systems—standard EM (method 1), and a previously reported system using model minimization (method 2) for the same task. We observe that DMLC produces better taggings than either of these and yields significant improvement in accuracy (+2% overall, +3.8% on ambiguous tokens). Note that it is not feasible to run the ILP-based baseline (method 2 in the table) directly since it is very slow in practice, so Ravi et al. (2010a) use a set of pre</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore. 2004. Part-ofspeech tagging in context. In Proceedings of COLING, pages 556–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Barron</author>
<author>Jorma Rissanen</author>
<author>Bin Yu</author>
</authors>
<title>The Minimum Description Length Principle in Coding and Modeling.</title>
<date>1998</date>
<journal>IEEE Transactions of Information Theory,</journal>
<volume>44</volume>
<issue>6</issue>
<contexts>
<context position="3913" citStr="Barron et al., 1998" startWordPosition="581" endWordPosition="584">or Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. perform well on this task (Johnson, 2007). More recent methods have yielded better performance than EM (see (Ravi and Knight, 2009) for an overview). One interesting line of research introduced by Ravi and Knight (2009) explores the idea of performing model minimization followed by EM training to learn taggers. Their idea is closely related to the classic Minimum Description Length principle for model selection (Barron et al., 1998). They (1) formulate an objective function to find the smallest model that explains the text (model minimization step), and then, (2) fit the minimized model to the data (EM step). For POS tagging, this method (Ravi and Knight, 2009) yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank. The original work from (Ravi and Knight, 2009) uses an integer linear programming (ILP) formulation to find minimal models, an approach which does not scale to large datasets. Ravi et al. (2010b) introduced a two-step greedy approximation to the o</context>
</contexts>
<marker>Barron, Rissanen, Yu, 1998</marker>
<rawString>Andrew R Barron, Jorma Rissanen, and Bin Yu. 1998. The Minimum Description Length Principle in Coding and Modeling. IEEE Transactions of Information Theory, 44(6):2743–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Vincenzo Lombardo</author>
<author>Daniela Vassallo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>Building a Treebank for Italian: a data-driven annotation schema.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000,</booktitle>
<pages>99--105</pages>
<contexts>
<context position="29986" citStr="Bosco et al., 2000" startWordPosition="5090" endWordPosition="5093"> possible tags). As in their setup, we then use the first 47,996 tokens of section 16 as raw data and perform final evaluation on the sections 22-24. We use the raw corpus along with the unlabeled test data to perform model minimization and EM training. Unknown words are allowed to have all possible tags in both these procedures. Italian Data: The minimization strategy presented here is a general-purpose method that does not require any specific tuning and works for other languages as well. To demonstrate this, we also perform evaluation on a different language (Italian) using the TUT corpus (Bosco et al., 2000). Following (Garrette and Baldridge, 2012), we use the same data splits as their setting. We take the first half of each of the five sections to build the word-tag dictionary, the next quarter as raw data and the last quarter as test data. The dictionary was constructed from 41,000 tokens comprised of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of 18,574 tokens and the test contained 18,763 tokens. We use the unlabeled corpus from the raw and test data to perform model minimization followed by unsu</context>
</contexts>
<marker>Bosco, Lombardo, Vassallo, Lesmo, 2000</marker>
<rawString>Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, and Leonardo Lesmo. 2000. Building a Treebank for Italian: a data-driven annotation schema. In Proceedings of the Second International Conference on Language Resources and Evaluation LREC-2000, pages 99–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="30899" citStr="Buchholz and Marsi, 2006" startWordPosition="5245" endWordPosition="5248"> comprised of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of 18,574 tokens and the test contained 18,763 tokens. We use the unlabeled corpus from the raw and test data to perform model minimization followed by unsupervised EM training. Other Languages: In order to test the effectiveness of our method in other non-English settings, we also report the performance of our method on several other Indo-European languages using treebank data from CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The corpus statistics for the five languages (Danish, Greek, Italian, Portuguese and Spanish) are listed below. For each language, we construct a dictionary from the raw training data. The unlabeled corpus from the raw training and test data is used to perform model minimization followed by unsupervised EM training. As before, unknown words are allowed to have all possible tags. We report the final tagging performance on the test data and compare it to baseline EM. Garrette and Baldridge (2012) treat unknown words (words that appear in the raw text but are missing from t</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>575--584</pages>
<contexts>
<context position="7312" citStr="Christodoulopoulos et al., 2010" startWordPosition="1126" endWordPosition="1130">e grammar. • We demonstrate the power of the new method by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limit</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 575–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised partof-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<pages>600--609</pages>
<contexts>
<context position="7903" citStr="Das and Petrov, 2011" startWordPosition="1224" endWordPosition="1227">hristodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only enco</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised partof-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 600– 609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="3235" citStr="Dempster et al., 1977" startWordPosition="479" endWordPosition="482">words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words using some heuristics (Toutanova and Johnson, 2008) or using other methods such as type-supervision (Garrette and Baldridge, 2012). In this work, we tackle the problem of unsupervised sequence labeling using tag dictionaries. The first reported work on this problem was on POS tagging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), though EM does not 105 Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. perform well on this task (Johnson, 2007). More recent methods have yielded better performance than EM (see (Ravi and Knight, 2009) for an overview). One interesting line of research introduced by Ravi and Knight (2009) explores the idea of performing model minimization followed by EM training to learn taggers. Their idea is closely related to the classic </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Typesupervised Hidden Markov Models for part-of-speech tagging with incomplete tag dictionaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>821--831</pages>
<contexts>
<context position="2917" citStr="Garrette and Baldridge, 2012" startWordPosition="428" endWordPosition="431">from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encounter new words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words using some heuristics (Toutanova and Johnson, 2008) or using other methods such as type-supervision (Garrette and Baldridge, 2012). In this work, we tackle the problem of unsupervised sequence labeling using tag dictionaries. The first reported work on this problem was on POS tagging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), though EM does not 105 Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. perform well on this task (Johnson, 2007</context>
<context position="4673" citStr="Garrette and Baldridge (2012)" startWordPosition="704" endWordPosition="707">2) fit the minimized model to the data (EM step). For POS tagging, this method (Ravi and Knight, 2009) yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank. The original work from (Ravi and Knight, 2009) uses an integer linear programming (ILP) formulation to find minimal models, an approach which does not scale to large datasets. Ravi et al. (2010b) introduced a two-step greedy approximation to the original objective function (called the MIN-GREEDY algorithm) that runs much faster while maintaining the high tagging performance. Garrette and Baldridge (2012) showed how to use several heuristics to further improve this algorithm (for instance, better choice of tag bigrams when breaking ties) and stack other techniques on top, such as careful initialization of HMM emission models which results in further performance gains. Their method also works under incomplete dictionary scenarios and can be applied to certain low-resource scenarios (Garrette and Baldridge, 2013) by combining model minimization with supervised training. In this work, we propose a new scalable algorithm for performing model minimization for this task. By making an assumption on t</context>
<context position="7662" citStr="Garrette and Baldridge, 2012" startWordPosition="1186" endWordPosition="1189">f-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging </context>
<context position="28500" citStr="Garrette and Baldridge, 2012" startWordPosition="4843" endWordPosition="4846">1.6 3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1 4. DMLC + EM (this work) 91.4 87.5 87.8 Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents the size of the raw text used to perform model minimization. ing a complete dictionary is often difficult, especially for new domains. To verify the utility of our method when the input dictionary is incomplete, we evaluate against standard datasets used in previous work (Garrette and Baldridge, 2012) and compare against the previous best reported performance for the same task. In all the experiments (described here and in subsequent sections), we use the following terminology—raw data refers to unlabeled text used by different methods (for model minimization or other unsupervised training procedures such as EM), dictionary consists of word/tag entries that are legal, and test refers to data over which tagging evaluation is performed. English Data: For English POS tagging with incomplete dictionary, we evaluate on the Penn Treebank (Marcus et al., 1993) data. Following (Garrette and Baldri</context>
<context position="30028" citStr="Garrette and Baldridge, 2012" startWordPosition="5096" endWordPosition="5099">up, we then use the first 47,996 tokens of section 16 as raw data and perform final evaluation on the sections 22-24. We use the raw corpus along with the unlabeled test data to perform model minimization and EM training. Unknown words are allowed to have all possible tags in both these procedures. Italian Data: The minimization strategy presented here is a general-purpose method that does not require any specific tuning and works for other languages as well. To demonstrate this, we also perform evaluation on a different language (Italian) using the TUT corpus (Bosco et al., 2000). Following (Garrette and Baldridge, 2012), we use the same data splits as their setting. We take the first half of each of the five sections to build the word-tag dictionary, the next quarter as raw data and the last quarter as test data. The dictionary was constructed from 41,000 tokens comprised of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of 18,574 tokens and the test contained 18,763 tokens. We use the unlabeled corpus from the raw and test data to perform model minimization followed by unsupervised EM training. Other Languages: In </context>
<context position="31421" citStr="Garrette and Baldridge (2012)" startWordPosition="5331" endWordPosition="5334">sing treebank data from CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The corpus statistics for the five languages (Danish, Greek, Italian, Portuguese and Spanish) are listed below. For each language, we construct a dictionary from the raw training data. The unlabeled corpus from the raw training and test data is used to perform model minimization followed by unsupervised EM training. As before, unknown words are allowed to have all possible tags. We report the final tagging performance on the test data and compare it to baseline EM. Garrette and Baldridge (2012) treat unknown words (words that appear in the raw text but are missing from the dictionary) in a special manner and use several heuristics to perform better initialization for such words (for example, the probability that an unknown word is associated with a particular tag is 112 conditioned on the openness of the tag). They also use an auto-supervision technique to smooth counts learnt from EM onto new words encountered during testing. In contrast, we do not apply any such technique for unknown words and allow them to be mapped uniformly to all possible tags in the dictionary. For this parti</context>
<context position="33036" citStr="Garrette and Baldridge, 2012" startWordPosition="5594" endWordPosition="5597">thods) and in general for scalable techniques where the goal is to perform inference on the same data for which one wishes to produce some structured prediction. Language Train Dict Test (tokens) (entries) (tokens) DANISH 94386 18797 5852 GREEK 65419 12894 4804 ITALIAN 71199 14934 5096 PORTUGUESE 206678 30053 5867 SPANISH 89334 17176 5694 Results: Table 2 (column 2) compares previously reported results against our approach for English. We observe that our method obtains a huge improvement over standard EM and gets comparable results to the previous best reported scores for the same task from (Garrette and Baldridge, 2012). It is encouraging to note that the new system achieves this performance without using any of the carefully-chosen heuristics employed by the previous method. However, we do note that some of these techniques can be easily combined with our method to produce further improvements. Table 2 (column 3) also shows results on Italian POS tagging. We observe that our method achieves significant improvements in tagging accuracy over all the baseline systems including the previous best system (+2.9%). This demonstrates that the method generalizes well to other languages and produces consistent tagging</context>
<context position="35923" citStr="Garrette and Baldridge, 2012" startWordPosition="6051" endWordPosition="6054">ion algorithm, (2) Weighted model minimization, (3) Expectation maximization (EM) training of HMMs using auto-supervision, (4) MaxEnt Markov Model (MEMM) training. The entire procedure results in a trained tagger model that can then be applied to tag any raw data.3 Step 2 in this procedure involves 3For more details, refer (Garrette and Baldridge, 2013). 90 EM DMLC+EM 80 70 60 50 81.3 83.1 78.5 80.1 82 84.6 65.6 66.3 77.8 79.4 Tagging Accuracy (%) 113 Method Tagging accuracy (%) English (PTB 00-15) Italian (TUT) 1. Random 63.53 62.81 2. EM 69.20 60.70 3. Type-supervision + HMM initialization (Garrette and Baldridge, 2012) 88.52 72.86 4. DMLC + EM (this work) 88.11 75.79 Table 2: Part-of-Speech tagging accuracy using PTB sections 00-15 and TUT to build the tag dictionary. For comparison, we also include the results for the previously reported state-of-the-art system (method 3) for the same task. Method Tagging accuracy (%) Total Known Unknown Low-resource tagging using (Garrette and Baldridge, 2013) 80.7 (70.2) 87.6 (90.3) 66.1 (45.1) Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5) Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/U</context>
</contexts>
<marker>Garrette, Baldridge, 2012</marker>
<rawString>Dan Garrette and Jason Baldridge. 2012. Typesupervised Hidden Markov Models for part-of-speech tagging with incomplete tag dictionaries. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 821– 831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="5087" citStr="Garrette and Baldridge, 2013" startWordPosition="768" endWordPosition="771">0b) introduced a two-step greedy approximation to the original objective function (called the MIN-GREEDY algorithm) that runs much faster while maintaining the high tagging performance. Garrette and Baldridge (2012) showed how to use several heuristics to further improve this algorithm (for instance, better choice of tag bigrams when breaking ties) and stack other techniques on top, such as careful initialization of HMM emission models which results in further performance gains. Their method also works under incomplete dictionary scenarios and can be applied to certain low-resource scenarios (Garrette and Baldridge, 2013) by combining model minimization with supervised training. In this work, we propose a new scalable algorithm for performing model minimization for this task. By making an assumption on the structure of the solution, we prove that a variant of the greedy set cover algorithm always finds an approximately optimal label set. This is in contrast to previous methods that employ heuristic approaches with no guarantee on the quality of the solution. In addition, we do not have to rely on ad hoc tie-breaking procedures or careful initializations for unknown words. Finally, not only is the proposed meth</context>
<context position="7966" citStr="Garrette and Baldridge, 2013" startWordPosition="1233" endWordPosition="1236">Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model</context>
<context position="34922" citStr="Garrette and Baldridge (2013)" startWordPosition="5894" endWordPosition="5897">es significant improvements over the baseline EM system on multiple languages (as shown in Figure 2). An additional advantage of the new method is that it can easily scale to larger corpora and it produces a much more compact grammar that can be efficiently incorporated for EM training. 6.1.3 Tagging for Low-Resource Languages Learning part-of-speech taggers for severely lowresource languages (e.g., Malagasy) is very challenging. In addition to scarce (token-supervised) labeled resources, the tag dictionaries available for training taggers are tiny compared to other languages such as English. Garrette and Baldridge (2013) combine various supervised and semi-supervised learning algorithms into a common POS tagger training pipeline to address some of these challenges. They also report tagging accuracy improvements on low-resource languages when using the combined system over any single algorithm. Their system has four main parts, in order: (1) Tag dictionary expansion using label propagation algorithm, (2) Weighted model minimization, (3) Expectation maximization (EM) training of HMMs using auto-supervision, (4) MaxEnt Markov Model (MEMM) training. The entire procedure results in a trained tagger model that can </context>
<context position="36307" citStr="Garrette and Baldridge, 2013" startWordPosition="6111" endWordPosition="6114">1.3 83.1 78.5 80.1 82 84.6 65.6 66.3 77.8 79.4 Tagging Accuracy (%) 113 Method Tagging accuracy (%) English (PTB 00-15) Italian (TUT) 1. Random 63.53 62.81 2. EM 69.20 60.70 3. Type-supervision + HMM initialization (Garrette and Baldridge, 2012) 88.52 72.86 4. DMLC + EM (this work) 88.11 75.79 Table 2: Part-of-Speech tagging accuracy using PTB sections 00-15 and TUT to build the tag dictionary. For comparison, we also include the results for the previously reported state-of-the-art system (method 3) for the same task. Method Tagging accuracy (%) Total Known Unknown Low-resource tagging using (Garrette and Baldridge, 2013) 80.7 (70.2) 87.6 (90.3) 66.1 (45.1) Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5) Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens in the test data. Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes: (a) small, (b) tiny (shown in parentheses). The new method (row 2) significantly outperforms the existing method with p &lt; 0.01 for small dictionary and p &lt; 0.05 for tiny dictionary. a weighted version of model minimization which uses the multi-step gr</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138–147.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
</authors>
<location>and</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, </marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>42--47</pages>
<marker>Smith, 2011</marker>
<rawString>Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="7721" citStr="Goldberg et al., 2008" startWordPosition="1194" endWordPosition="1197">rk Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternativ</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of ACL, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8422" citStr="Goldwater and Griffiths, 2007" startWordPosition="1307" endWordPosition="1310">tstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. Formally, we are given a set of sequences, S = {S1, S2, ... , Sn} where each Si is a sequence of words, Si = wi1, wi2, ... , wi,|Si|. With each w</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C Gomes</author>
<author>Cludio N Meneses</author>
<author>Panos M Pardalos</author>
<author>Gerardo Valdisio R Viana</author>
</authors>
<title>Experimental analysis of approximation algorithms for the vertex cover and set covering problems.</title>
<date>2006</date>
<contexts>
<context position="14744" citStr="Gomes et al., 2006" startWordPosition="2472" endWordPosition="2475">1, wij) as (NN, VP), then the label for the next word pair (wij, wi,j+1) has to be of the form (VP, *), i.e., it has to start with VP. Previous work (Ravi et al., 2010a; Ravi et al., 2010b) recognized this challenge and employed two phase heuristic approaches. Eschewing heuristics, we will show that with one natural assumption, even with this extra set of constraints, the standard greedy algorithm for this problem results in a solution with a provable approximation ratio of O(log m). In practice, however, the algorithm performs far better than the worst case ratio, and similar to the work of (Gomes et al., 2006), we find that the greedy approach selects a cover approximately 11% worse than the optimum solution. 4.2 MLC Algorithm We present in Algorithm 1 our MINIMUM LABEL COVER algorithm to approximately solve the minimum label cover problem. The algorithm is simple, efficient, and easy to distribute. The algorithm chooses labels one at a time, selecting a label that covers as many words as possible in 108 every iteration. For this, it generates and maintains a multi-set of all possible labels M (Step 3). The multi-set contains an occurrence of each valid label, for example, if wi,j−1 has two possibl</context>
</contexts>
<marker>Gomes, Meneses, Pardalos, Viana, 2006</marker>
<rawString>Fernando C. Gomes, Cludio N. Meneses, Panos M. Pardalos, and Gerardo Valdisio R. Viana. 2006. Experimental analysis of approximation algorithms for the vertex cover and set covering problems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference on the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>363--371</pages>
<contexts>
<context position="7631" citStr="Hasan and Ng, 2009" startWordPosition="1182" endWordPosition="1185">hod achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence label</context>
</contexts>
<marker>Hasan, Ng, 2009</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2009. Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages. In Proceedings of the 12th Conference on the European Chapter of the Association for Computational Linguistics, pages 363–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="38984" citStr="Hockenmaier and Steedman, 2007" startWordPosition="6521" endWordPosition="6524">inimization combined with additional heuristics. We observe that switching to the new model minimization procedure alone yields significant improvement in tagging accuracy under both dictionary scenarios. It is encouraging that a better minimization procedure also leads to higher tagging quality on the unknown word tokens (column 4 in the table), even when the input dictionary is tiny. 6.2 Supertagging Compared to POS tagging, a more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. As described earlier, our approach scales easily to large datasets as well as label sizes. To evaluate it on the supertagging task, we use the same dataset from (Ravi et al., 2010a) and compare against their baseline method t</context>
<context position="40425" citStr="Hockenmaier and Steedman, 2007" startWordPosition="6754" endWordPosition="6757">nsupervised supertagging with a dictionary. Here, we report the total accuracy as well as accuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility). ∗The baseline method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In contrast, the new approach (DMLC) runs fast and also permits efficient parallelization. of the ILP formulation for model minimization. Data: We use the CCGbank data for this experiment. This data was created by semi- automatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semisupervised tagging experiments (Banko and Moore, 2004)—sections 0-18 for training (i.e., to construct the word-tag dictionary), and sections 22-24 for test. Results: Table 4 compares the results for two baseline systems—standard EM (method 1), and a previously reported system using model minimization (method 2) for the same task. We observe that DMLC produces better taggings than either of these and yields significant improvement in accuracy (+2% overall, +3.8% on ambiguous tokens). Note that it is not feasible to run the ILP-based baseline </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<contexts>
<context position="3518" citStr="Johnson, 2007" startWordPosition="518" endWordPosition="519">dridge, 2012). In this work, we tackle the problem of unsupervised sequence labeling using tag dictionaries. The first reported work on this problem was on POS tagging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), though EM does not 105 Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. perform well on this task (Johnson, 2007). More recent methods have yielded better performance than EM (see (Ravi and Knight, 2009) for an overview). One interesting line of research introduced by Ravi and Knight (2009) explores the idea of performing model minimization followed by EM training to learn taggers. Their idea is closely related to the classic Minimum Description Length principle for model selection (Barron et al., 1998). They (1) formulate an objective function to find the smallest model that explains the text (model minimization step), and then, (2) fit the minimized model to the data (EM step). For POS tagging, this me</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="24815" citStr="Koehn, 2005" startWordPosition="4241" endWordPosition="4242">m the test corpus that were labeled correctly by the system. 6.1 Part-of-Speech Tagging Task 6.1.1 Tagging Using a Complete Dictionary Data: We use a standard test set (consisting of 24,115 word tokens from the Penn Treebank) for the POS tagging task. The tagset consists of 45 distinct tag labels and the dictionary contains 57,388 word/tag pairs derived from the entire Penn Treebank. Per-token ambiguity for the test data is about 1.5 tags/token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). • MIN-GREEDY + EM: Minimizing grammar size using the two-step greedy method (Ravi et al., 2010b). • DMLC + EM: This work. Results: Table 1 shows the results for POS tagging on English Penn Treebank data. On the smaller test datasets, all of the m</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Machine Translation Summit X, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Malewicz</author>
<author>Matthew H Austern</author>
<author>Aart J C Bik</author>
<author>James C Dehnert</author>
<author>Ilan Horn</author>
<author>Naty Leiser</author>
<author>Grzegorz Czajkowski</author>
</authors>
<title>Pregel: a system for largescale graph processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,</booktitle>
<pages>135--146</pages>
<contexts>
<context position="21219" citStr="Malewicz et al., 2010" startWordPosition="3634" endWordPosition="3637">e emission parameters. In the next phase, we alternate the initialization by choosing the pruned emission model along with the original transition model (with full set of tag bigrams) and retrain using EM. The alternating EM iterations are terminated when the change in the size of the observed grammar (i.e., the number of unique bigrams in the tagging output) is ≤ 5%.1 We refer to our entire approach using greedy minimization followed by EM training as DMLC + EM. 5 Distributed Implementation The DMLC algorithm is directly suited towards parallelization across many machines. We turn to Pregel (Malewicz et al., 2010), and its open source version Giraph (Apa, 2013). In these systems the computation proceeds in rounds. In every round, every machine does some local processing and then sends arbitrary messages to other machines. Semantically, we think of the communication graph as fixed, and in each round each vertex performs some local computation and then sends messages to its neighbors. This mode of parallel programming directs the programmers to “Think like a vertex.” The specific systems like Pregel and Giraph build infrastructure that ensures that the overall system 1For more details on the alternating </context>
</contexts>
<marker>Malewicz, Austern, Bik, Dehnert, Horn, Leiser, Czajkowski, 2010</marker>
<rawString>Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system for largescale graph processing. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 135–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="29063" citStr="Marcus et al., 1993" startWordPosition="4932" endWordPosition="4935">sets used in previous work (Garrette and Baldridge, 2012) and compare against the previous best reported performance for the same task. In all the experiments (described here and in subsequent sections), we use the following terminology—raw data refers to unlabeled text used by different methods (for model minimization or other unsupervised training procedures such as EM), dictionary consists of word/tag entries that are legal, and test refers to data over which tagging evaluation is performed. English Data: For English POS tagging with incomplete dictionary, we evaluate on the Penn Treebank (Marcus et al., 1993) data. Following (Garrette and Baldridge, 2012), we extracted a word-tag dictionary from sections 00-15 (751,059 tokens) consisting of 39,087 word types, 45,331 word/tag entries, a per-type ambiguity of 1.16 yielding a pertoken ambiguity of 2.21 on the raw corpus (treating unknown words as having all 45 possible tags). As in their setup, we then use the first 47,996 tokens of section 16 as raw data and perform final evaluation on the sections 22-24. We use the raw corpus along with the unlabeled test data to perform model minimization and EM training. Unknown words are allowed to have all poss</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2187" citStr="Merialdo, 1994" startWordPosition="313" endWordPosition="314">ed while the author was working at Google. instance, state of the art systems obtain tagging accuracies over 97% for part-of-speech (POS) tagging on the English Penn Treebank. However, learning accurate taggers without labeled data remains a challenge. The accuracies quickly drop when faced with data from a different domain, language, or when there is very little labeled information available for training (Banko and Moore, 2004). Recently, there has been an increasing amount of research tackling this problem using unsupervised methods. A popular approach is to learn from POS-tag dictionaries (Merialdo, 1994), where we are given a raw word sequence and a dictionary of legal tags for each word type. Learning from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encounter new words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words </context>
<context position="7562" citStr="Merialdo, 1994" startWordPosition="1173" endWordPosition="1174">pertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has bee</context>
<context position="25048" citStr="Merialdo, 1994" startWordPosition="4277" endWordPosition="4278">the POS tagging task. The tagset consists of 45 distinct tag labels and the dictionary contains 57,388 word/tag pairs derived from the entire Penn Treebank. Per-token ambiguity for the test data is about 1.5 tags/token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). • MIN-GREEDY + EM: Minimizing grammar size using the two-step greedy method (Ravi et al., 2010b). • DMLC + EM: This work. Results: Table 1 shows the results for POS tagging on English Penn Treebank data. On the smaller test datasets, all of the model minimization strategies (methods 2, 3, 4) tend to perform equally well, yielding state-of-the-art results and large improvement over standard EM. When training (and testing) on larger corpora sizes, DMLC yields the best reported</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesun Moon</author>
<author>Katrin Erk</author>
<author>Jason Baldridge</author>
</authors>
<title>Crouching Dirichlet, Hidden Markov Model: Unsupervised POS tagging with context local tag generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>196--206</pages>
<contexts>
<context position="7355" citStr="Moon et al., 2010" startWordPosition="1135" endWordPosition="1138">by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldr</context>
</contexts>
<marker>Moon, Erk, Baldridge, 2010</marker>
<rawString>Taesun Moon, Katrin Erk, and Jason Baldridge. 2010. Crouching Dirichlet, Hidden Markov Model: Unsupervised POS tagging with context local tag generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 196– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>504--512</pages>
<contexts>
<context position="3608" citStr="Ravi and Knight, 2009" startWordPosition="531" endWordPosition="534">ng using tag dictionaries. The first reported work on this problem was on POS tagging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), though EM does not 105 Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Association for Computational Linguistics. perform well on this task (Johnson, 2007). More recent methods have yielded better performance than EM (see (Ravi and Knight, 2009) for an overview). One interesting line of research introduced by Ravi and Knight (2009) explores the idea of performing model minimization followed by EM training to learn taggers. Their idea is closely related to the classic Minimum Description Length principle for model selection (Barron et al., 1998). They (1) formulate an objective function to find the smallest model that explains the text (model minimization step), and then, (2) fit the minimized model to the data (EM step). For POS tagging, this method (Ravi and Knight, 2009) yields the best performance to date; 91.6% tagging accuracy o</context>
<context position="7586" citStr="Ravi and Knight, 2009" startWordPosition="1175" endWordPosition="1178">results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for bui</context>
<context position="10073" citStr="Ravi and Knight, 2009" startWordPosition="1624" endWordPosition="1627">t to minimize a specific objective function. We will refer to a tag pair (ti,j−1, tij) as a label. Our aim is to minimize the number of distinct labels used to cover the full input. Formally, given a sequence S0 i and a tag tij for each word wij in S0i, let the induced set of labels for sequence S0 i be |S&apos;| Li = U {(ti,j−1, tij)}. j=1 The total number of distinct labels used over all sequences is then �� � φ = �� ∪i Li |= i Note that the order of the tokens in the label makes a difference as {(NN, VP)} and {(VP, NN)} are two distinct labels. Now we can define the problem formally, following (Ravi and Knight, 2009). Problem 1 (Minimum Label Cover). Given a set S of sequences of words, where each word wij has a set of valid tags Tij, the problem is to find a valid tag assignment tij ∈ Tij for each word that minimizes the number of distinct labels or tag pairs over all sequences, φ = Ui U�sai+1{(ti,j−1, tij) }|. The problem is closely related to the classical Set Cover problem and is also NP-complete. To reduce Set Cover to the label selection problem, map each element i of the Set Cover instance to a single word sentence Si = wi1, and let the valid tags Ti1 contain the names of the sets that contain elem</context>
<context position="18211" citStr="Ravi and Knight, 2009" startWordPosition="3108" endWordPosition="3111">he tag for some words, an optimal label may no longer be consistent with the chosen tags, and it is not hard to find contrived examples where the size of the optimal solution doubles after each iteration of MLC. Since the underlying problem is NP-complete, it is computationally hard to give direct evidence verifying the assumption on natural language inputs. However, on small examples we are able to show that the greedy algorithm is within a small constant factor of the optimum, specifically it is within 11% of the optimum model size for the POS tagging problem using the standard 24k dataset (Ravi and Knight, 2009). Combined with the fact that the final method outperforms state of the art approaches, this leads us to conclude that the structural assumption is well justified. Lemma 1. Under the assumption of c-feasibility, the MLC algorithm achieves a O(clog m) approximation to the minimum label cover problem, where m = Ei |Si |is the total number of tokens. Proof. To prove the Lemma we will define an objective function ¯φ, counting the number of unlabeled word pairs, as a function of possible labels, and show that φ¯ decreases by a factor of (1− O(1/ck)) at every iteration. To define ¯φ, we first define</context>
<context position="20038" citStr="Ravi and Knight (2009)" startWordPosition="3440" endWordPosition="3443">s some solution having ck labels. Thus, some label from that solution covers at least a 1/ck fraction of the remaining words. The selected label (t, t0) maximizes the intersection with the remaining feasible labels. The conflict resolution step ensures that in expectation the realized benefit is at least a half of the maximum, thereby reducing φ¯ by at least a 109 (1 − 1/2ck) fraction. Therefore, after O(kclogm) operations all of the labels are covered. 4.3 Fitting the Model Using EM Once the greedy algorithm terminates and returns a minimized grammar of tag bigrams, we follow the approach of Ravi and Knight (2009) and fit the minimized model to the data using the alternating EM strategy. In this step, we run an alternating optimization procedure iteratively in phases. In each phase, we initialize (and prune away) parameters within the two HMM components (transition or emission model) using the output from the previous phase. We initialize this procedure by restricting the transition parameters to only those tag bigrams selected in the model minimization step. We train in conjunction with the original emission model using EM algorithm which prunes away some of the emission parameters. In the next phase,</context>
<context position="21958" citStr="Ravi and Knight, 2009" startWordPosition="3750" endWordPosition="3753">, every machine does some local processing and then sends arbitrary messages to other machines. Semantically, we think of the communication graph as fixed, and in each round each vertex performs some local computation and then sends messages to its neighbors. This mode of parallel programming directs the programmers to “Think like a vertex.” The specific systems like Pregel and Giraph build infrastructure that ensures that the overall system 1For more details on the alternating EM strategy and how initialization with minimized models improve EM performance in alternating iterations, refer to (Ravi and Knight, 2009). is fault tolerant, efficient, and fast. In addition, they provide implementation of commonly used distributed data structures, such as, for example global counters. The programmer’s job is simply to specify the code that each vertex will run at every round. We implemented the DMLC algorithm in Pregel. The implementation is straightforward and given in Algorithm 2. The multi-set M of Algorithm 1 is represented as a global counter in Algorithm 2. The message passing (Step 3) and counter update (Step 4) steps update this global counter and hence perform the role of Step 3 of Algorithm 1. Step 5</context>
<context position="25167" citStr="Ravi and Knight, 2009" startWordPosition="4294" endWordPosition="4297">pairs derived from the entire Penn Treebank. Per-token ambiguity for the test data is about 1.5 tags/token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). • MIN-GREEDY + EM: Minimizing grammar size using the two-step greedy method (Ravi et al., 2010b). • DMLC + EM: This work. Results: Table 1 shows the results for POS tagging on English Penn Treebank data. On the smaller test datasets, all of the model minimization strategies (methods 2, 3, 4) tend to perform equally well, yielding state-of-the-art results and large improvement over standard EM. When training (and testing) on larger corpora sizes, DMLC yields the best reported performance on this task to date. A major advantage of the new method is that it can easily scale to large corpora siz</context>
<context position="27869" citStr="Ravi and Knight, 2009" startWordPosition="4739" endWordPosition="4742">slightly faster as data size increases. Figure 1: Runtime vs. data size (measured in # of word tokens) on 100 machines. For comparison, we also plot a straight line passing through the first two runtimes. The straight line essentially plots runtimes corresponding to a linear speedup. DMLC clearly achieves better runtimes showing a better than linear speedup. 6.1.2 Tagging Using Incomplete Dictionaries We also evaluate our approach for POS tagging under other resource-constrained scenarios. Obtain111 Method Tagging accuracy (%) te=24k te=973k tr=24k tr=973k tr=3.7M 1. EM 81.7 82.3 2. ILP + EM (Ravi and Knight, 2009) 91.6 3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1 4. DMLC + EM (this work) 91.4 87.5 87.8 Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents the size of the raw text used to perform model minimization. ing a complete dictionary is often difficult, especially for new domains. To verify the utility of our method when the input dictionary is incomplete, we evaluate against standard datasets used in previous work</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP), pages 504– 512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Jason Baldridge</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>495--503</pages>
<contexts>
<context position="4459" citStr="Ravi et al. (2010" startWordPosition="674" endWordPosition="677"> Description Length principle for model selection (Barron et al., 1998). They (1) formulate an objective function to find the smallest model that explains the text (model minimization step), and then, (2) fit the minimized model to the data (EM step). For POS tagging, this method (Ravi and Knight, 2009) yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank. The original work from (Ravi and Knight, 2009) uses an integer linear programming (ILP) formulation to find minimal models, an approach which does not scale to large datasets. Ravi et al. (2010b) introduced a two-step greedy approximation to the original objective function (called the MIN-GREEDY algorithm) that runs much faster while maintaining the high tagging performance. Garrette and Baldridge (2012) showed how to use several heuristics to further improve this algorithm (for instance, better choice of tag bigrams when breaking ties) and stack other techniques on top, such as careful initialization of HMM emission models which results in further performance gains. Their method also works under incomplete dictionary scenarios and can be applied to certain low-resource scenarios (G</context>
<context position="8280" citStr="Ravi et al., 2010" startWordPosition="1287" endWordPosition="1290">and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. For</context>
<context position="14292" citStr="Ravi et al., 2010" startWordPosition="2400" endWordPosition="2403">at random with equal probability. 9: If a bigram (wi,j−1, wij) is consistent with assignments in (t0, t), fix the tentative assignments, and set Ti,j−1 = {t0} and Tij = {t}. Recompute M, the multiset of possible labels, with the updated Ti,j−1 and Tij. 10: until there are no unassigned words lem. An additional challenge comes from the fact that labels are tags for a pair of words, and hence are related. For example, if we label a word pair (wi,j−1, wij) as (NN, VP), then the label for the next word pair (wij, wi,j+1) has to be of the form (VP, *), i.e., it has to start with VP. Previous work (Ravi et al., 2010a; Ravi et al., 2010b) recognized this challenge and employed two phase heuristic approaches. Eschewing heuristics, we will show that with one natural assumption, even with this extra set of constraints, the standard greedy algorithm for this problem results in a solution with a provable approximation ratio of O(log m). In practice, however, the algorithm performs far better than the worst case ratio, and similar to the work of (Gomes et al., 2006), we find that the greedy approach selects a cover approximately 11% worse than the optimum solution. 4.2 MLC Algorithm We present in Algorithm 1 ou</context>
<context position="25263" citStr="Ravi et al., 2010" startWordPosition="4310" endWordPosition="4313">token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). • MIN-GREEDY + EM: Minimizing grammar size using the two-step greedy method (Ravi et al., 2010b). • DMLC + EM: This work. Results: Table 1 shows the results for POS tagging on English Penn Treebank data. On the smaller test datasets, all of the model minimization strategies (methods 2, 3, 4) tend to perform equally well, yielding state-of-the-art results and large improvement over standard EM. When training (and testing) on larger corpora sizes, DMLC yields the best reported performance on this task to date. A major advantage of the new method is that it can easily scale to large corpora sizes and the distributed nature of the algorithm still permits fast, efficient optimization of the</context>
<context position="27912" citStr="Ravi et al., 2010" startWordPosition="4748" endWordPosition="4751">: Runtime vs. data size (measured in # of word tokens) on 100 machines. For comparison, we also plot a straight line passing through the first two runtimes. The straight line essentially plots runtimes corresponding to a linear speedup. DMLC clearly achieves better runtimes showing a better than linear speedup. 6.1.2 Tagging Using Incomplete Dictionaries We also evaluate our approach for POS tagging under other resource-constrained scenarios. Obtain111 Method Tagging accuracy (%) te=24k te=973k tr=24k tr=973k tr=3.7M 1. EM 81.7 82.3 2. ILP + EM (Ravi and Knight, 2009) 91.6 3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1 4. DMLC + EM (this work) 91.4 87.5 87.8 Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents the size of the raw text used to perform model minimization. ing a complete dictionary is often difficult, especially for new domains. To verify the utility of our method when the input dictionary is incomplete, we evaluate against standard datasets used in previous work (Garrette and Baldridge, 2012) and compare</context>
<context position="36943" citStr="Ravi et al. (2010" startWordPosition="6208" endWordPosition="6211">90.3) 66.1 (45.1) Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5) Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens in the test data. Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes: (a) small, (b) tiny (shown in parentheses). The new method (row 2) significantly outperforms the existing method with p &lt; 0.01 for small dictionary and p &lt; 0.05 for tiny dictionary. a weighted version of model minimization which uses the multi-step greedy approach from Ravi et al. (2010b) enhanced with additional heuristics that uses tag weights learnt via label propagation (in Step 1) within the minimization process. We replace the model minimization procedure in their Step 2 with our method (DMLC + EM) and directly compare this new system with their approach in terms of tagging accuracy. Note for all other steps in the pipeline we follow the same procedure (and run the same code) as Garrette and Baldridge (2013), including the same smoothing procedure for EM initialization in Step 3. Data: We use the exact same setup as Garrette and Baldridge (2013) and run experiments on </context>
<context position="39538" citStr="Ravi et al., 2010" startWordPosition="6610" endWordPosition="6613">n, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. As described earlier, our approach scales easily to large datasets as well as label sizes. To evaluate it on the supertagging task, we use the same dataset from (Ravi et al., 2010a) and compare against their baseline method that uses an modified (two-step) version 114 Method Supertagging accuracy (%) Ambiguous Total 1. EM 38.7 45.6 2. ILP∗ + EM (Ravi et al., 2010a) 52.1 57.3 3. DMLC + EM (this work) 55.9 59.3 Table 4: Results for unsupervised supertagging with a dictionary. Here, we report the total accuracy as well as accuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility). ∗The baseline method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In contrast, the new approa</context>
<context position="41113" citStr="Ravi et al. (2010" startWordPosition="6867" endWordPosition="6870"> experiments (Banko and Moore, 2004)—sections 0-18 for training (i.e., to construct the word-tag dictionary), and sections 22-24 for test. Results: Table 4 compares the results for two baseline systems—standard EM (method 1), and a previously reported system using model minimization (method 2) for the same task. We observe that DMLC produces better taggings than either of these and yields significant improvement in accuracy (+2% overall, +3.8% on ambiguous tokens). Note that it is not feasible to run the ILP-based baseline (method 2 in the table) directly since it is very slow in practice, so Ravi et al. (2010a) use a set of pre-processing steps to prune the original grammar size (unique tag pairs) from &gt;1M to several thousand entries followed by a modified twostep ILP minimization strategy. This is required to permit their model minimization step to be run in a feasible manner. On the other hand, the new approach DMLC (method 3) scales better even when the data/label sizes are large, hence it can be run with the full data using the original model minimization formulation (rather than a two-step heuristic). Ravi et al. (2010a) also report further improvements using an alternative approach involving</context>
</contexts>
<marker>Ravi, Baldridge, Knight, 2010</marker>
<rawString>Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a. Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Ashish Vaswani</author>
<author>Kevin Knight</author>
<author>David Chiang</author>
</authors>
<title>Fast, greedy model minimization for unsupervised tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>940--948</pages>
<contexts>
<context position="4459" citStr="Ravi et al. (2010" startWordPosition="674" endWordPosition="677"> Description Length principle for model selection (Barron et al., 1998). They (1) formulate an objective function to find the smallest model that explains the text (model minimization step), and then, (2) fit the minimized model to the data (EM step). For POS tagging, this method (Ravi and Knight, 2009) yields the best performance to date; 91.6% tagging accuracy on a standard test dataset from the English Penn Treebank. The original work from (Ravi and Knight, 2009) uses an integer linear programming (ILP) formulation to find minimal models, an approach which does not scale to large datasets. Ravi et al. (2010b) introduced a two-step greedy approximation to the original objective function (called the MIN-GREEDY algorithm) that runs much faster while maintaining the high tagging performance. Garrette and Baldridge (2012) showed how to use several heuristics to further improve this algorithm (for instance, better choice of tag bigrams when breaking ties) and stack other techniques on top, such as careful initialization of HMM emission models which results in further performance gains. Their method also works under incomplete dictionary scenarios and can be applied to certain low-resource scenarios (G</context>
<context position="8280" citStr="Ravi et al., 2010" startWordPosition="1287" endWordPosition="1290">and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. For</context>
<context position="14292" citStr="Ravi et al., 2010" startWordPosition="2400" endWordPosition="2403">at random with equal probability. 9: If a bigram (wi,j−1, wij) is consistent with assignments in (t0, t), fix the tentative assignments, and set Ti,j−1 = {t0} and Tij = {t}. Recompute M, the multiset of possible labels, with the updated Ti,j−1 and Tij. 10: until there are no unassigned words lem. An additional challenge comes from the fact that labels are tags for a pair of words, and hence are related. For example, if we label a word pair (wi,j−1, wij) as (NN, VP), then the label for the next word pair (wij, wi,j+1) has to be of the form (VP, *), i.e., it has to start with VP. Previous work (Ravi et al., 2010a; Ravi et al., 2010b) recognized this challenge and employed two phase heuristic approaches. Eschewing heuristics, we will show that with one natural assumption, even with this extra set of constraints, the standard greedy algorithm for this problem results in a solution with a provable approximation ratio of O(log m). In practice, however, the algorithm performs far better than the worst case ratio, and similar to the work of (Gomes et al., 2006), we find that the greedy approach selects a cover approximately 11% worse than the optimum solution. 4.2 MLC Algorithm We present in Algorithm 1 ou</context>
<context position="25263" citStr="Ravi et al., 2010" startWordPosition="4310" endWordPosition="4313">token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). • MIN-GREEDY + EM: Minimizing grammar size using the two-step greedy method (Ravi et al., 2010b). • DMLC + EM: This work. Results: Table 1 shows the results for POS tagging on English Penn Treebank data. On the smaller test datasets, all of the model minimization strategies (methods 2, 3, 4) tend to perform equally well, yielding state-of-the-art results and large improvement over standard EM. When training (and testing) on larger corpora sizes, DMLC yields the best reported performance on this task to date. A major advantage of the new method is that it can easily scale to large corpora sizes and the distributed nature of the algorithm still permits fast, efficient optimization of the</context>
<context position="27912" citStr="Ravi et al., 2010" startWordPosition="4748" endWordPosition="4751">: Runtime vs. data size (measured in # of word tokens) on 100 machines. For comparison, we also plot a straight line passing through the first two runtimes. The straight line essentially plots runtimes corresponding to a linear speedup. DMLC clearly achieves better runtimes showing a better than linear speedup. 6.1.2 Tagging Using Incomplete Dictionaries We also evaluate our approach for POS tagging under other resource-constrained scenarios. Obtain111 Method Tagging accuracy (%) te=24k te=973k tr=24k tr=973k tr=3.7M 1. EM 81.7 82.3 2. ILP + EM (Ravi and Knight, 2009) 91.6 3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1 4. DMLC + EM (this work) 91.4 87.5 87.8 Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents the size of the raw text used to perform model minimization. ing a complete dictionary is often difficult, especially for new domains. To verify the utility of our method when the input dictionary is incomplete, we evaluate against standard datasets used in previous work (Garrette and Baldridge, 2012) and compare</context>
<context position="36943" citStr="Ravi et al. (2010" startWordPosition="6208" endWordPosition="6211">90.3) 66.1 (45.1) Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5) Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens in the test data. Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes: (a) small, (b) tiny (shown in parentheses). The new method (row 2) significantly outperforms the existing method with p &lt; 0.01 for small dictionary and p &lt; 0.05 for tiny dictionary. a weighted version of model minimization which uses the multi-step greedy approach from Ravi et al. (2010b) enhanced with additional heuristics that uses tag weights learnt via label propagation (in Step 1) within the minimization process. We replace the model minimization procedure in their Step 2 with our method (DMLC + EM) and directly compare this new system with their approach in terms of tagging accuracy. Note for all other steps in the pipeline we follow the same procedure (and run the same code) as Garrette and Baldridge (2013), including the same smoothing procedure for EM initialization in Step 3. Data: We use the exact same setup as Garrette and Baldridge (2013) and run experiments on </context>
<context position="39538" citStr="Ravi et al., 2010" startWordPosition="6610" endWordPosition="6613">n, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. As described earlier, our approach scales easily to large datasets as well as label sizes. To evaluate it on the supertagging task, we use the same dataset from (Ravi et al., 2010a) and compare against their baseline method that uses an modified (two-step) version 114 Method Supertagging accuracy (%) Ambiguous Total 1. EM 38.7 45.6 2. ILP∗ + EM (Ravi et al., 2010a) 52.1 57.3 3. DMLC + EM (this work) 55.9 59.3 Table 4: Results for unsupervised supertagging with a dictionary. Here, we report the total accuracy as well as accuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility). ∗The baseline method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In contrast, the new approa</context>
<context position="41113" citStr="Ravi et al. (2010" startWordPosition="6867" endWordPosition="6870"> experiments (Banko and Moore, 2004)—sections 0-18 for training (i.e., to construct the word-tag dictionary), and sections 22-24 for test. Results: Table 4 compares the results for two baseline systems—standard EM (method 1), and a previously reported system using model minimization (method 2) for the same task. We observe that DMLC produces better taggings than either of these and yields significant improvement in accuracy (+2% overall, +3.8% on ambiguous tokens). Note that it is not feasible to run the ILP-based baseline (method 2 in the table) directly since it is very slow in practice, so Ravi et al. (2010a) use a set of pre-processing steps to prune the original grammar size (unique tag pairs) from &gt;1M to several thousand entries followed by a modified twostep ILP minimization strategy. This is required to permit their model minimization step to be run in a feasible manner. On the other hand, the new approach DMLC (method 3) scales better even when the data/label sizes are large, hence it can be run with the full data using the original model minimization formulation (rather than a two-step heuristic). Ravi et al. (2010a) also report further improvements using an alternative approach involving</context>
</contexts>
<marker>Ravi, Vaswani, Knight, Chiang, 2010</marker>
<rawString>Sujith Ravi, Ashish Vaswani, Kevin Knight, and David Chiang. 2010b. Fast, greedy model minimization for unsupervised tagging. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 940–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Raanan Fattal</author>
<author>Ari Rappoport</author>
</authors>
<title>Improved unsupervised POS induction using intrinsic clustering quality and a Zipfian constraint.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>57--66</pages>
<contexts>
<context position="7335" citStr="Reichart et al., 2010" startWordPosition="1131" endWordPosition="1134">ower of the new method by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). 106 Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources</context>
</contexts>
<marker>Reichart, Fattal, Rappoport, 2010</marker>
<rawString>Roi Reichart, Raanan Fattal, and Ari Rappoport. 2010. Improved unsupervised POS induction using intrinsic clustering quality and a Zipfian constraint. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="38929" citStr="Steedman, 2000" startWordPosition="6515" endWordPosition="6516">orporates a multi-step weighted model minimization combined with additional heuristics. We observe that switching to the new model minimization procedure alone yields significant improvement in tagging accuracy under both dictionary scenarios. It is encouraging that a better minimization procedure also leads to higher tagging quality on the unknown word tokens (column 4 in the table), even when the input dictionary is tiny. 6.2 Supertagging Compared to POS tagging, a more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. As described earlier, our approach scales easily to large datasets as well as label sizes. To evaluate it on the supertagging task, we use the same dataset from (Ravi et </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised partof-speech tagging.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>1521--1528</pages>
<contexts>
<context position="2838" citStr="Toutanova and Johnson, 2008" startWordPosition="417" endWordPosition="420">raw word sequence and a dictionary of legal tags for each word type. Learning from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encounter new words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words using some heuristics (Toutanova and Johnson, 2008) or using other methods such as type-supervision (Garrette and Baldridge, 2012). In this work, we tackle the problem of unsupervised sequence labeling using tag dictionaries. The first reported work on this problem was on POS tagging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), though EM does not 105 Transactions of the Association for Computational Linguistics, 2 (2014) 105–118. Action Editor: Sharon Goldwater. Submitted 11/2013; Revised 2/2014; Published 4/2014. c�2014 Asso</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian LDA-based model for semi-supervised partof-speech tagging. In Advances in Neural Information Processing Systems (NIPS), pages 1521–1528.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001827">
<title confidence="0.998903">
Grounded Compositional Semantics
for Finding and Describing Images with Sentences
</title>
<author confidence="0.997641">
Richard Socher, Andrej Karpathy, Quoc V. Le*, Christopher D. Manning, Andrew Y. Ng
</author>
<affiliation confidence="0.997279">
Stanford University, Computer Science Department, *Google Inc.
</affiliation>
<email confidence="0.9688355">
richard@socher.org, karpathy@cs.stanford.edu,
qvl@google.com, manning@stanford.edu, ang@cs.stanford.edu
</email>
<sectionHeader confidence="0.99662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999420045454546">
Previous work on Recursive Neural Networks
(RNNs) shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im-
ages. However, the sentence vectors of previ-
ous models cannot accurately represent visu-
ally grounded meaning. We introduce the DT-
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences. Unlike previous RNN-based mod-
els which use constituency trees, DT-RNNs
naturally focus on the action and agents in
a sentence. They are better able to abstract
from the details of word order and syntactic
expression. DT-RNNs outperform other re-
cursive and recurrent neural networks, kernel-
ized CCA and a bag-of-words baseline on the
tasks of finding an image that fits a sentence
description and vice versa. They also give
more similar representations to sentences that
describe the same image.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869931818182">
Single word vector spaces are widely used (Turney
and Pantel, 2010) and successful at classifying sin-
gle words and capturing their meaning (Collobert
and Weston, 2008; Huang et al., 2012; Mikolov et
al., 2013). Since words rarely appear in isolation,
the task of learning compositional meaning repre-
sentations for longer phrases has recently received a
lot of attention (Mitchell and Lapata, 2010; Socher
et al., 2010; Socher et al., 2012; Grefenstette et al.,
2013). Similarly, classifying whole images into a
fixed set of classes also achieves very high perfor-
mance (Le et al., 2012; Krizhevsky et al., 2012).
However, similar to words, objects in images are of-
ten seen in relationships with other objects which are
not adequately described by a single label.
In this work, we introduce a model, illustrated in
Fig. 1, which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other. We assume word and
image representations are first learned in their re-
spective single modalities but finally mapped into a
jointly learned multimodal embedding space.
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
(RNNs) (Pollack, 1990; Costa et al., 2003; Socher
et al., 2011b). However, unlike all previous RNN
models which are based on constituency trees (CT-
RNNs), our model computes compositional vector
representations inside dependency trees. The com-
positional vectors computed by this new dependency
tree RNN (DT-RNN) capture more of the meaning
of sentences, where we define meaning in terms of
similarity to a “visual representation” of the textual
description. DT-RNN induced vector representa-
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod-
els such as CT-RNNs or Recurrent Neural Networks
since they naturally focus on a sentence’s action and
its agents.
We evaluate and compare DT-RNN induced rep-
resentations on their ability to use a sentence such as
“A man wearing a helmet jumps on his bike near a
beach.” to find images that show such a scene. The
goal is to learn sentence representations that capture
</bodyText>
<page confidence="0.986603">
207
</page>
<tableCaption confidence="0.3543385">
Transactions of the Association for Computational Linguistics, 2 (2014) 207–218. Action Editor: Alexander Clark.
Submitted 10/2013; Revised 3/2014; Published 4/2014. c�2014 Association for Computational Linguistics.
</tableCaption>
<figure confidence="0.994723125">
A man wearing a helmet jumps on his bike near a beach.
Compositional Sentence Vectors
A small child sits on a cement wall near white flower.
Two airplanes parked in an airport.
A man jumping his downhill bike.
Multi-Modal
Representations
Image Vector Representation
</figure>
<figureCaption confidence="0.994161">
Figure 1: The DT-RNN learns vector representations for sentences based on their dependency trees. We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images. This allows us to query images with a sentence and give sentence descriptions to images.
</figureCaption>
<bodyText confidence="0.999539833333333">
the visual scene described and to find appropriate
images in the learned, multi-modal sentence-image
space. Conversely, when given a query image, we
would like to find a description that goes beyond a
single label by providing a correct sentence describ-
ing it, a task that has recently garnered a lot of at-
tention (Farhadi et al., 2010; Ordonez et al., 2011;
Kuznetsova et al., 2012). We use the dataset intro-
duced by (Rashtchian et al., 2010) which consists of
1000 images, each with 5 descriptions. On all tasks,
our model outperforms baselines and related mod-
els.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998585">
The presented model is connected to several areas of
NLP and vision research, each with a large amount
of related work to which we can only do some justice
given space constraints.
</bodyText>
<subsectionHeader confidence="0.917559">
Semantic Vector Spaces and Their Composition-
</subsectionHeader>
<bodyText confidence="0.999966553191489">
ality. The dominant approach in semantic vec-
tor spaces uses distributional similarities of single
words. Often, co-occurrence statistics of a word and
its context are used to describe each word (Turney
and Pantel, 2010; Baroni and Lenci, 2010), such
as tf-idf. Most of the compositionality algorithms
and related datasets capture two-word compositions.
For instance, (Mitchell and Lapata, 2010) use two-
word phrases and analyze similarities computed by
vector addition, multiplication and others. Compo-
sitionality is an active field of research with many
different models and representations being explored
(Grefenstette et al., 2013), among many others. We
compare to supervised compositional models that
can learn task-specific vector representations such as
constituency tree recursive neural networks (Socher
et al., 2011b; Socher et al., 2011a), chain structured
recurrent neural networks and other baselines. An-
other alternative would be to use CCG trees as a
backbone for vector composition (K.M. Hermann,
2013).
Multimodal Embeddings. Multimodal embed-
ding methods project data from multiple sources
such as sound and video (Ngiam et al., 2011) or im-
ages and text. Socher et al. (Socher and Fei-Fei,
2010) project words and image regions into a com-
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an-
notation and segmentation. Similar to our work, they
use unsupervised large text corpora to learn seman-
tic word representations. Among other recent work
is that by Srivastava and Salakhutdinov (2012) who
developed multimodal Deep Boltzmann Machines.
Similar to their work, we use techniques from the
broad field of deep learning to represent images and
words.
Recently, single word vector embeddings have
been used for zero shot learning (Socher et al.,
2013c). Mapping images to word vectors enabled
their system to classify images as depicting objects
such as ”cat” without seeing any examples of this
class. Related work has also been presented at NIPS
(Socher et al., 2013b; Frome et al., 2013). This work
moves zero-shot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences, making use of similar ideas of se-
mantic spaces grounded in visual knowledge.
</bodyText>
<page confidence="0.996889">
208
</page>
<bodyText confidence="0.997769292682927">
Detailed Image Annotation. Interactions be-
tween images and texts is a growing research field.
Early work in this area includes generating single
words or fixed phrases from images (Duygulu et al.,
2002; Barnard et al., 2003) or using contextual in-
formation to improve recognition (Gupta and Davis,
2008; Torralba et al., 2010).
Apart from a large body of work on single object
image classification (Le et al., 2012), there is also
work on attribute classification and other mid-level
elements (Kumar et al., 2009), some of which we
hope to capture with our approach as well.
Our work is close in spirit with recent work in de-
scribing images with more detailed, longer textual
descriptions. In particular, Yao et al. (2010) describe
images using hierarchical knowledge and humans in
the loop. In contrast, our work does not require hu-
man interactions. Farhadi et al. (2010) and Kulkarni
et al. (2011), on the other hand, use a more automatic
method to parse images. For instance, the former ap-
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images. It forms representations
to describe 1 object, 1 action, and 1 scene. Kulkarni
et al. (2011) extends their method to describe an im-
age with multiple objects. None of these approaches
have used a compositional sentence vector repre-
sentation and they require specific language gener-
ation techniques and sophisticated inference meth-
ods. Since our model is based on neural networks in-
ference is fast and simple. Kuznetsova et al. (2012)
use a very large parallel corpus to connect images
and sentences. Feng and Lapata (2013) use a large
dataset of captioned images and experiments with
both extractive (search) and abstractive (generation)
models.
Most related is the very recent work of Hodosh et
al. (2013). They too evaluate using a ranking mea-
sure. In our experiments, we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments.
</bodyText>
<sectionHeader confidence="0.928165" genericHeader="method">
3 Dependency-Tree Recursive Neural
Networks
</sectionHeader>
<bodyText confidence="0.99977869047619">
In this section we first focus on the DT-RNN model
that computes compositional vector representations
for phrases and sentences of variable length and syn-
tactic type. In section 5 the resulting vectors will
then become multimodal features by mapping im-
ages that show what the sentence describes to the
same space and learning both the image and sen-
tence mapping jointly.
The most common way of building representa-
tions for longer phrases from single word vectors is
to simply linearly average the word vectors. While
this bag-of-words approach can yield reasonable
performance in some tasks, it gives all the words the
same weight and cannot distinguish important dif-
ferences in simple visual descriptions such as The
bike crashed into the standing car. vs. The car
crashed into the standing bike..
RNN models (Pollack, 1990; Goller and K¨uchler,
1996; Socher et al., 2011b; Socher et al., 2011a) pro-
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag-
ing. They combine vectors with an RNN in binary
constituency trees which have potentially many hid-
den layers. While the induced vector representations
work very well on many tasks, they also inevitably
capture a lot of syntactic structure of the sentence.
However, the task of finding images from sentence
descriptions requires us to be more invariant to syn-
tactic differences. One such example are active-
passive constructions which can collapse words such
as “by” in some formalisms (de Marneffe et al.,
2006), relying instead on the semantic relationship
of “agent”. For instance, The mother hugged her
child. and The child was hugged by its mother.
should map to roughly the same visual space. Cur-
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep-
resentations would be influenced by the words was
and by. The model we describe below focuses more
on recognizing actions and agents and has the po-
tential to learn representations that are invariant to
active-passive differences.
</bodyText>
<subsectionHeader confidence="0.9448985">
3.1 DT-RNN Inputs: Word Vectors and
Dependency Trees
</subsectionHeader>
<bodyText confidence="0.999889">
In order for the DT-RNN to compute a vector repre-
sentation for an ordered list of m words (a phrase or
sentence), we map the single words to a vector space
and then parse the sentence.
First, we map each word to a d-dimensional vec-
tor. We initialize these word vectors with the un-
</bodyText>
<page confidence="0.994491">
209
</page>
<figure confidence="0.99951175">
root
prep poss
prep
pobj
pobj
det
nsubj
dobj
det
partmod
det
A man wearing a helmet jumps on his bike near a beach
</figure>
<figureCaption confidence="0.990637">
Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. The final representation is computed
at the root node, here at the verb jumps. Note that more important activity and object words are higher up in this tree
structure.
</figureCaption>
<bodyText confidence="0.9973361875">
supervised model of Huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrix A which stores word vectors as columns. In
order to predict correct scores the vectors in the ma-
trix capture co-occurrence statistics. We use d = 50
in all our experiments. The embedding matrix X
is then used by finding the column index i of each
word: [w] = i and retrieving the corresponding col-
umn xw from X. Henceforth, we represent an input
sentence s as an ordered list of (word,vector) pairs:
s = ((w1, xwi), ... , (wm, xwm)).
Next, the sequence of words (w1, ... , wm) is
parsed by the dependency parser of de Marneffe
et al. (2006). Fig. 2 shows an example. We can
represent a dependency tree d of a sentence s as
an ordered list of (child,parent) indices: d(s) =
{(i, j)}, where every child word in the sequence
i = 1, ... , m is present and has any word j ∈
{1, ... , m} ∪ {0} as its parent. The root word has
as its parent 0 and we notice that the same word can
be a parent between zero and m number of times.
Without loss of generality, we assume that these in-
dices form a tree structure. To summarize, the input
to the DT-RNN for each sentence is the pair (s, d):
the words and their vectors and the dependency tree.
</bodyText>
<subsectionHeader confidence="0.999357">
3.2 Forward Propagation in DT-RNNs
</subsectionHeader>
<bodyText confidence="0.984070166666667">
Given these two inputs, we now illustrate how the
DT-RNN computes parent vectors. We will use the
following sentence as a running example: Students1
ride2 bikes3 at4 night5. Fig. 3 shows its tree
and computed vector representations. The depen-
Students bikes night
</bodyText>
<figureCaption confidence="0.9915425">
Figure 3: Example of a DT-RNN tree structure for com-
puting a sentence representation in a bottom up fashion.
</figureCaption>
<bodyText confidence="0.989020733333333">
dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d =
{(1, 2), (2, 0), (3, 2), (4, 2), (5, 4)}.
The DT-RNN model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function gθ which is parameterized by
all the model parameters θ. To this end, the algo-
rithm searches for nodes in a tree that have either
(i) no children or (ii) whose children have already
been computed and then computes the correspond-
ing vector.
In our example, the words x1, x3, x5 are leaf
nodes and hence, we can compute their correspond-
ing hidden nodes via:
</bodyText>
<equation confidence="0.954573">
hc = gθ(xc) = f(Wvxc) for c = 1, 3, 5, (1)
</equation>
<bodyText confidence="0.9999525">
where we compute the hidden vector at position c
via our general composition function gθ. In the case
of leaf nodes, this composition function becomes
simply a linear layer, parameterized by Wv ∈ ][8n×d,
followed by a nonlinearity. We cross-validate over
using no nonlinearity (f = id), tanh, sigmoid or
rectified linear units (f = max(0, x), but generally
find tanh to perform best.
The final sentence representation we want to com-
pute is at h2, however, since we still do not have h4,
</bodyText>
<figure confidence="0.97846125">
x1
x3
at
x5
h2
x2
h4
ride
h1
x4
h3
h5
</figure>
<page confidence="0.987257">
210
</page>
<bodyText confidence="0.83317">
we compute that one next:
</bodyText>
<equation confidence="0.99811">
h4 = gθ(x4, h5) = f(Wvx4 + Wr1h5), (2)
</equation>
<bodyText confidence="0.999635277777778">
where we use the same Wv as before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5, the only child
of the fourth node. The matrix Wr1 ∈ Rn×n is used
because node 5 is the first child node on the right
side of node 4. Generally, we have multiple matri-
ces for composing with hidden child vectors from
the right and left sides: Wr· = (Wr1, . . . , Wrkr) and
Wl· = (Wl1, . . . , Wlkl). The number of needed ma-
trices is determined by the data by simply finding
the maximum numbers of left kl and right kr chil-
dren any node has. If at test time a child appeared
at an even large distance (this does not happen in
our test set), the corresponding matrix would be the
identity matrix.
Now that all children of h2 have their hidden vec-
tors, we can compute the final sentence representa-
tion via:
</bodyText>
<equation confidence="0.999918">
h2 = gθ(x2, h1, h3, h4) = (3)
f(Wvx2 + Wl1h1 + Wr1h3 + Wr2h4).
</equation>
<bodyText confidence="0.999973846153846">
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node.
Another modification that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under-
neath them and normalize by the sum of words under
all children. This encourages the intuitive desidera-
tum that nodes describing longer phrases are more
important. Let `(i) be the number of leaf nodes
(words) under node i and C(i, y) be the set of child
nodes of node i in dependency tree y. The final com-
position function for a node vector hi becomes:
</bodyText>
<equation confidence="0.991466">
� �1
hi = f Wvxi + E `(j)Wpos(i,j)hj
j ∈C(i) ))
</equation>
<bodyText confidence="0.9027945">
(4)
where by definition `(i) = 1 + Ej∈C(i) `(j) and
pos(i, j) is the relative position of child j with re-
spect to node i, e.g. l1 or r2 in Eq. 3.
</bodyText>
<subsectionHeader confidence="0.997968">
3.3 Semantic Dependency Tree RNNs
</subsectionHeader>
<bodyText confidence="0.999648210526316">
An alternative is to condition the weight matrices
on the semantic relations given by the dependency
parser. We use the collapsed tree formalism of
the Stanford dependency parser (de Marneffe et al.,
2006). With such a semantic untying of the weights,
the DT-RNN makes better use of the dependency
formalism and could give active-passive reversals
similar semantic vector representation. The equation
for this semantic DT-RNN (SDT-RNN) is the same
as the one above except that the matrices Wpos(i,j)
are replaced with matrices based on the dependency
relationship. There are a total of 141 unique such
relationships in the dataset. However, most are very
rare. For examples of semantic relationships, see
Fig. 2 and the model analysis section 6.7.
This forward propagation can be used for com-
puting compositional vectors and in Sec. 5 we will
explain the objective function in which these are
trained.
</bodyText>
<subsectionHeader confidence="0.990638">
3.4 Comparison to Previous RNN Models
</subsectionHeader>
<bodyText confidence="0.999515142857143">
The DT-RNN has several important differences to
previous RNN models of Socher et al. (2011a) and
(Socher et al., 2011b; Socher et al., 2011c). These
constituency tree RNNs (CT-RNNs) use the follow-
ing composition function to compute a hidden par-
ent vector h from exactly two child vectors (c1, c2)
in a binary tree: h = fc2
</bodyText>
<equation confidence="0.664741333333333">
W c1
( Iwhere W ∈
1),
</equation>
<bodyText confidence="0.999687523809524">
Rd×2d is the main parameter to learn. This can be
rewritten to show the similarity to the DT-RNN as
h = f(Wl1c1 + Wr1c2). However, there are several
important differences.
Note first that in previous RNN models the par-
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition. In contrast, our new model first
maps single words into a hidden space and then par-
ent nodes are composed from these hidden vectors.
This allows a higher capacity representation which
is especially helpful for nodes that have many chil-
dren.
Secondly, the DT-RNN allows for n-ary nodes in
the tree. This is an improvement that is possible even
for constituency tree CT-RNNs but it has not been
explored in previous models.
Third, due to computing parent nodes in con-
stituency trees, previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the final sentence rep-
</bodyText>
<page confidence="0.997354">
211
</page>
<figureCaption confidence="0.997986333333333">
Figure 4: The architecture of the visual model. This model has 3 sequences of filtering, pooling and local contrast
normalization layers. The learnable parameters are the filtering layer. The filters are not shared, i.e., the network is
nonconvolutional.
</figureCaption>
<bodyText confidence="0.999932705882353">
resentation. This can be problematic since these are
often simple non-content words, such as a leading
‘But,’. While such single words can be important for
tasks such as sentiment analysis, we argue that for
describing visual scenes the DT-RNN captures the
more important effects: The dependency tree struc-
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence, by construction, the final sentence
representation is more robust to less important ad-
jectival modifiers, word order changes, etc.
Fourth, we allow some untying of weights de-
pending on either how far away a constituent is from
the current word or what its semantic relationship is.
Now that we can compute compositional vector
representations for sentences, the next section de-
scribes how we represent images.
</bodyText>
<sectionHeader confidence="0.891232" genericHeader="method">
4 Learning Image Representations with
Neural Networks
</sectionHeader>
<bodyText confidence="0.998772729166666">
The image features that we use in our experiments
are extracted from a deep neural network, replicated
from the one described in (Le et al., 2012). The net-
work was trained using both unlabeled data (random
web images) and labeled data to classify 22,000 cat-
egories in ImageNet (Deng et al., 2009). We then
used the features at the last layer, before the classi-
fier, as the feature representation in our experiments.
The dimension of the feature vector of the last layer
is 4,096. The details of the model and its training
procedures are as follows.
The architecture of the network can be seen in
Figure 4. The network takes 200x200 pixel images
as inputs and has 9 layers. The layers consist of
three sequences of filtering, pooling and local con-
trast normalization (Jarrett et al., 2009). The pooling
function is L2 pooling of the previous layer (taking
the square of the filtering units, summing them up
in a small area in the image, and taking the square-
root). The local contrast normalization takes inputs
in a small area of the lower layer, subtracts the mean
and divides by the standard deviation.
The network was first trained using an unsuper-
vised objective: trying to reconstruct the input while
keeping the neurons sparse. In this phase, the net-
work was trained on 20 million images randomly
sampled from the web. We resized a given image
so that its short dimension has 200 pixels. We then
cropped a fixed size 200x200 pixel image right at the
center of the resized image. This means we may dis-
card a fraction of the long dimension of the image.
After unsupervised training, we used Ima-
geNet (Deng et al., 2009) to adjust the features in the
entire network. The ImageNet dataset has 22,000
categories and 14 million images. The number of
images in each category is equal across categories.
The 22,000 categories are extracted from WordNet.
To speed up the supervised training of this net-
work, we made a simple modification to the algo-
rithm described in Le et al. (2012): adding a “bottle-
neck” layer in between the last layer and the classi-
fier. to reduce the number of connections. We added
one “bottleneck” layer which has 4,096 units in be-
tween the last layer of the network and the softmax
layer. This newly-added layer is fully connected to
the previous layer and has a linear activation func-
tion. The total number of connections of this net-
work is approximately 1.36 billion.
</bodyText>
<page confidence="0.996052">
212
</page>
<bodyText confidence="0.999928315789474">
The network was trained again using the super-
vised objective of classifying the 22,000 classes in
ImageNet. Most features in the networks are local,
which allows model parallelism. Data parallelism
by asynchronous SGD was also employed as in Le
et al. (2012). The entire training, both unsupervised
and supervised, took 8 days on a large cluster of ma-
chines. This network achieves 18.3% precision@1
on the full ImageNet dataset (Release Fall 2011).
We will use the features at the bottleneck layer as
the feature vector z of an image. Each scaled and
cropped image is presented to our network. The net-
work then performs a feedforward computation to
compute the values of the bottleneck layer. This
means that every image is represented by a fixed
length vector of 4,096 dimensions. Note that during
training, no aligned sentence-image data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset.
</bodyText>
<sectionHeader confidence="0.994586" genericHeader="method">
5 Multimodal Mappings
</sectionHeader>
<bodyText confidence="0.999548043478261">
The previous two sections described how we can
map sentences into a d = 50-dimensional space and
how to extract high quality image feature vectors of
4096 dimensions. We now define our final multi-
modal objective function for learning joint image-
sentence representations with these models. Our
training set consists of N images and their feature
vectors zi and each image has 5 sentence descrip-
tions sit, . . . , si5 for which we use the DT-RNN to
compute vector representations. See Fig. 5 for ex-
amples from the dataset. For training, we use a max-
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products. Let vi = WIzi be the mapped image
vector and yij = DTRNNθ(sij) the composed sen-
tence vector. We define S to be the set of all sentence
indices and S(i) the set of sentence indices corre-
sponding to image i. Similarly, Z is the set of all im-
age indices and Z(j) is the image index of sentence
j. The set P is the set of all correct image-sentence
training pairs (i, j). The ranking cost function to
minimize is then: J(WI, 0) =
</bodyText>
<equation confidence="0.9988485">
� � max(0, A − vT i yj + vTi yc)
(i,j)EP cES\S(i)
+ � � max(0, A − vT i yj + vT c yj), (5)
(i,j)EP cEI\I(j)
</equation>
<bodyText confidence="0.999979">
where 0 are the language composition matrices,
and both second sums are over other sentences com-
ing from different images and vice versa. The hyper-
parameter A is the margin. The margin is found via
cross validation on the dev set and usually around 1.
The final objective also includes the regulariza-
tion term λ/left(II0II22 + IIWIIIF). Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters. Hence, to prevent overfitting,
we assume their weights are fixed and only train the
DT-RNN parameters WI. If larger training corpora
become available in the future, training both jointly
becomes feasible and would present a very promis-
ing direction. We use a modified version of Ada-
Grad (Duchi et al., 2011) for optimization of both
WI and the DT-RNN as well as the other baselines
(except kCCA). Adagrad has achieved good perfor-
mance previously in neural networks models (Dean
et al., 2012; Socher et al., 2013a). We modify it
by resetting all squared gradient sums to 1 every 5
epochs. With both images and sentences in the same
multimodal space, we can easily query the model for
similar images or sentences by finding the nearest
neighbors in terms of negative inner products.
An alternative objective function is based on the
squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This
requires an alternating minimization scheme that
first trains only WI, then fixes WI and trains the
DT-RNN weights 0 and then repeats this several
times. We find that the performance with this ob-
jective function (paired with finding similar images
using Euclidean distances) is worse for all models
than the margin loss of Eq. 5. In addition kCCA
also performs much better using inner products in
the multimodal space.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999887222222222">
We use the dataset of Rashtchian et al. (2010) which
consists of 1000 images, each with 5 sentences. See
Fig. 5 for examples.
We evaluate and compare the DT-RNN in three
different experiments. First, we analyze how well
the sentence vectors capture similarity in visual
meaning. Then we analyze Image Search with
Query Sentences: to query each model with a sen-
tence in order to find an image showing that sen-
</bodyText>
<page confidence="0.994526">
213
</page>
<listItem confidence="0.992419">
1. A woman and her dog watch the cameraman in their living with wooden floors.
2. A woman sitting on the couch while a black faced dog runs across the floor.
3. A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her.
4. A women sitting on a sofa while a small Jack Russell walks towards the camera.
5. White and black small dog walks toward the camera while woman sits on couch, desk and computer seen
in the background as well as a pillow, teddy bear and moggie toy on the wood floor.
1. A man in a cowboy hat check approaches a small red sports car.
2. The back and left side of a red Ferrari and two men admiring it.
3. The sporty car is admired by passer by.
4. Two men next to a red sports car in a parking lot.
5. Two men stand beside a red sports car.
</listItem>
<figureCaption confidence="0.9970055">
Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence
length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering.
</figureCaption>
<bodyText confidence="0.999221875">
tence’s visual ‘meaning.’ The last experiment De-
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to find the closest textual description
in the embedding space.
In our comparison to other methods we focus on
those models that can also compute fixed, continu-
ous vectors for sentences. In particular, we compare
to the RNN model on constituency trees of Socher
et al. (2011a), a standard recurrent neural network;
a simple bag-of-words baseline which averages the
words. All models use the word vectors provided by
Huang et al. (2012) and do not update them as dis-
cussed above. Models are trained with their corre-
sponding gradients and backpropagation techniques.
A standard recurrent model is used where the hidden
vector at word index t is computed from the hidden
vector at the previous time step and the current word
vector: ht = f(Whht−1 + Wxxt). During training,
we take the last hidden vector of the sentence chain
and propagate the error into that. It is also this vector
that is used to represent the sentence.
Other possible comparisons are to the very differ-
ent models mentioned in the related work section.
These models use a lot more task-specific engineer-
ing, such as running object detectors with bounding
boxes, attribute classifiers, scene classifiers, CRFs
for composing the sentences, etc. Another line of
work uses large sentence-image aligned resources
(Kuznetsova et al., 2012), whereas we focus on eas-
ily obtainable training data of each modality sepa-
rately and a rather small multimodal corpus.
In our experiments we split the data into 800 train-
ing, 100 development and 100 test images. Since
there are 5 sentences describing each image, we
have 4000 training sentences and 500 testing sen-
tences. The dataset has 3020 unique words, half of
which only appear once. Hence, the unsupervised,
pre-trained semantic word vector representations are
crucial. Word vectors are not fine tuned during train-
ing. Hence, the main parameters are the DT-RNN’s
Wl·, Wr· or the semantic matrices of which there are
141 and the image mapping WI. For both DT-RNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise. Word vectors and hid-
den vectors are set o length 50. Using the develop-
ment split, we found A = 0.08 and the learning rate
of AdaGrad to 0.0001. The best model uses a mar-
gin of Δ = 3.
Inspired by Socher and Fei-Fei (2010) and Ho-
dosh et al. (2013) we also compare to kernelized
Canonical Correlation Analysis (kCCA). We use the
average of word vectors for describing sentences and
the same powerful image vectors as before. We
use the code of Socher and Fei-Fei (2010). Tech-
nically, one could combine the recently introduced
deep CCA Andrew et al. (2013) and train the re-
cursive neural network architectures with the CCA
objective. We leave this to future work. With lin-
ear kernels, kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences close-by in embedding space.
All other models are trained by replacing the DT-
RNN function in Eq. 5.
</bodyText>
<subsectionHeader confidence="0.9996135">
6.1 Similarity of Sentences Describing the
Same Image
</subsectionHeader>
<bodyText confidence="0.985166666666667">
In this experiment, we first map all 500 sentences
from the test set into the multi-modal space. Then
for each sentence, we find the nearest neighbor sen-
</bodyText>
<page confidence="0.995866">
214
</page>
<table confidence="0.993181333333333">
Sentences Similarity for Image Image Search Describing Images
Model Mean Rank Model Mean Rank Model Mean Rank
Random 101.1 Random 52.1 Random 92.1
BoW 11.8 BoW 14.6 BoW 21.1
CT-RNN 15.8 CT-RNN 16.1 CT-RNN 23.9
Recurrent NN 18.5 Recurrent NN 19.2 Recurrent NN 27.1
kCCA 10.7 kCCA 15.9 kCCA 18.0
DT-RNN 11.1 DT-RNN 13.6 DT-RNN 19.2
SDT-RNN 10.5 SDT-RNN 12.5 SDT-RNN 16.9
</table>
<tableCaption confidence="0.988405">
Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate
that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the
test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the
single correct image that is being described. Right: Average rank of a correct sentence description for a query image.
</tableCaption>
<bodyText confidence="0.999835222222222">
tences in terms of inner products. We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im-
age. If all the images were very unique and the vi-
sual descriptions close-paraphrases and consistent,
we would expect a very low rank. However, usually
a handful of images are quite similar (for instance,
there are various images of airplanes flying, parking,
taxiing or waiting on the runway) and sentence de-
scriptions can vary greatly in detail and specificity
for the same image.
Table 1 (left) shows the results. We can see that
averaging the high quality word vectors already cap-
tures a lot of similarity. The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear-
lier words.
</bodyText>
<subsectionHeader confidence="0.99797">
6.2 Image Search with Query Sentences
</subsectionHeader>
<bodyText confidence="0.999934166666667">
This experiment evaluates how well we can find im-
ages that display the visual meaning of a given sen-
tence. We first map a query sentence into the vector
space and then find images in the same space using
simple inner products. As shown in Table 1 (center),
the new DT-RNN outperforms all other models.
</bodyText>
<subsectionHeader confidence="0.9994595">
6.3 Describing Images by Finding Suitable
Sentences
</subsectionHeader>
<bodyText confidence="0.999145833333333">
Lastly, we repeat the above experiments but with
roles reversed. For an image, we search for suitable
textual descriptions again simply by finding close-
by sentence vectors in the multi-modal embedding
space. Table 1 (right) shows that the DT-RNN again
outperforms related models. Fig. 2assigned to im-
</bodyText>
<table confidence="0.999778625">
Image Search Describing Images
Model mRank Model mRank
BoW 24.7 BoW 30.7
CT-RNN 22.2 CT-RNN 29.4
Recurrent NN 28.4 Recurrent NN 31.4
kCCA 13.7 kCCA 38.0
DT-RNN 13.3 DT-RNN 26.8
SDT-RNN 15.8 SDT-RNN 37.5
</table>
<tableCaption confidence="0.8549582">
Table 2: Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis-
tance in the multimodal space. Better performance is
reached for all models when trained in a max-margin loss
and using inner products as in the previous table.
</tableCaption>
<bodyText confidence="0.9996245">
ages. The average ranking of 25.3 for a correct sen-
tence description is out of 500 possible sentences. A
random assignment would give an average ranking
of 100.
</bodyText>
<subsectionHeader confidence="0.9985585">
6.4 Analysis: Squared Error Loss vs. Margin
Loss
</subsectionHeader>
<bodyText confidence="0.9997504">
We analyze the influence of the multimodal loss
function on the performance. In addition, we com-
pare using Euclidean distances instead of inner prod-
ucts. Table 2 shows that performance is worse for all
models in this setting.
</bodyText>
<subsectionHeader confidence="0.994776">
6.5 Analysis: Recall at n vs Mean Rank
</subsectionHeader>
<bodyText confidence="0.999669875">
Hodosh et al. (2013) and other related work use re-
call at n as an evaluation measure. Recall at n cap-
tures how often one of the top n closest vectors were
a correct image or sentence and gives a good intu-
ition of how a model would perform in a ranking
task that presents n such results to a user. Below, we
compare three commonly used and high performing
models: bag of words, kCCA and our SDT-RNN on
</bodyText>
<page confidence="0.963283">
215
</page>
<bodyText confidence="0.92971336">
A gray convertible sports car is parked in front of the trees.
A close-up view of the headlights of a blue old-fashioned car.
Black shiny sports car parked on concrete driveway.
Five cows grazing on a patch of grass between two roadways.
A jockey rides a brown and white horse in a dirt corral.
A young woman is riding a Bay hose in a dirt riding-ring.
A white bird pushes a miniature teal shopping cart.
A person rides a brown horse.
A motocross bike with rider flying through the air.
White propeller plane parked in middle of grassy field.
The white jet with its landing gear down flies in the blue sky.
An elderly woman catches a ride on the back of the bicycle.
A green steam train running down the tracks.
Steamy locomotive speeding thou the forest.
A steam engine comes down a train track near trees.
A double decker bus is driving by Big Ben in London.
People in an outrigger canoe sail on emerald green water
Two people sailing a small white sail boat.
behind a cliff, a boat sails away
Tourist move in on Big Ben on a typical overcast London day.
A group of people sitting around a table on a porch.
A group of four people walking past a giant mushroom.
A man and women smiling for the camera in a kitchen.
A group of men sitting around a table drinking while a man behind
stands pointing.
</bodyText>
<figureCaption confidence="0.99317">
Figure 6: Images and their sentence descriptions assigned by the DT-RNN.
</figureCaption>
<table confidence="0.984783111111111">
Image Search
Model mRank 0 R@1 p R@5 p R@10 p
BoW 14.6 15.8 42.2 60.0
kCCA 15.9 16.4 41.4 58.0
SDT-RNN 12.5 16.4 46.6 65.6
Describing Images
BoW 21.1 19.0 38.0 57.0
kCCA 18.0 21.0 47.0 61.0
SDT-RNN 16.9 23.0 45.0 63.0
</table>
<tableCaption confidence="0.999715">
Table 3: Evaluation comparison between mean rank of
</tableCaption>
<bodyText confidence="0.861422111111111">
the closest correct image or sentence (lower is better A)
with recall at different thresholds (higher is better, p).
With one exception (R@5, bottom table), the SDT-RNN
outperforms the other two models and all other models
we did not include here.
this different metric. Table 3 shows that the mea-
sures do correlate well and the SDT-RNN also per-
forms best on the multimodal ranking tasks when
evaluated with this measure.
</bodyText>
<subsectionHeader confidence="0.788968">
6.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999177">
In order to understand the main problems with the
composed sentence vectors, we analyze the sen-
tences that have the worst nearest neighbor rank be-
tween each other. We find that the main failure mode
of the SDT-RNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb. For
example, the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image:
</bodyText>
<listItem confidence="0.995725666666667">
1. A blue and yellow airplane flying straight down
while emitting white smoke
2. Airplane in dive position
</listItem>
<bodyText confidence="0.99972825">
Generally, as long as both sentences either have a
verb or do not, the SDT-RNN is more robust to dif-
ferent sentence lengths than bag of words represen-
tations.
</bodyText>
<subsectionHeader confidence="0.7128615">
6.7 Model Analysis: Semantic Composition
Matrices
</subsectionHeader>
<bodyText confidence="0.9999624375">
The best model uses composition matrices based on
semantic relationships from the dependency parser.
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms. Intuitively, these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the final composed parent vec-
tor. In decreasing order of Frobenius norm, the re-
lationship matrices are: nominal subject, possession
modifier (e.g. their), passive auxiliary, preposition
at, preposition in front of, passive auxiliary, passive
nominal subject, object of preposition, preposition
in and preposition on.
The model learns that nouns are very important as
well as their spatial prepositions and adjectives.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999521">
We introduced a new recursive neural network
model that is based on dependency trees. For eval-
uation, we use the challenging task of mapping sen-
tences and images into a common space for finding
one from the other. Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen-
tences. In comparison to related models, the DT-
RNN is more invariant and robust to surface changes
such as word order.
</bodyText>
<page confidence="0.998296">
216
</page>
<sectionHeader confidence="0.983743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999308254716981">
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013.
Deep canonical correlation analysis. In ICML, At-
lanta, Georgia.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160–167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language using
recursive neural networks. Applied Intelligence.
M. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin,
Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, and A.Y. Ng. 2012. Large scale distributed
deep networks. In NIPS.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR, 12, July.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation. In
ECCV.
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In ECCV.
Y. Feng and M. Lapata. 2013. Automatic caption gen-
eration for news images. IEEE Trans. Pattern Anal.
Mach. Intell., 35.
A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. 2013. Devise: A deep
visual-semantic embedding model. In NIPS.
C. Goller and A. K¨uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks.
E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS.
A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploit-
ing prepositions and comparative adjectives for learn-
ing visual classifiers. In ECCV.
M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, mod-
els and evaluation metrics. J. Artif. Intell. Res. (JAIR),
47:853–899.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y. Le-
Cun. 2009. What is the best multi-stage architecture
for object recognition? In ICCV.
P. Blunsom. K.M. Hermann. 2013. The role of syntax
in vector space models of compositional semantics. In
ACL.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
Imagenet classification with deep convolutional neural
networks. In NIPS.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understanding
and generating image descriptions. In CVPR.
N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Na-
yar. 2009. Attribute and simile classifiers for face ver-
ification. In ICCV.
P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Yejin Choi. 2012. Collective generation of natural
image descriptions. In ACL.
Q. V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen,
G.S. Corrado, J. Dean, and A. Y. Ng. 2012. Build-
ing high-level features using large scale unsupervised
learning. In ICML.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic
regularities in continuous spaceword representations.
In HLT-NAACL.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388–1429.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In ICML.
V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
Amazon’s Mechanical Turk. In Workshop on Creat-
ing Speech and Language Data with Amazon’s MTurk.
R. Socher and L. Fei-Fei. 2010. Connecting modalities:
Semi-supervised segmentation and annotation of im-
ages using unaligned text corpora. In CVPR.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
</reference>
<page confidence="0.976012">
217
</page>
<reference confidence="0.999764277777778">
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In EMNLP.
R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. 2013a.
Parsing With Compositional Vector Grammars. In
ACL.
R. Socher, M. Ganjoo, C. D. Manning, and A. Y. Ng.
2013b. Zero-Shot Learning Through Cross-Modal
Transfer. In NIPS.
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and
A. Y. Ng. C. D. Manning and. 2013c. Zero-shot learn-
ing through cross-modal transfer. In Proceedings of
the International Conference on Learning Representa-
tions (ICLR, Workshop Track).
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In NIPS.
A. Torralba, K. P. Murphy, and W. T. Freeman. 2010.
Using the forest to see the trees: exploiting context for
visual object detection and localization. Communica-
tions of the ACM.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010.
I2t:image parsing to text description. IEEEXplore.
</reference>
<page confidence="0.997159">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908894">
<title confidence="0.9924775">Grounded Compositional for Finding and Describing Images with Sentences</title>
<author confidence="0.99393">Richard Socher</author>
<author confidence="0.99393">Andrej Karpathy</author>
<author confidence="0.99393">Quoc V Le</author>
<author confidence="0.99393">Christopher D Manning</author>
<author confidence="0.99393">Y Andrew</author>
<affiliation confidence="0.927876">Stanford University, Computer Science Department, *Google</affiliation>
<email confidence="0.999146">qvl@google.com,manning@stanford.edu,ang@cs.stanford.edu</email>
<abstract confidence="0.999859043478261">Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT- RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>R Arora</author>
<author>K Livescu</author>
<author>J Bilmes</author>
</authors>
<title>Deep canonical correlation analysis.</title>
<date>2013</date>
<booktitle>In ICML,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="31293" citStr="Andrew et al. (2013)" startWordPosition="5326" endWordPosition="5329">ces are initialized to block identity matrices plus Gaussian noise. Word vectors and hidden vectors are set o length 50. Using the development split, we found A = 0.08 and the learning rate of AdaGrad to 0.0001. The best model uses a margin of Δ = 3. Inspired by Socher and Fei-Fei (2010) and Hodosh et al. (2013) we also compare to kernelized Canonical Correlation Analysis (kCCA). We use the average of word vectors for describing sentences and the same powerful image vectors as before. We use the code of Socher and Fei-Fei (2010). Technically, one could combine the recently introduced deep CCA Andrew et al. (2013) and train the recursive neural network architectures with the CCA objective. We leave this to future work. With linear kernels, kCCA does well for image search but is worse for sentence self similarity and describing images with sentences close-by in embedding space. All other models are trained by replacing the DTRNN function in Eq. 5. 6.1 Similarity of Sentences Describing the Same Image In this experiment, we first map all 500 sentences from the test set into the multi-modal space. Then for each sentence, we find the nearest neighbor sen214 Sentences Similarity for Image Image Search Descr</context>
</contexts>
<marker>Andrew, Arora, Livescu, Bilmes, 2013</marker>
<rawString>G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013. Deep canonical correlation analysis. In ICML, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>N de Freitas</author>
<author>D Forsyth</author>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth, D. Blei, and M. Jordan. 2003. Matching words and pictures. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="5337" citStr="Baroni and Lenci, 2010" startWordPosition="833" endWordPosition="836">an et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1465" citStr="Collobert and Weston, 2008" startWordPosition="207" endWordPosition="210">uency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Costa</author>
<author>P Frasconi</author>
<author>V Lombardo</author>
<author>G Soda</author>
</authors>
<title>Towards incremental parsing of natural language using recursive neural networks.</title>
<date>2003</date>
<journal>Applied</journal>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="2548" citStr="Costa et al., 2003" startWordPosition="386" endWordPosition="389"> words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks </context>
</contexts>
<marker>Costa, Frasconi, Lombardo, Soda, 2003</marker>
<rawString>F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003. Towards incremental parsing of natural language using recursive neural networks. Applied Intelligence. M. de Marneffe, B. MacCartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>G S Corrado</author>
<author>R Monga</author>
<author>K Chen</author>
<author>M Devin</author>
<author>Q V Le</author>
<author>M Z Mao</author>
<author>M Ranzato</author>
<author>A Senior</author>
<author>P Tucker</author>
<author>K Yang</author>
<author>A Y Ng</author>
</authors>
<title>Large scale distributed deep networks.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="26244" citStr="Dean et al., 2012" startWordPosition="4448" endWordPosition="4451">ual model and the word vector learning require a very large amount of training data and both have a huge number of parameters. Hence, to prevent overfitting, we assume their weights are fixed and only train the DT-RNN parameters WI. If larger training corpora become available in the future, training both jointly becomes feasible and would present a very promising direction. We use a modified version of AdaGrad (Duchi et al., 2011) for optimization of both WI and the DT-RNN as well as the other baselines (except kCCA). Adagrad has achieved good performance previously in neural networks models (Dean et al., 2012; Socher et al., 2013a). We modify it by resetting all squared gradient sums to 1 every 5 epochs. With both images and sentences in the same multimodal space, we can easily query the model for similar images or sentences by finding the nearest neighbors in terms of negative inner products. An alternative objective function is based on the squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This requires an alternating minimization scheme that first trains only WI, then fixes WI and trains the DT-RNN weights 0 and then repeats this several times. We find that the performance with this objective fun</context>
</contexts>
<marker>Dean, Corrado, Monga, Chen, Devin, Le, Mao, Ranzato, Senior, Tucker, Yang, Ng, 2012</marker>
<rawString>J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A.Y. Ng. 2012. Large scale distributed deep networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Deng</author>
<author>W Dong</author>
<author>R Socher</author>
<author>L-J Li</author>
<author>K Li</author>
<author>L FeiFei</author>
</authors>
<title>ImageNet: A Large-Scale Hierarchical Image Database.</title>
<date>2009</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="20971" citStr="Deng et al., 2009" startWordPosition="3522" endWordPosition="3525">allow some untying of weights depending on either how far away a constituent is from the current word or what its semantic relationship is. Now that we can compute compositional vector representations for sentences, the next section describes how we represent images. 4 Learning Image Representations with Neural Networks The image features that we use in our experiments are extracted from a deep neural network, replicated from the one described in (Le et al., 2012). The network was trained using both unlabeled data (random web images) and labeled data to classify 22,000 categories in ImageNet (Deng et al., 2009). We then used the features at the last layer, before the classifier, as the feature representation in our experiments. The dimension of the feature vector of the last layer is 4,096. The details of the model and its training procedures are as follows. The architecture of the network can be seen in Figure 4. The network takes 200x200 pixel images as inputs and has 9 layers. The layers consist of three sequences of filtering, pooling and local contrast normalization (Jarrett et al., 2009). The pooling function is L2 pooling of the previous layer (taking the square of the filtering units, summin</context>
<context position="22297" citStr="Deng et al., 2009" startWordPosition="3755" endWordPosition="3758">nputs in a small area of the lower layer, subtracts the mean and divides by the standard deviation. The network was first trained using an unsupervised objective: trying to reconstruct the input while keeping the neurons sparse. In this phase, the network was trained on 20 million images randomly sampled from the web. We resized a given image so that its short dimension has 200 pixels. We then cropped a fixed size 200x200 pixel image right at the center of the resized image. This means we may discard a fraction of the long dimension of the image. After unsupervised training, we used ImageNet (Deng et al., 2009) to adjust the features in the entire network. The ImageNet dataset has 22,000 categories and 14 million images. The number of images in each category is equal across categories. The 22,000 categories are extracted from WordNet. To speed up the supervised training of this network, we made a simple modification to the algorithm described in Le et al. (2012): adding a “bottleneck” layer in between the last layer and the classifier. to reduce the number of connections. We added one “bottleneck” layer which has 4,096 units in between the last layer of the network and the softmax layer. This newly-</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, FeiFei, 2009</marker>
<rawString>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<volume>12</volume>
<contexts>
<context position="26061" citStr="Duchi et al., 2011" startWordPosition="4417" endWordPosition="4420">e margin. The margin is found via cross validation on the dev set and usually around 1. The final objective also includes the regularization term λ/left(II0II22 + IIWIIIF). Both the visual model and the word vector learning require a very large amount of training data and both have a huge number of parameters. Hence, to prevent overfitting, we assume their weights are fixed and only train the DT-RNN parameters WI. If larger training corpora become available in the future, training both jointly becomes feasible and would present a very promising direction. We use a modified version of AdaGrad (Duchi et al., 2011) for optimization of both WI and the DT-RNN as well as the other baselines (except kCCA). Adagrad has achieved good performance previously in neural networks models (Dean et al., 2012; Socher et al., 2013a). We modify it by resetting all squared gradient sums to 1 every 5 epochs. With both images and sentences in the same multimodal space, we can easily query the model for similar images or sentences by finding the nearest neighbors in terms of negative inner products. An alternative objective function is based on the squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This requires an alternating</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duygulu</author>
<author>K Barnard</author>
<author>N de Freitas</author>
<author>D Forsyth</author>
</authors>
<title>Object recognition as machine translation.</title>
<date>2002</date>
<booktitle>In ECCV.</booktitle>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth. 2002. Object recognition as machine translation. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farhadi</author>
<author>M Hejrati</author>
<author>M A Sadeghi</author>
<author>P Young</author>
<author>C Rashtchian</author>
<author>J Hockenmaier</author>
<author>D Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="4622" citStr="Farhadi et al., 2010" startWordPosition="715" endWordPosition="718">s for sentences based on their dependency trees. We learn to map the outputs of convolutional neural networks applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence st</context>
<context position="8233" citStr="Farhadi et al. (2010)" startWordPosition="1288" endWordPosition="1291">ation to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a more automatic method to parse images. For instance, the former approach uses a single triple of objects estimated for an image to retrieve sentences from a collection written to describe similar images. It forms representations to describe 1 object, 1 action, and 1 scene. Kulkarni et al. (2011) extends their method to describe an image with multiple objects. None of these approaches have used a compositional sentence vector representation and they require specific language generation techniques and sophisticated inference methods. Since ou</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010. Every picture tells a story: Generating sentences from images. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Feng</author>
<author>M Lapata</author>
</authors>
<title>Automatic caption generation for news images.</title>
<date>2013</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>35</volume>
<contexts>
<context position="9012" citStr="Feng and Lapata (2013)" startWordPosition="1417" endWordPosition="1420">s estimated for an image to retrieve sentences from a collection written to describe similar images. It forms representations to describe 1 object, 1 action, and 1 scene. Kulkarni et al. (2011) extends their method to describe an image with multiple objects. None of these approaches have used a compositional sentence vector representation and they require specific language generation techniques and sophisticated inference methods. Since our model is based on neural networks inference is fast and simple. Kuznetsova et al. (2012) use a very large parallel corpus to connect images and sentences. Feng and Lapata (2013) use a large dataset of captioned images and experiments with both extractive (search) and abstractive (generation) models. Most related is the very recent work of Hodosh et al. (2013). They too evaluate using a ranking measure. In our experiments, we compare to kernelized Canonical Correlation Analysis which is the main technique in their experiments. 3 Dependency-Tree Recursive Neural Networks In this section we first focus on the DT-RNN model that computes compositional vector representations for phrases and sentences of variable length and syntactic type. In section 5 the resulting vectors</context>
</contexts>
<marker>Feng, Lapata, 2013</marker>
<rawString>Y. Feng and M. Lapata. 2013. Automatic caption generation for news images. IEEE Trans. Pattern Anal. Mach. Intell., 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frome</author>
<author>G Corrado</author>
<author>J Shlens</author>
<author>S Bengio</author>
<author>J Dean</author>
<author>M Ranzato</author>
<author>T Mikolov</author>
</authors>
<title>Devise: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7148" citStr="Frome et al., 2013" startWordPosition="1110" endWordPosition="1113">t corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image clas</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. 2013. Devise: A deep visual-semantic embedding model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goller</author>
<author>A K¨uchler</author>
</authors>
<title>Learning taskdependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Neural Networks.</booktitle>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>C. Goller and A. K¨uchler. 1996. Learning taskdependent distributed representations by backpropagation through structure. In Proceedings of the International Conference on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y-Z Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In IWCS.</booktitle>
<contexts>
<context position="1765" citStr="Grefenstette et al., 2013" startWordPosition="255" endWordPosition="258">an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a</context>
<context position="5727" citStr="Grefenstette et al., 2013" startWordPosition="887" endWordPosition="890">The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and ima</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and M. Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In IWCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gupta</author>
<author>L S Davis</author>
</authors>
<title>Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers.</title>
<date>2008</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="7663" citStr="Gupta and Davis, 2008" startWordPosition="1192" endWordPosition="1195">les of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), o</context>
</contexts>
<marker>Gupta, Davis, 2008</marker>
<rawString>A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hodosh</author>
<author>P Young</author>
<author>J Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: Data, models and evaluation metrics.</title>
<date>2013</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>47--853</pages>
<contexts>
<context position="9196" citStr="Hodosh et al. (2013)" startWordPosition="1446" endWordPosition="1449"> (2011) extends their method to describe an image with multiple objects. None of these approaches have used a compositional sentence vector representation and they require specific language generation techniques and sophisticated inference methods. Since our model is based on neural networks inference is fast and simple. Kuznetsova et al. (2012) use a very large parallel corpus to connect images and sentences. Feng and Lapata (2013) use a large dataset of captioned images and experiments with both extractive (search) and abstractive (generation) models. Most related is the very recent work of Hodosh et al. (2013). They too evaluate using a ranking measure. In our experiments, we compare to kernelized Canonical Correlation Analysis which is the main technique in their experiments. 3 Dependency-Tree Recursive Neural Networks In this section we first focus on the DT-RNN model that computes compositional vector representations for phrases and sentences of variable length and syntactic type. In section 5 the resulting vectors will then become multimodal features by mapping images that show what the sentence describes to the same space and learning both the image and sentence mapping jointly. The most commo</context>
<context position="30986" citStr="Hodosh et al. (2013)" startWordPosition="5275" endWordPosition="5279">pear once. Hence, the unsupervised, pre-trained semantic word vector representations are crucial. Word vectors are not fine tuned during training. Hence, the main parameters are the DT-RNN’s Wl·, Wr· or the semantic matrices of which there are 141 and the image mapping WI. For both DT-RNNs the weight matrices are initialized to block identity matrices plus Gaussian noise. Word vectors and hidden vectors are set o length 50. Using the development split, we found A = 0.08 and the learning rate of AdaGrad to 0.0001. The best model uses a margin of Δ = 3. Inspired by Socher and Fei-Fei (2010) and Hodosh et al. (2013) we also compare to kernelized Canonical Correlation Analysis (kCCA). We use the average of word vectors for describing sentences and the same powerful image vectors as before. We use the code of Socher and Fei-Fei (2010). Technically, one could combine the recently introduced deep CCA Andrew et al. (2013) and train the recursive neural network architectures with the CCA objective. We leave this to future work. With linear kernels, kCCA does well for image search but is worse for sentence self similarity and describing images with sentences close-by in embedding space. All other models are tra</context>
<context position="35189" citStr="Hodosh et al. (2013)" startWordPosition="5984" endWordPosition="5987">r performance is reached for all models when trained in a max-margin loss and using inner products as in the previous table. ages. The average ranking of 25.3 for a correct sentence description is out of 500 possible sentences. A random assignment would give an average ranking of 100. 6.4 Analysis: Squared Error Loss vs. Margin Loss We analyze the influence of the multimodal loss function on the performance. In addition, we compare using Euclidean distances instead of inner products. Table 2 shows that performance is worse for all models in this setting. 6.5 Analysis: Recall at n vs Mean Rank Hodosh et al. (2013) and other related work use recall at n as an evaluation measure. Recall at n captures how often one of the top n closest vectors were a correct image or sentence and gives a good intuition of how a model would perform in a ranking task that presents n such results to a user. Below, we compare three commonly used and high performing models: bag of words, kCCA and our SDT-RNN on 215 A gray convertible sports car is parked in front of the trees. A close-up view of the headlights of a blue old-fashioned car. Black shiny sports car parked on concrete driveway. Five cows grazing on a patch of grass</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>M. Hodosh, P. Young, and J. Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Intell. Res. (JAIR), 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Huang</author>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1485" citStr="Huang et al., 2012" startWordPosition="211" endWordPosition="214">ly focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introdu</context>
<context position="12287" citStr="Huang et al. (2012)" startWordPosition="1962" endWordPosition="1965">ce. First, we map each word to a d-dimensional vector. We initialize these word vectors with the un209 root prep poss prep pobj pobj det nsubj dobj det partmod det A man wearing a helmet jumps on his bike near a beach Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations at every word that represents that word and an arbitrary number of child nodes. The final representation is computed at the root node, here at the verb jumps. Note that more important activity and object words are higher up in this tree structure. supervised model of Huang et al. (2012) which can learn single word vector representations from both local and global contexts. The idea is to construct a neural network that outputs high scores for windows and documents that occur in a large unlabeled corpus and low scores for window-document pairs where one word is replaced by a random word. When such a network is optimized via gradient descent the derivatives backpropagate into a word embedding matrix A which stores word vectors as columns. In order to predict correct scores the vectors in the matrix capture co-occurrence statistics. We use d = 50 in all our experiments. The emb</context>
<context position="29122" citStr="Huang et al. (2012)" startWordPosition="4959" endWordPosition="4962"> to word ordering. tence’s visual ‘meaning.’ The last experiment Describing Images by Finding Suitable Sentences does the reverse search where we query the model with an image and try to find the closest textual description in the embedding space. In our comparison to other methods we focus on those models that can also compute fixed, continuous vectors for sentences. In particular, we compare to the RNN model on constituency trees of Socher et al. (2011a), a standard recurrent neural network; a simple bag-of-words baseline which averages the words. All models use the word vectors provided by Huang et al. (2012) and do not update them as discussed above. Models are trained with their corresponding gradients and backpropagation techniques. A standard recurrent model is used where the hidden vector at word index t is computed from the hidden vector at the previous time step and the current word vector: ht = f(Whht−1 + Wxxt). During training, we take the last hidden vector of the sentence chain and propagate the error into that. It is also this vector that is used to represent the sentence. Other possible comparisons are to the very different models mentioned in the related work section. These models us</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jarrett</author>
<author>K Kavukcuoglu</author>
<author>M A Ranzato</author>
<author>Y LeCun</author>
</authors>
<title>What is the best multi-stage architecture for object recognition?</title>
<date>2009</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="21463" citStr="Jarrett et al., 2009" startWordPosition="3607" endWordPosition="3610">ined using both unlabeled data (random web images) and labeled data to classify 22,000 categories in ImageNet (Deng et al., 2009). We then used the features at the last layer, before the classifier, as the feature representation in our experiments. The dimension of the feature vector of the last layer is 4,096. The details of the model and its training procedures are as follows. The architecture of the network can be seen in Figure 4. The network takes 200x200 pixel images as inputs and has 9 layers. The layers consist of three sequences of filtering, pooling and local contrast normalization (Jarrett et al., 2009). The pooling function is L2 pooling of the previous layer (taking the square of the filtering units, summing them up in a small area in the image, and taking the squareroot). The local contrast normalization takes inputs in a small area of the lower layer, subtracts the mean and divides by the standard deviation. The network was first trained using an unsupervised objective: trying to reconstruct the input while keeping the neurons sparse. In this phase, the network was trained on 20 million images randomly sampled from the web. We resized a given image so that its short dimension has 200 pix</context>
</contexts>
<marker>Jarrett, Kavukcuoglu, Ranzato, LeCun, 2009</marker>
<rawString>K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y. LeCun. 2009. What is the best multi-stage architecture for object recognition? In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Hermann</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6111" citStr="Hermann, 2013" startWordPosition="945" endWordPosition="946">ord phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to </context>
</contexts>
<marker>Hermann, 2013</marker>
<rawString>P. Blunsom. K.M. Hermann. 2013. The role of syntax in vector space models of compositional semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>G E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1909" citStr="Krizhevsky et al., 2012" startWordPosition="279" endWordPosition="282"> Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (R</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kulkarni</author>
<author>V Premraj</author>
<author>S Dhar</author>
<author>S Li</author>
<author>Y Choi</author>
<author>A C Berg</author>
<author>T L Berg</author>
</authors>
<title>Baby talk: Understanding and generating image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="8260" citStr="Kulkarni et al. (2011)" startWordPosition="1293" endWordPosition="1296">on (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a more automatic method to parse images. For instance, the former approach uses a single triple of objects estimated for an image to retrieve sentences from a collection written to describe similar images. It forms representations to describe 1 object, 1 action, and 1 scene. Kulkarni et al. (2011) extends their method to describe an image with multiple objects. None of these approaches have used a compositional sentence vector representation and they require specific language generation techniques and sophisticated inference methods. Since our model is based on neural </context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. 2011. Baby talk: Understanding and generating image descriptions. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kumar</author>
<author>A C Berg</author>
<author>P N Belhumeur</author>
</authors>
<title>Attribute and simile classifiers for face verification.</title>
<date>2009</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="7874" citStr="Kumar et al., 2009" startWordPosition="1227" endWordPosition="1230">d full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a more automatic method to parse images. For instance, the former approach uses a single triple of objects estimated for an image to retrieve sentences from a collection written to describe</context>
</contexts>
<marker>Kumar, Berg, Belhumeur, 2009</marker>
<rawString>N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Nayar. 2009. Attribute and simile classifiers for face verification. In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kuznetsova</author>
<author>V Ordonez</author>
<author>A C Berg</author>
<author>T L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4670" citStr="Kuznetsova et al., 2012" startWordPosition="723" endWordPosition="726">ees. We learn to map the outputs of convolutional neural networks applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to d</context>
<context position="8923" citStr="Kuznetsova et al. (2012)" startWordPosition="1402" endWordPosition="1405">ic method to parse images. For instance, the former approach uses a single triple of objects estimated for an image to retrieve sentences from a collection written to describe similar images. It forms representations to describe 1 object, 1 action, and 1 scene. Kulkarni et al. (2011) extends their method to describe an image with multiple objects. None of these approaches have used a compositional sentence vector representation and they require specific language generation techniques and sophisticated inference methods. Since our model is based on neural networks inference is fast and simple. Kuznetsova et al. (2012) use a very large parallel corpus to connect images and sentences. Feng and Lapata (2013) use a large dataset of captioned images and experiments with both extractive (search) and abstractive (generation) models. Most related is the very recent work of Hodosh et al. (2013). They too evaluate using a ranking measure. In our experiments, we compare to kernelized Canonical Correlation Analysis which is the main technique in their experiments. 3 Dependency-Tree Recursive Neural Networks In this section we first focus on the DT-RNN model that computes compositional vector representations for phrase</context>
<context position="29987" citStr="Kuznetsova et al., 2012" startWordPosition="5100" endWordPosition="5103">t the previous time step and the current word vector: ht = f(Whht−1 + Wxxt). During training, we take the last hidden vector of the sentence chain and propagate the error into that. It is also this vector that is used to represent the sentence. Other possible comparisons are to the very different models mentioned in the related work section. These models use a lot more task-specific engineering, such as running object detectors with bounding boxes, attribute classifiers, scene classifiers, CRFs for composing the sentences, etc. Another line of work uses large sentence-image aligned resources (Kuznetsova et al., 2012), whereas we focus on easily obtainable training data of each modality separately and a rather small multimodal corpus. In our experiments we split the data into 800 training, 100 development and 100 test images. Since there are 5 sentences describing each image, we have 4000 training sentences and 500 testing sentences. The dataset has 3020 unique words, half of which only appear once. Hence, the unsupervised, pre-trained semantic word vector representations are crucial. Word vectors are not fine tuned during training. Hence, the main parameters are the DT-RNN’s Wl·, Wr· or the semantic matri</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q V Le</author>
<author>M A Ranzato</author>
<author>R Monga</author>
<author>M Devin</author>
<author>K Chen</author>
<author>G S Corrado</author>
<author>J Dean</author>
<author>A Y Ng</author>
</authors>
<title>Building high-level features using large scale unsupervised learning.</title>
<date>2012</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1883" citStr="Le et al., 2012" startWordPosition="275" endWordPosition="278">the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Re</context>
<context position="7776" citStr="Le et al., 2012" startWordPosition="1212" endWordPosition="1215">moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a more automatic method to parse images. For instance, the former approach uses a single tr</context>
<context position="20821" citStr="Le et al., 2012" startWordPosition="3496" endWordPosition="3499">ence, by construction, the final sentence representation is more robust to less important adjectival modifiers, word order changes, etc. Fourth, we allow some untying of weights depending on either how far away a constituent is from the current word or what its semantic relationship is. Now that we can compute compositional vector representations for sentences, the next section describes how we represent images. 4 Learning Image Representations with Neural Networks The image features that we use in our experiments are extracted from a deep neural network, replicated from the one described in (Le et al., 2012). The network was trained using both unlabeled data (random web images) and labeled data to classify 22,000 categories in ImageNet (Deng et al., 2009). We then used the features at the last layer, before the classifier, as the feature representation in our experiments. The dimension of the feature vector of the last layer is 4,096. The details of the model and its training procedures are as follows. The architecture of the network can be seen in Figure 4. The network takes 200x200 pixel images as inputs and has 9 layers. The layers consist of three sequences of filtering, pooling and local con</context>
<context position="22655" citStr="Le et al. (2012)" startWordPosition="3816" endWordPosition="3819">rt dimension has 200 pixels. We then cropped a fixed size 200x200 pixel image right at the center of the resized image. This means we may discard a fraction of the long dimension of the image. After unsupervised training, we used ImageNet (Deng et al., 2009) to adjust the features in the entire network. The ImageNet dataset has 22,000 categories and 14 million images. The number of images in each category is equal across categories. The 22,000 categories are extracted from WordNet. To speed up the supervised training of this network, we made a simple modification to the algorithm described in Le et al. (2012): adding a “bottleneck” layer in between the last layer and the classifier. to reduce the number of connections. We added one “bottleneck” layer which has 4,096 units in between the last layer of the network and the softmax layer. This newly-added layer is fully connected to the previous layer and has a linear activation function. The total number of connections of this network is approximately 1.36 billion. 212 The network was trained again using the supervised objective of classifying the 22,000 classes in ImageNet. Most features in the networks are local, which allows model parallelism. Dat</context>
</contexts>
<marker>Le, Ranzato, Monga, Devin, Chen, Corrado, Dean, Ng, 2012</marker>
<rawString>Q. V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A. Y. Ng. 2012. Building high-level features using large scale unsupervised learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>W Yih</author>
<author>G Zweig</author>
</authors>
<title>Linguistic regularities in continuous spaceword representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1508" citStr="Mikolov et al., 2013" startWordPosition="215" endWordPosition="218">on and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic regularities in continuous spaceword representations. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1695" citStr="Mitchell and Lapata, 2010" startWordPosition="243" endWordPosition="246">, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first l</context>
<context position="5488" citStr="Mitchell and Lapata, 2010" startWordPosition="853" endWordPosition="856">d Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector compositi</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ngiam</author>
<author>A Khosla</author>
<author>M Kim</author>
<author>J Nam</author>
<author>H Lee</author>
<author>A Y Ng</author>
</authors>
<title>Multimodal deep learning.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="6244" citStr="Ngiam et al., 2011" startWordPosition="963" endWordPosition="966"> of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embed</context>
</contexts>
<marker>Ngiam, Khosla, Kim, Nam, Lee, Ng, 2011</marker>
<rawString>J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y. Ng. 2011. Multimodal deep learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ordonez</author>
<author>G Kulkarni</author>
<author>T L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4644" citStr="Ordonez et al., 2011" startWordPosition="719" endWordPosition="722">on their dependency trees. We learn to map the outputs of convolutional neural networks applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<contexts>
<context position="2528" citStr="Pollack, 1990" startWordPosition="384" endWordPosition="385">ver, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurr</context>
<context position="10235" citStr="Pollack, 1990" startWordPosition="1613" endWordPosition="1614"> become multimodal features by mapping images that show what the sentence describes to the same space and learning both the image and sentence mapping jointly. The most common way of building representations for longer phrases from single word vectors is to simply linearly average the word vectors. While this bag-of-words approach can yield reasonable performance in some tasks, it gives all the words the same weight and cannot distinguish important differences in simple visual descriptions such as The bike crashed into the standing car. vs. The car crashed into the standing bike.. RNN models (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2011b; Socher et al., 2011a) provided a novel way of combining word vectors for longer phrases that moved beyond simple averaging. They combine vectors with an RNN in binary constituency trees which have potentially many hidden layers. While the induced vector representations work very well on many tasks, they also inevitably capture a lot of syntactic structure of the sentence. However, the task of finding images from sentence descriptions requires us to be more invariant to syntactic differences. One such example are activepassive constructions whi</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rashtchian</author>
<author>P Young</author>
<author>M Hodosh</author>
<author>J Hockenmaier</author>
</authors>
<title>Collecting image annotations using Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data with Amazon’s MTurk.</booktitle>
<contexts>
<context position="4730" citStr="Rashtchian et al., 2010" startWordPosition="734" endWordPosition="737">works applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci</context>
<context position="27117" citStr="Rashtchian et al. (2010)" startWordPosition="4596" endWordPosition="4599">bors in terms of negative inner products. An alternative objective function is based on the squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This requires an alternating minimization scheme that first trains only WI, then fixes WI and trains the DT-RNN weights 0 and then repeats this several times. We find that the performance with this objective function (paired with finding similar images using Euclidean distances) is worse for all models than the margin loss of Eq. 5. In addition kCCA also performs much better using inner products in the multimodal space. 6 Experiments We use the dataset of Rashtchian et al. (2010) which consists of 1000 images, each with 5 sentences. See Fig. 5 for examples. We evaluate and compare the DT-RNN in three different experiments. First, we analyze how well the sentence vectors capture similarity in visual meaning. Then we analyze Image Search with Query Sentences: to query each model with a sentence in order to find an image showing that sen213 1. A woman and her dog watch the cameraman in their living with wooden floors. 2. A woman sitting on the couch while a black faced dog runs across the floor. 3. A woman wearing a backpack sits on a couch while a small dog runs on the </context>
<context position="28390" citStr="Rashtchian et al., 2010" startWordPosition="4840" endWordPosition="4843"> a sofa while a small Jack Russell walks towards the camera. 5. White and black small dog walks toward the camera while woman sits on couch, desk and computer seen in the background as well as a pillow, teddy bear and moggie toy on the wood floor. 1. A man in a cowboy hat check approaches a small red sports car. 2. The back and left side of a red Ferrari and two men admiring it. 3. The sporty car is admired by passer by. 4. Two men next to a red sports car in a parking lot. 5. Two men stand beside a red sports car. Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering. tence’s visual ‘meaning.’ The last experiment Describing Images by Finding Suitable Sentences does the reverse search where we query the model with an image and try to find the closest textual description in the embedding space. In our comparison to other methods we focus on those models that can also compute fixed, continuous vectors for sentences. In particular, we compare to the RNN model on constituency trees of Socher et al. (2011a), a standard recurrent neur</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier. 2010. Collecting image annotations using Amazon’s Mechanical Turk. In Workshop on Creating Speech and Language Data with Amazon’s MTurk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>L Fei-Fei</author>
</authors>
<title>Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora.</title>
<date>2010</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="6305" citStr="Socher and Fei-Fei, 2010" startWordPosition="975" endWordPosition="978">ions being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2</context>
<context position="30961" citStr="Socher and Fei-Fei (2010)" startWordPosition="5270" endWordPosition="5273">e words, half of which only appear once. Hence, the unsupervised, pre-trained semantic word vector representations are crucial. Word vectors are not fine tuned during training. Hence, the main parameters are the DT-RNN’s Wl·, Wr· or the semantic matrices of which there are 141 and the image mapping WI. For both DT-RNNs the weight matrices are initialized to block identity matrices plus Gaussian noise. Word vectors and hidden vectors are set o length 50. Using the development split, we found A = 0.08 and the learning rate of AdaGrad to 0.0001. The best model uses a margin of Δ = 3. Inspired by Socher and Fei-Fei (2010) and Hodosh et al. (2013) we also compare to kernelized Canonical Correlation Analysis (kCCA). We use the average of word vectors for describing sentences and the same powerful image vectors as before. We use the code of Socher and Fei-Fei (2010). Technically, one could combine the recently introduced deep CCA Andrew et al. (2013) and train the recursive neural network architectures with the CCA objective. We leave this to future work. With linear kernels, kCCA does well for image search but is worse for sentence self similarity and describing images with sentences close-by in embedding space.</context>
</contexts>
<marker>Socher, Fei-Fei, 2010</marker>
<rawString>R. Socher and L. Fei-Fei. 2010. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="1716" citStr="Socher et al., 2010" startWordPosition="247" endWordPosition="250">of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respe</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS.</title>
<date>2011</date>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="390" endWordPosition="393">mages are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally </context>
<context position="5918" citStr="Socher et al., 2011" startWordPosition="913" endWordPosition="916"> Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervise</context>
<context position="10283" citStr="Socher et al., 2011" startWordPosition="1619" endWordPosition="1622">ges that show what the sentence describes to the same space and learning both the image and sentence mapping jointly. The most common way of building representations for longer phrases from single word vectors is to simply linearly average the word vectors. While this bag-of-words approach can yield reasonable performance in some tasks, it gives all the words the same weight and cannot distinguish important differences in simple visual descriptions such as The bike crashed into the standing car. vs. The car crashed into the standing bike.. RNN models (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2011b; Socher et al., 2011a) provided a novel way of combining word vectors for longer phrases that moved beyond simple averaging. They combine vectors with an RNN in binary constituency trees which have potentially many hidden layers. While the induced vector representations work very well on many tasks, they also inevitably capture a lot of syntactic structure of the sentence. However, the task of finding images from sentence descriptions requires us to be more invariant to syntactic differences. One such example are activepassive constructions which can collapse words such as “by” in some forma</context>
<context position="18311" citStr="Socher et al. (2011" startWordPosition="3072" endWordPosition="3075">emantic DT-RNN (SDT-RNN) is the same as the one above except that the matrices Wpos(i,j) are replaced with matrices based on the dependency relationship. There are a total of 141 unique such relationships in the dataset. However, most are very rare. For examples of semantic relationships, see Fig. 2 and the model analysis section 6.7. This forward propagation can be used for computing compositional vectors and in Sec. 5 we will explain the objective function in which these are trained. 3.4 Comparison to Previous RNN Models The DT-RNN has several important differences to previous RNN models of Socher et al. (2011a) and (Socher et al., 2011b; Socher et al., 2011c). These constituency tree RNNs (CT-RNNs) use the following composition function to compute a hidden parent vector h from exactly two child vectors (c1, c2) in a binary tree: h = fc2 W c1 ( Iwhere W ∈ 1), Rd×2d is the main parameter to learn. This can be rewritten to show the similarity to the DT-RNN as h = f(Wl1c1 + Wr1c2). However, there are several important differences. Note first that in previous RNN models the parent vectors were of the same dimensionality to be recursively compatible and be used as input to the next composition. In contr</context>
<context position="28961" citStr="Socher et al. (2011" startWordPosition="4934" endWordPosition="4937"> sentence descriptions (Rashtchian et al., 2010). Sentence length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering. tence’s visual ‘meaning.’ The last experiment Describing Images by Finding Suitable Sentences does the reverse search where we query the model with an image and try to find the closest textual description in the embedding space. In our comparison to other methods we focus on those models that can also compute fixed, continuous vectors for sentences. In particular, we compare to the RNN model on constituency trees of Socher et al. (2011a), a standard recurrent neural network; a simple bag-of-words baseline which averages the words. All models use the word vectors provided by Huang et al. (2012) and do not update them as discussed above. Models are trained with their corresponding gradients and backpropagation techniques. A standard recurrent model is used where the hidden vector at word index t is computed from the hidden vector at the previous time step and the current word vector: ht = f(Whht−1 + Wxxt). During training, we take the last hidden vector of the sentence chain and propagate the error into that. It is also this </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. Manning. 2011a. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="390" endWordPosition="393">mages are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally </context>
<context position="5918" citStr="Socher et al., 2011" startWordPosition="913" endWordPosition="916"> Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervise</context>
<context position="10283" citStr="Socher et al., 2011" startWordPosition="1619" endWordPosition="1622">ges that show what the sentence describes to the same space and learning both the image and sentence mapping jointly. The most common way of building representations for longer phrases from single word vectors is to simply linearly average the word vectors. While this bag-of-words approach can yield reasonable performance in some tasks, it gives all the words the same weight and cannot distinguish important differences in simple visual descriptions such as The bike crashed into the standing car. vs. The car crashed into the standing bike.. RNN models (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2011b; Socher et al., 2011a) provided a novel way of combining word vectors for longer phrases that moved beyond simple averaging. They combine vectors with an RNN in binary constituency trees which have potentially many hidden layers. While the induced vector representations work very well on many tasks, they also inevitably capture a lot of syntactic structure of the sentence. However, the task of finding images from sentence descriptions requires us to be more invariant to syntactic differences. One such example are activepassive constructions which can collapse words such as “by” in some forma</context>
<context position="18311" citStr="Socher et al. (2011" startWordPosition="3072" endWordPosition="3075">emantic DT-RNN (SDT-RNN) is the same as the one above except that the matrices Wpos(i,j) are replaced with matrices based on the dependency relationship. There are a total of 141 unique such relationships in the dataset. However, most are very rare. For examples of semantic relationships, see Fig. 2 and the model analysis section 6.7. This forward propagation can be used for computing compositional vectors and in Sec. 5 we will explain the objective function in which these are trained. 3.4 Comparison to Previous RNN Models The DT-RNN has several important differences to previous RNN models of Socher et al. (2011a) and (Socher et al., 2011b; Socher et al., 2011c). These constituency tree RNNs (CT-RNNs) use the following composition function to compute a hidden parent vector h from exactly two child vectors (c1, c2) in a binary tree: h = fc2 W c1 ( Iwhere W ∈ 1), Rd×2d is the main parameter to learn. This can be rewritten to show the similarity to the DT-RNN as h = f(Wl1c1 + Wr1c2). However, there are several important differences. Note first that in previous RNN models the parent vectors were of the same dimensionality to be recursively compatible and be used as input to the next composition. In contr</context>
<context position="28961" citStr="Socher et al. (2011" startWordPosition="4934" endWordPosition="4937"> sentence descriptions (Rashtchian et al., 2010). Sentence length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering. tence’s visual ‘meaning.’ The last experiment Describing Images by Finding Suitable Sentences does the reverse search where we query the model with an image and try to find the closest textual description in the embedding space. In our comparison to other methods we focus on those models that can also compute fixed, continuous vectors for sentences. In particular, we compare to the RNN model on constituency trees of Socher et al. (2011a), a standard recurrent neural network; a simple bag-of-words baseline which averages the words. All models use the word vectors provided by Huang et al. (2012) and do not update them as discussed above. Models are trained with their corresponding gradients and backpropagation techniques. A standard recurrent model is used where the hidden vector at word index t is computed from the hidden vector at the previous time step and the current word vector: ht = f(Whht−1 + Wxxt). During training, we take the last hidden vector of the sentence chain and propagate the error into that. It is also this </context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2569" citStr="Socher et al., 2011" startWordPosition="390" endWordPosition="393">mages are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally </context>
<context position="5918" citStr="Socher et al., 2011" startWordPosition="913" endWordPosition="916"> Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervise</context>
<context position="10283" citStr="Socher et al., 2011" startWordPosition="1619" endWordPosition="1622">ges that show what the sentence describes to the same space and learning both the image and sentence mapping jointly. The most common way of building representations for longer phrases from single word vectors is to simply linearly average the word vectors. While this bag-of-words approach can yield reasonable performance in some tasks, it gives all the words the same weight and cannot distinguish important differences in simple visual descriptions such as The bike crashed into the standing car. vs. The car crashed into the standing bike.. RNN models (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2011b; Socher et al., 2011a) provided a novel way of combining word vectors for longer phrases that moved beyond simple averaging. They combine vectors with an RNN in binary constituency trees which have potentially many hidden layers. While the induced vector representations work very well on many tasks, they also inevitably capture a lot of syntactic structure of the sentence. However, the task of finding images from sentence descriptions requires us to be more invariant to syntactic differences. One such example are activepassive constructions which can collapse words such as “by” in some forma</context>
<context position="18311" citStr="Socher et al. (2011" startWordPosition="3072" endWordPosition="3075">emantic DT-RNN (SDT-RNN) is the same as the one above except that the matrices Wpos(i,j) are replaced with matrices based on the dependency relationship. There are a total of 141 unique such relationships in the dataset. However, most are very rare. For examples of semantic relationships, see Fig. 2 and the model analysis section 6.7. This forward propagation can be used for computing compositional vectors and in Sec. 5 we will explain the objective function in which these are trained. 3.4 Comparison to Previous RNN Models The DT-RNN has several important differences to previous RNN models of Socher et al. (2011a) and (Socher et al., 2011b; Socher et al., 2011c). These constituency tree RNNs (CT-RNNs) use the following composition function to compute a hidden parent vector h from exactly two child vectors (c1, c2) in a binary tree: h = fc2 W c1 ( Iwhere W ∈ 1), Rd×2d is the main parameter to learn. This can be rewritten to show the similarity to the DT-RNN as h = f(Wl1c1 + Wr1c2). However, there are several important differences. Note first that in previous RNN models the parent vectors were of the same dimensionality to be recursively compatible and be used as input to the next composition. In contr</context>
<context position="28961" citStr="Socher et al. (2011" startWordPosition="4934" endWordPosition="4937"> sentence descriptions (Rashtchian et al., 2010). Sentence length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering. tence’s visual ‘meaning.’ The last experiment Describing Images by Finding Suitable Sentences does the reverse search where we query the model with an image and try to find the closest textual description in the embedding space. In our comparison to other methods we focus on those models that can also compute fixed, continuous vectors for sentences. In particular, we compare to the RNN model on constituency trees of Socher et al. (2011a), a standard recurrent neural network; a simple bag-of-words baseline which averages the words. All models use the word vectors provided by Huang et al. (2012) and do not update them as discussed above. Models are trained with their corresponding gradients and backpropagation techniques. A standard recurrent model is used where the hidden vector at word index t is computed from the hidden vector at the previous time step and the current word vector: ht = f(Whht−1 + Wxxt). During training, we take the last hidden vector of the sentence chain and propagate the error into that. It is also this </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011c. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1737" citStr="Socher et al., 2012" startWordPosition="251" endWordPosition="254">the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modaliti</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Bauer</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6908" citStr="Socher et al., 2013" startWordPosition="1070" endWordPosition="1073">d Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single wo</context>
<context position="26265" citStr="Socher et al., 2013" startWordPosition="4452" endWordPosition="4455">ord vector learning require a very large amount of training data and both have a huge number of parameters. Hence, to prevent overfitting, we assume their weights are fixed and only train the DT-RNN parameters WI. If larger training corpora become available in the future, training both jointly becomes feasible and would present a very promising direction. We use a modified version of AdaGrad (Duchi et al., 2011) for optimization of both WI and the DT-RNN as well as the other baselines (except kCCA). Adagrad has achieved good performance previously in neural networks models (Dean et al., 2012; Socher et al., 2013a). We modify it by resetting all squared gradient sums to 1 every 5 epochs. With both images and sentences in the same multimodal space, we can easily query the model for similar images or sentences by finding the nearest neighbors in terms of negative inner products. An alternative objective function is based on the squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This requires an alternating minimization scheme that first trains only WI, then fixes WI and trains the DT-RNN weights 0 and then repeats this several times. We find that the performance with this objective function (paired with fi</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. 2013a. Parsing With Compositional Vector Grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>M Ganjoo</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Zero-Shot Learning Through Cross-Modal Transfer.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6908" citStr="Socher et al., 2013" startWordPosition="1070" endWordPosition="1073">d Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single wo</context>
<context position="26265" citStr="Socher et al., 2013" startWordPosition="4452" endWordPosition="4455">ord vector learning require a very large amount of training data and both have a huge number of parameters. Hence, to prevent overfitting, we assume their weights are fixed and only train the DT-RNN parameters WI. If larger training corpora become available in the future, training both jointly becomes feasible and would present a very promising direction. We use a modified version of AdaGrad (Duchi et al., 2011) for optimization of both WI and the DT-RNN as well as the other baselines (except kCCA). Adagrad has achieved good performance previously in neural networks models (Dean et al., 2012; Socher et al., 2013a). We modify it by resetting all squared gradient sums to 1 every 5 epochs. With both images and sentences in the same multimodal space, we can easily query the model for similar images or sentences by finding the nearest neighbors in terms of negative inner products. An alternative objective function is based on the squared loss J(WI, 0) = E(i,j)EP IIvi − yjII22. This requires an alternating minimization scheme that first trains only WI, then fixes WI and trains the DT-RNN weights 0 and then repeats this several times. We find that the performance with this objective function (paired with fi</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>R. Socher, M. Ganjoo, C. D. Manning, and A. Y. Ng. 2013b. Zero-Shot Learning Through Cross-Modal Transfer. In NIPS.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C D</author>
</authors>
<title>Manning and. 2013c. Zero-shot learning through cross-modal transfer.</title>
<booktitle>In Proceedings of the International Conference on Learning Representations (ICLR, Workshop Track).</booktitle>
<marker>D, </marker>
<rawString>R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and A. Y. Ng. C. D. Manning and. 2013c. Zero-shot learning through cross-modal transfer. In Proceedings of the International Conference on Learning Representations (ICLR, Workshop Track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Srivastava</author>
<author>R Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep boltzmann machines.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6649" citStr="Srivastava and Salakhutdinov (2012)" startWordPosition="1029" endWordPosition="1032">r alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phra</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>N. Srivastava and R. Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>K P Murphy</author>
<author>W T Freeman</author>
</authors>
<title>Using the forest to see the trees: exploiting context for visual object detection and localization.</title>
<date>2010</date>
<journal>Communications of the ACM.</journal>
<contexts>
<context position="7687" citStr="Torralba et al., 2010" startWordPosition="1196" endWordPosition="1199">ted work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. 208 Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a </context>
</contexts>
<marker>Torralba, Murphy, Freeman, 2010</marker>
<rawString>A. Torralba, K. P. Murphy, and W. T. Freeman. 2010. Using the forest to see the trees: exploiting context for visual object detection and localization. Communications of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1366" citStr="Turney and Pantel, 2010" startWordPosition="192" endWordPosition="195"> images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often </context>
<context position="5312" citStr="Turney and Pantel, 2010" startWordPosition="829" endWordPosition="832">t introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al.</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yao</author>
<author>X Yang</author>
<author>L Lin</author>
<author>M W Lee</author>
<author>S-C Zhu</author>
</authors>
<title>I2t:image parsing to text description.</title>
<date>2010</date>
<tech>IEEEXplore.</tech>
<contexts>
<context position="8083" citStr="Yao et al. (2010)" startWordPosition="1264" endWordPosition="1267">in this area includes generating single words or fixed phrases from images (Duygulu et al., 2002; Barnard et al., 2003) or using contextual information to improve recognition (Gupta and Davis, 2008; Torralba et al., 2010). Apart from a large body of work on single object image classification (Le et al., 2012), there is also work on attribute classification and other mid-level elements (Kumar et al., 2009), some of which we hope to capture with our approach as well. Our work is close in spirit with recent work in describing images with more detailed, longer textual descriptions. In particular, Yao et al. (2010) describe images using hierarchical knowledge and humans in the loop. In contrast, our work does not require human interactions. Farhadi et al. (2010) and Kulkarni et al. (2011), on the other hand, use a more automatic method to parse images. For instance, the former approach uses a single triple of objects estimated for an image to retrieve sentences from a collection written to describe similar images. It forms representations to describe 1 object, 1 action, and 1 scene. Kulkarni et al. (2011) extends their method to describe an image with multiple objects. None of these approaches have used</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010. I2t:image parsing to text description. IEEEXplore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
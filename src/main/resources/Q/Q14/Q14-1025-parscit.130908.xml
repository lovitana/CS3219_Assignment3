<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.750653">
The Benefits of a Model of Annotation
</title>
<author confidence="0.892973">
Rebecca J. Passonneau
</author>
<affiliation confidence="0.985584">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.929274">
New York, NY USA
</address>
<email confidence="0.99922">
becky@ccls.columbia.edu
</email>
<author confidence="0.985412">
Bob Carpenter
</author>
<affiliation confidence="0.9960875">
Department of Statistics
Columbia University
</affiliation>
<address confidence="0.905704">
New York, NY USA
</address>
<email confidence="0.929716">
carp@alias-i.com
</email>
<sectionHeader confidence="0.995796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998699833333333">
Standard agreement measures for interannota-
tor reliability are neither necessary nor suffi-
cient to ensure a high quality corpus. In a case
study of word sense annotation, conventional
methods for evaluating labels from trained an-
notators are contrasted with a probabilistic an-
notation model applied to crowdsourced data.
The annotation model provides far more in-
formation, including a certainty measure for
each gold standard label; the crowdsourced
data was collected at less than half the cost of
the conventional approach.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787277777778">
The quality of annotated data for computational lin-
guistics is generally assumed to be good enough
if a few annotators can be shown to be consistent
with one another. Standard practice relies on met-
rics that measure consistency, either in an absolute
way, or in a chance-adjusted fashion. Such mea-
sures, however, merely report how often annota-
tors agree, with no direct measure of corpus qual-
ity, nor of the quality of individual items. We ar-
gue that high chance-adjusted interannotator agree-
ment is neither necessary nor sufficient to ensure
high quality gold-standard labels. We contrast the
use of agreement metrics with the use of probabilis-
tic models to draw inferences about annotated data
where the items have been labeled by many anno-
tators. A probabilistic model to fit many annota-
tors’ observed labels produces much more informa-
tion about the annotated corpus. In particular, there
will be a confidence estimate for each ground truth
label.
Probabilistic models of agreement and gold-
standard inference have been used in psychomet-
rics and marketing since the 1950s (e.g., IRT mod-
els or Bradley-Terry models) and in epidemiology
since the 1970s (e.g., diagnostic disease prevalence
models). More recently, crowdsourcing has moti-
vated their application to data annotation for ma-
chine learning. The model we apply here (Dawid
and Skene, 1979) assumes that annotators differ
from one another in their accuracy at identifying the
true label values, and that these true values occur at
certain rates (their prevalence).
To contrast the two approaches to creation of an
annotated corpus, we present a case study of word
sense annotation. The items that were annotated are
occurrences of words in their sentence contexts, and
each label is a WordNet sense (Miller, 1995). Each
item has sense labels from up to twenty-five different
annotators, collected through crowdsourcing. Ap-
plication of an annotation model does not require
this many labels per item, and crowdsourced annota-
tion data does not require a probabilistic model. The
case study, however, shows how the two benefit each
other.
MASC (Manually Annotated Sub-Corpus of the
Open American National Corpus) contains a sub-
sidiary word sense sentence corpus that consists of
approximately one thousand sentences per word for
116 words. Word senses were annotated in their sen-
tence contexts using WordNet sense labels. Chance-
adjusted agreement levels ranged from very high to
chance levels, with similar variation for pairwise
agreement (Passonneau et al., 2012a). As a result,
the annotations for certain words appear to be low
</bodyText>
<page confidence="0.991473">
311
</page>
<bodyText confidence="0.945143541666667">
Transactions of the Association for Computational Linguistics, 2 (2014) 311–326. Action Editor: Chris Callison-Burch.
Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
quality.1 Our case study shows how we created
a more reliable word sense corpus for a randomly
selected subset of 45 of the same words, through
crowdsourcing and application of the Dawid and
Skene model. The model yields a certainty measure
for each labeled instance. For most instances, the
certainty of the estimated true labels is high, even
on words where pairwise and chance-adjusted agree-
ment of trained annotators were both low.
The paper first summarizes the limitations of
agreement metrics, then presents the Dawid and
Skene model. The next two sections present a case
study of the crowdsourced data, and the annotation
results. While many of the MASC words had low
agreement from trained annotators on the small pro-
portion of the data where agreement was assessed,
the same words have many instances with highly
confident labels estimated from the crowdsourced
annotations. In the discussion section, we compare
the model-based labels to the labels from the trained
annotators. The final sections present related work
and our conclusions.
</bodyText>
<sectionHeader confidence="0.956856" genericHeader="method">
2 Agreement Metrics versus a Model
</sectionHeader>
<bodyText confidence="0.999993894736842">
A high-confidence ground truth label for each an-
notated instance is the ultimate goal of annotation,
but can often be impractical or infeasible to achieve.
On the grounds that more knowledge is always bet-
ter, we argue that it is desirable to provide a confi-
dence measure for each estimated label. This section
first presents the case that the conventional steps to
compute agreement provide at best an indirect mea-
sure of confidence on labels. We then present the
Dawid and Skene model (1979), which estimates a
probability of each label value on every instance. To
motivate its application to the crowdsourced sense
labels, we work through an example to show how
true labels are inferred, and to illustrate that infor-
mation about the true label is derived from both ac-
curate and inaccurate annotators. With many anno-
tators to compare, the value of gathering a label can
be quantified using information gain and mutual in-
formation, as illustrated in Section 2.2.2.
</bodyText>
<footnote confidence="0.996588666666667">
1One potential use for the words with low agreement is to
investigate whether features of the WordNet definitions, or sen-
tence contexts, or both, correlate with low agreement.
</footnote>
<note confidence="0.7945585">
2.1 Pairwise and Chance-Adjusted Agreement
Measures
</note>
<bodyText confidence="0.999838976190476">
Current best practice for creating annotation stan-
dards involves iteration over four steps: 1) design
or redesign the annotation task, 2) write or revise
guidelines to instruct annotators how to carry out
the task, possibly with some training, 3) have two or
more annotators work independently to annotate a
sample of data, 4) measure the interannotator agree-
ment on the data sample. Once the desired agree-
ment has been obtained, the final step is to create a
gold standard dataset where each item is annotated
by a single annotator. How much chance-adjusted
agreement is sufficient has been much debated (Art-
stein and Poesio, 2008; di Eugenio and Glass, 2004;
di Eugenio, 2000; Bruce and Wiebe, 1998). Surpris-
ingly, little attention has been devoted to the ques-
tion of whether the agreement subset is a represen-
tative sample of the corpus. Without such an assur-
ance, there is little justification to take interannota-
tor agreement as a quality measure of the corpus as a
whole. Given the influence that a gold standard cor-
pus can have on progress in our field, it is not clear
that agreement measures on a corpus subset provide
a sufficient guarantee of corpus quality.
While it is taken for granted that some annotators
perform better than others,2 agreement metrics do
not differentiate annotators. Since there are many
ways to be inaccurate, and only one way to be accu-
rate, it is assumed that if annotators have high pair-
wise or chance-adjusted agreement, then the anno-
tation must be accurate. This is not necessarily a
correct inference, as we show below. If two annota-
tors do not agree well, this method does not identify
whether one annotator is more accurate. More im-
portantly, no information is gained about the quality
of the ground truth labels.
To assess the limitations of agreement metrics,
consider how they are computed and what they mea-
sure. Let i E 1:I represent the items, j E 1:J the
annotators, k E 1:K the label classes in a categorical
labeling scheme (e.g., word senses), and yi,j E 1:K
the observed labels from annotator j for item i. As-
sume every annotator labels every item exactly once
</bodyText>
<footnote confidence="0.993150333333333">
2Some researchers believe that all that is needed is one trust-
worthy annotator, which begs the question of how trust is as-
sessed.
</footnote>
<page confidence="0.998467">
312
</page>
<bodyText confidence="0.9949914">
(we later relax this constraint).
Agreement: Pairwise agreement Am,n between
two annotators m, n E 1:J is defined as the pro-
portion of items i E 1:I for which the annotators
supplied the same label,
</bodyText>
<equation confidence="0.9612845">
Am,n = 1EI i=1 I(yi,m = yi,n),
I
</equation>
<bodyText confidence="0.9999574">
where ff(s) = 1 if s is true and 0 otherwise. In other
words, Am,n is the maximum likelihood estimate of
chance of agreement in a binomial model.
Pairwise agreement can be extended to the full set
of annotators by averaging over all (J ) pairs:
</bodyText>
<equation confidence="0.987405333333333">
2
J2 E−1 �nJ=
+1 AA_ 1mJ=1 m,n.()
</equation>
<bodyText confidence="0.998329545454545">
In sum, A is the proportion of all pairs of items that
annotators agreed on. It does not take into account
the proportion of each label from 1:K in the data.
Chance-Adjusted Agreement: Agreement coeffi-
cients measure the proportion of observed agree-
ments that are above the proportion expected by
chance. Given an estimate Am,n of the probabil-
ity that two annotators m, n E 1:J will agree on
a label and an estimate of the probability Cm,n that
they will agree by chance, chance-adjusted agree-
ment ZAm,n E [−1, 1] is defined by
</bodyText>
<equation confidence="0.969184">
ZAm,n = Am,n−Cm,n
1−Cm,n .
</equation>
<bodyText confidence="0.999931454545455">
Chance agreement takes into account the prevalence
of the individual labels in 1:K. Specifically, it is de-
fined to be the probability that a pair of labels drawn
at random for two annotators will agree. There are
two common ways to define this draw. Cohen’s
κ statistic (Cohen, 1960) assumes each annotator
draws uniformly at random from her set of labels.
Letting ψj,k = I EIi=1 I(yi,j = k) be the propor-
tion of the label k in annotator j’s labels, this notion
of chance agreement for a pair of annotators m, n is
estimated as the product of their proportions ψ:
</bodyText>
<equation confidence="0.868491333333333">
EK
Cm,n /&apos; /&apos;
— �k=1 4&apos;m,k X 4&apos;n,k.
</equation>
<bodyText confidence="0.992399518518519">
Krippendorff’s α, another chance-adjusted metric in
wide use, assumes each annotator draws uniformly
at random from the pooled set of labels from all an-
notators (Krippendorff, 1980). Letting φk be the
proportion of label k in the entire set of labels, this
alternative estimate, C0m,n = EKk=1 φ2k, does not de-
pend on the identity of the annotators m and n.
Agreement coefficients suffer from multiple
shortcomings. (1) They are intrinsically pairwise,
although one can compare to a voted consensus or
average over multiple pairwise agreements. (2) In
agreement-based analyses, two wrongs make a right
in the sense that if two annotators both make the
same mistake, they agree. If annotators are 80%
accurate on a binary task, then chance agreement
on the wrong category occurs at a 4% rate. (3)
Chance-adjusted agreement reduces to simple agree-
ment as chance agreement approaches zero. When
chance agreement is high, even high-accuracy an-
notators can have low chance-adjusted agreement,
as when the data is skewed towards a few values,
a typical case for NLP tasks. Feinstein and Ci-
cchetti (1990) referred to this as the paradox of
κ (see section 6). For example, in a binary task
with 95% prevalence of one category, two 90%
accurate annotators would have negative chance-
adjusted agreements of 0.9−(.952+.052) = −.053.
</bodyText>
<equation confidence="0.67535">
1−(.952+.052)
</equation>
<bodyText confidence="0.99980328">
Thus high chance-adjusted interannotator agreement
is not a necessary condition for a high-quality cor-
pus. An alternative metric discussed in Section 6
addresses skewed prevalence of label values, but has
not been adopted in the NLP community (Gwet,
2008). (4) Interannotator agreement statistics im-
plicitly assume annotators are unbiased; if they are
biased in the same direction, e.g., the most preva-
lent category, then agreement is an overestimate of
their accuracy. In the extreme case, in a binary la-
beling task, two adversarial annotators who always
provide the wrong answer have a chance-adjusted
agreement of 100%. (5) Item-level effects such as
difficulty can inflate levels of agreement-in-error.
For example, in a named-entity corpus one of the
co-authors helped collect for MUC, hard-to-identify
names have correlated false negatives among an-
notators, leading to higher agreement-in-error than
would otherwise be expected. (6) Interannotator
agreement statistics are rarely computed with con-
fidence intervals, which can be quite wide even un-
der optimistic assumptions of no annotator bias or
item-level effects. Given a sample of 100 anno-
tations, if the true gold standard categories were
known (as opposed to being themselves estimated as
</bodyText>
<page confidence="0.999006">
313
</page>
<bodyText confidence="0.999742266666667">
in our setup here), an annotator getting 80 out of 100
items correct would produce a 95% interval for ac-
curacy of roughly (74%,86%).3 Agreement statis-
tics have even wider error bounds. This introduces
enough uncertainty to span the rather arbitrary deci-
sion boundaries for acceptability employed for inter-
annotator agreement statistics. Note that bootstrap-
ping is a reliable method to compute confidence in-
tervals (Efron and Tibshirani, 1986). Briefly, given
a sample of size N, a large number of samples of
size N are drawn randomly with replacement from
the original sample, the statistic of interest is com-
puted for each random draw, and the mean f 1.96
standard deviations gives the estimated value and its
approximate 95% confidence interval.
</bodyText>
<subsectionHeader confidence="0.997233">
2.2 A Probabilistic Annotation Model
</subsectionHeader>
<bodyText confidence="0.999891136363637">
A probabilistic model provides a recipe to randomly
“generate” a dataset from a set of model parame-
ters and constants.4,5 The utility of such a model lies
in its ability to support meaningful inferences from
data, such as an estimate of the true prevalence of
each category. Dawid and Skene (1979) proposed a
model to determine a consensus among patient his-
tories taken by multiple doctors. Inference is driven
by accuracies and biases estimated for each annota-
tor on a per-category basis. A graphical sketch of
the model is shown in Figure 1.
Let K be the number of possible labels or cat-
egories for an item, I the number of items to an-
notate, J the number of annotators, and N the to-
tal number of labels provided by annotators, where
each annotator may label each instance zero or more
times. Because the data is not a simple I x J data
matrix where every annotator labels every item ex-
actly once, a database-like indexing scheme is used
in which each annotation n is represented as a tu-
ple of an item ii[n] E 1:I, an annotator jj[n] E 1:J,
and a label y[n] E 1:K.6 Figure 2 illustrates how the
</bodyText>
<footnote confidence="0.9990537">
3If items are not independent, as assumed here, the interval
becomes wider.
4In a Bayesian setting, model parameters are also modeled
as randomly generated from a prior distribution.
5The size constants defining the data collection are not gen-
erated as part of the model. In a “discriminative” model, only
the outcomes and parameters are generated in this sense, not the
predictors (i.e., features).
6For the data indexing, we use jj and ii to avoid confusion
with the I items and J annotators of the model.
</footnote>
<figureCaption confidence="0.984165">
Figure 1:Graphical model sketch of the Dawid and
Fgure 1: Graphcal mode sketch of the Dawd- tor j a
</figureCaption>
<bodyText confidence="0.703352">
Skn m hced with Dri p
Skene model enhanced with Dirichlet priors. Sizes: J
</bodyText>
<equation confidence="0.937916476190476">
cgo
Size: J number o annotators K number of cat
number of annotators, K number of categories, I num-
egories I numbr of items N numer of labels The gener
ber of items, N number of labels collected. Estimated
colcted. Estimate parameters ✓ annotator goy o te
ie le z i
parameters: θ annotator accuracies/biases, π category
egy Observed dat: y labels Hyperpriors: ↵ h bs
prevalence, z true category. Observed data: y labels.
accuacies iases � prevnce. nnotator
Hyperpriors: α accuracies/biases, β prevalence.
na iin
pa
h1
sk3
17 os
.
.
t .
ac ano e sc o o
</equation>
<figureCaption confidence="0.999263">
Figure 2: Table of annotations y indexed by word in-
</figureCaption>
<bodyText confidence="0.812434333333333">
ore tims Because the data i not a simple of thee v
stance ii and annotator jj.
I ⇥ J data matrix wh
</bodyText>
<subsectionHeader confidence="0.581">
annotations can be assembled in a table where each
</subsectionHeader>
<bodyText confidence="0.902825230769231">
an annotator jj[n] 2 1:J, and a label y[n] 2 1:K.6 stance, the
ac
row is an annotation, and the column values are in-
As illustrated in ble 1, we asemble the nnota
used o e
i i dbelik tbl h h i
dices over items, annotators, and labels. The first
annotation, nd the values in each coumn are in
two rows show that on item 1, annotators 1 and 3
ces ove the itms, annotatrs nd labels. For p(zy,
assigned labels 4 andx1, respectively. The third row
says that for item 192 annotator 17 provided label 5.
Dawid and Skene’s model includes parameters
</bodyText>
<listItem confidence="0.992347">
• zi E 1:K for the true category of item i,
• πk E [0, 1] for the probability that an item is of
category k, subject to EKk=1 πk = 1, and
• θj,k,k&apos; E [0, 1] for the probabilty that annota-
tor j assigns the label k� to an item whose true
category is k, subject to EKk&apos;=1 θj,k,k&apos; = 1.
</listItem>
<bodyText confidence="0.775805666666667">
The generative model first selects the true category
for item i according to the prevalence of categories,
zi — Categorical(π).
</bodyText>
<figure confidence="0.988927636363636">
J
K
↵
✓
N
y
I
Q
⇡
z
1 e
as2
e 3
.
..
d1
ra1
umbe
192
.
..
t
jjnis
di
ynak
b
4y
t
1t
ab 5
e .
.
.
</figure>
<page confidence="0.996532">
314
</page>
<bodyText confidence="0.999860666666667">
The observed labels yn are generated based on an-
notator jj[n]’s responses θjj[n], z[ii[n]] to items ii[n]
whose true category is z[ii[n]],
</bodyText>
<equation confidence="0.890113">
yn — Categorical(θjj[n], z[ii[n]]).
</equation>
<bodyText confidence="0.939372">
We use additively smoothed maximum likelihood
estimation (MLE) to stabilize inference.
</bodyText>
<equation confidence="0.9977035">
θj,k — Dirichlet(αk)
π — Dirichlet(β).
</equation>
<bodyText confidence="0.9991115">
The unsmoothed MLE is equivalent to the MAP esti-
mate when αk and β are unit vectors.
</bodyText>
<subsubsectionHeader confidence="0.847517">
2.2.1 Estimated Senses
</subsubsectionHeader>
<bodyText confidence="0.99997675">
Given a set of annotators’ labels for a word in-
stance, the prevalence of senses, and the annotators’
accuracies and biases, Bayes’s rule can be used to
estimate the true sense of( each instance.
</bodyText>
<equation confidence="0.620244">
p(zi  |y, θ, π) oC p(zi |π) p(y|zi, θ)
</equation>
<bodyText confidence="0.99997625">
As we discuss in the next section, the important dif-
ference is that the weighting is based on the true
category, allowing the model to adjust for annotator
bias.
Spam annotators. The Dawid and Skene model
adjusts for annotations from noisy annotators. In the
limit, a label for a word instance from an annotator
whose response is independent of the true category
provides no information about the true sense of that
instance, and such a label provides no impact on the
resulting category estimate. For example, in a binary
task, a label from an annotator with response matrix
</bodyText>
<equation confidence="0.968742">
10.9 0.11
0.9 0.1
</equation>
<bodyText confidence="0.9997195">
provides no information on the true category. The
model cancels the effect of such an annotator’s label
because Pr[zi = 1|y&apos;, θj, π] = Pr[zi = 1|π], which
follows from the fact that
</bodyText>
<equation confidence="0.99629175">
θj =
Y= πz[i] θjj[n],z[i],y[n]. π1 X θj,1,1 π1 .
ii[n]=i π2 X θj,2,1 =
π2
</equation>
<bodyText confidence="0.999971666666667">
As a simple example, consider K = 2 outcomes
with prevalences π1 = 0.2, and π2 = 0.8. Suppose
three annotators with response matrices
</bodyText>
<equation confidence="0.982058">
~.75 .25 � ~.65 .35 � ~.9 .1 �
θ2 = θ3=
.40 .60 .30 .70 .2 .8
</equation>
<bodyText confidence="0.5976235">
supplied labels y1 = 1, y2 = 1, and y3 = 2 for word
instance i, respectively. Then
</bodyText>
<equation confidence="0.996736888888889">
Pr[zi =1|y, θ, π] oC π1 θ1,1,1 θ2,1,1 θ3,1,2 = .00975
Pr[zi =2|y, θ, π] oC π2 θ1,2,1 θ2,2,1 θ3,2,2 = .0768.
By normalizing (and rounding),
.00975
Pr[zi = 1|y, θ, π] = = .11
.00975 + .0768
.0768
Pr[zi = 2|y, θ, π] = = .89
.00975 + .0768
</equation>
<bodyText confidence="0.984662942857143">
Although the majority vote on i is for category 1, the
estimated probability that the category is 1 is only
0.11, given the adjustments for annotators’ accura-
cies and biases.
Comparison to voting. On the log scale, the an-
notation model is similar to a weighted additive vot-
ing scheme with maximum weight zero and no min-
imum weight; if u E (0, 1], then log u E (—oc, 0].
Biased Annotators. Biased annotators can have
low accuracy and low agreement with other anno-
tators, yet still provide a great deal of information
about the true label. For example, in a binary task,
a positively biased annotator will return relatively
more false positives and relatively fewer false neg-
atives compared to an unbiased one. As shown in
Section 4.2, our word sense task had fairly small es-
timated biases toward the high-frequency senses in
most cases. Other tasks, such as ordinal ranking of
author certainty for assertions, show systematically
biased annotators. Annotators may be biased toward
one end of an ordinal scale, or toward the center.
These kinds of biases are apparent in the annota-
tors in the annotation task described in (Rzhetsky
et al., 2009), where biologists labeled sentences in
biomedical research articles on a 1 to 7 scale of po-
larity and certainty.
Adversarial Annotators. An adversarial annota-
tor who always returns the wrong answer exhibits an
extreme bias. In a binary annotation case, it is clear
how perfectly adversarial answers provide the same
information as perfectly cooperative answers. Al-
though it is possible to estimate the response matrix
of an adversarial annotator, if too many of the anno-
tators are adversarial, the Dawid and Skene model
θ1=
</bodyText>
<page confidence="0.980592">
315
</page>
<bodyText confidence="0.998985">
cannot separate the truth from the lies. None of the
data sets we have collected showed any evidence of
adversarial labeling.
</bodyText>
<subsubsectionHeader confidence="0.644474">
2.2.2 How Much Information is in a Label?
</subsubsectionHeader>
<bodyText confidence="0.998415">
By comparing the uncertainty before and after in-
cluding a new label from an annotator, we can mea-
sure the reduction in uncertainty provided by the an-
notator’s label. By considering the expected reduc-
tion in uncertainty due to observing a label from an
annotator, we can quantify how much information
the label is expected to provide.
Entropy. The information-theoretic notion of en-
tropy makes the notion of uncertainty precise (Cover
and Thomas, 1991). If Zi is the random variable cor-
responding to the true label of word instance i with
K possible labels and probability mass function pZi,
its entropy is
</bodyText>
<equation confidence="0.990367">
H[Zi] = −EKk=1 pZi(k) log pZi(k).
</equation>
<bodyText confidence="0.714391666666667">
Conditional Entropy. Consider a label Yn = k&apos;
from annotator j = jjn for item i = iin. The en-
tropy of Zi conditioned on the observed label is
</bodyText>
<equation confidence="0.7402065">
H[Zi|Yn=k&apos;]
= − EKk=1 pZi|Yn(k|k&apos;) log pZi|Yn(k|k&apos;).
</equation>
<bodyText confidence="0.942401">
Conditional entropy is defined by the expected en-
tropy of Zi after observing Yn,
</bodyText>
<equation confidence="0.93124">
H[Zi|Yn] = EKk,=1 pYn(k&apos;) H[Zi|Yn=k&apos;].
</equation>
<bodyText confidence="0.999901428571428">
Conditional entropy can be generalized in the ob-
vious way to condition on more than one observed
label, for instance to compute the expected entropy
of Zi after observing two labels, Yn and Yn,.
Mutual Information. Mutual information is the
expected reduction in entropy in the state of Zi after
observing one or more labels,
</bodyText>
<equation confidence="0.792042">
I[Zi; Yn] = H[Zi] − H[Zi|Yn].
</equation>
<bodyText confidence="0.994190178571429">
Gibbs’ inequality ensures that mutual information is
positive. In theory at least, it never hurts to observe
a label (in expectation), no matter how bad the an-
notator is. In practice, we may not have an accu-
rate estimate of an annotator’s response probabili-
ties pYn Zi. Using log base 2, which measures in-
formation in bits, consider the three hypothetical an-
notators illustrated above. Clearly the most accurate
confusion matrix is θ3. The conditional entropies of
a new label for the three cases are, respectively, 0.71,
0.60 and 0.47 and the mutual information values are
0.01, 0.13 and 0.25.
Kinds of Annotators. A spam annotator pro-
vides zero information about a category, because
H[Zi|Yn] = H[Zi]. Spam annotators provide
the minimum possible mutual information, i.e.,
I[Zi; Yn] = 0.
A perfectly accurate annotator is one for whom
Pr[Yi = k|Zi] is 1 if k = Zi and 0 otherwise. For
such annotators, observing their label removes all
uncertainty, so that H[Zi|Yn] = 0. A perfect an-
notator provides maximum mutual information, i.e.,
I[Zi; Yn] = H[Zi].
A highly biased and hence inaccurate annotator
can provide as much information as a more accurate
annotator. This demonstrates that weighted voting
schemes are not the correct approach to inference
for true category labels.
</bodyText>
<subsectionHeader confidence="0.591758">
2.2.3 Implementation and Priors
</subsectionHeader>
<bodyText confidence="0.99913376">
The results in this paper were derived by ex-
pectation maximization using software written in
R. The code is distributed with the data under an
open-source license.7 Other implementations of the
Dawid and Skene model should produce the same
penalized maximum likelihood (equivalently maxi-
mum a posteriori) estimates.
The very weak Dirichlet priors added only arith-
metic stabilization to the inferences, allowing an
identified penalized maximum likelihood estimate in
cases where an annotator did not label any instances
of some sense for a word.
Bayesian posterior means provide similar results
for this model; full Bayes would also quantify es-
timation uncertainty, which as noted above, is sub-
stantial for the data sizes discussed here. Carpenter
(2008) discusses a more general approach based on a
hierarchical model for the accuracy/bias parameters
θ.
Modeling a random effect per item, such as
item difficulty, widens confidence intervals on ac-
curacies/biases, because observed labels may be
the result of item ease/difficulty or annotator ac-
curacy/bias. This would have been more realis-
tic, and would have provided additional information,
</bodyText>
<footnote confidence="0.81092">
7URL not given yet to preserve anonymity.
</footnote>
<page confidence="0.980475">
316
</page>
<table confidence="0.999328127659575">
Word Pos Senses α Agr.
All Used
late adj 9 7 0.85 0.90
high adj 7 5 0.84 0.91
long adj 8 7 0.67 0.81
full adj 9 8 0.57 0.69
poor adj 11 9 0.57 0.66
fair adj 10 8 0.54 0.70
common adj 12 6 0.40 0.53
particular adj 7 5 0.20 0.48
normal adj 4 4 0.02 0.38
work noun 8 7 0.70 0.80
number noun 7 7 0.62 0.95
book noun 12 9 0.60 0.84
image noun 17 9 0.57 0.71
paper noun 10 7 0.57 0.66
board noun 9 8 0.56 0.80
time noun 12 8 0.56 0.63
sense noun 8 5 0.54 0.65
way noun 19 12 0.49 0.62
window noun 10 8 0.48 0.62
date noun 11 7 0.47 0.57
land noun 14 10 0.47 0.55
life noun 26 14 0.43 0.52
control noun 13 9 0.34 0.47
level noun 10 7 0.21 0.44
color noun 12 7 0.15 0.66
family noun 14 8 0.14 0.32
live verb 14 7 0.69 0.78
read verb 13 9 0.64 0.89
appear verb 7 7 0.63 0.73
meet verb 19 11 0.58 0.66
serve verb 19 14 0.57 0.67
suggest verb 5 4 0.56 0.78
add verb 10 6 0.55 0.72
fold verb 8 5 0.52 0.72
wait verb 7 4 0.49 0.65
show verb 13 11 0.46 0.53
tell verb 10 8 0.44 0.59
lose verb 16 10 0.43 0.59
know verb 13 10 0.38 0.52
say verb 14 11 0.37 0.56
find verb 19 14 0.28 0.38
help verb 9 6 0.26 0.58
kill verb 14 12 0.26 0.76
win verb 11 5 0.25 0.72
ask verb 6 6 0.20 0.45
</table>
<figureCaption confidence="0.8018986">
Figure 3: Krippendorff’s α and pairwise agreement for
the 45 MASC words in the crowdsourcing study, with
number of WordNet senses available and used. Pair-
wise agreement was computed according to the formula
in Section 2.
</figureCaption>
<bodyText confidence="0.9999145">
but we felt the increased model complexity, espe-
cially with multivariate outputs, would distract from
our main point in contrasting model-based inference
with agreement statistics.
</bodyText>
<sectionHeader confidence="0.990214" genericHeader="method">
3 Two Data Collections
</sectionHeader>
<subsectionHeader confidence="0.999357">
3.1 MASC Word Sense Sentence Corpus
</subsectionHeader>
<bodyText confidence="0.999988205128205">
To motivate our case study, we briefly discuss some
of the limitations of the MASC word sense sentence
corpus, which is an addendum to the MASC cor-
pus.8 For convenience, we refer here to the word
sense sentence corpus as the MASC corpus. This
is a 1.3 million word corpus with approximately
one thousand sentences per word, for 116 words
nearly evenly balanced among nouns, adjectives and
verbs (Passonneau et al., 2012a). Each sentence is
drawn from the MASC corpus or the Open Ameri-
can National Corpus, exemplifies at least one of the
116 MASC words, and has been annotated by trained
annotators who used WordNet senses as annotation
labels. The annotation process is described in de-
tail in (Passonneau et al., 2012a; Passonneau et al.,
2012b).
The annotators were college students from Vas-
sar, Barnard, and Columbia who were given general
training in the annotation process, then were trained
together on each word with a sample of fifty sen-
tences, which included discussion with Christiane
Fellbaum, one of the designers of WordNet. After
the pre-annotation sample, annotators worked inde-
pendently to label 1,000 sentences for each word
using an annotation tool that presented the Word-
Net senses and example usages, plus four variants of
none of the above. For each word, 100 of the 1,000
sentences were annotated by two to four annotators
to assess inter-annotator reliability.
Figure 3 shows 45 randomly selected MASC
words that were re-annotated using crowdsourcing.
Shown are the part of speech, the number of Word-
Net senses, the number of senses used by annota-
tors, the α value, and pairwise agreement. While the
MASC word sense data demonstrates that annota-
tors can agree on words with many senses, there are
many words with low agreement, and correspond-
ingly questionable ground truth labels. There is no
correlation between the agreement and number of
</bodyText>
<footnote confidence="0.967305">
8http://www.anc.org/data/masc/
</footnote>
<page confidence="0.997857">
317
</page>
<bodyText confidence="0.989065363636364">
available senses, or senses used by annotators (Pas-
sonneau et al., 2012a).
Due to limited resources, the project deviated
from best practice in having only a single round of
annotation per word, and no iteration to achieve an
agreement threshold. All annotators, however, had
at least two phases of training, and most annotated
several rounds. Below we use mutual information to
show that the quality of the crowdsourced labels is
equivalent to or superior than labels from the trained
MASC annotators.
</bodyText>
<subsectionHeader confidence="0.999306">
3.2 Crowdsourced Word Sense Annotation
</subsectionHeader>
<bodyText confidence="0.999886714285714">
To collect the data, we relied on Amazon Me-
chanical Turk, a crowdsourcing marketplace that is
used extensively in the NLP community (Callison-
Burch and Dredze, 2010). Human Intelligence Tasks
(HITs) are presented to Turkers by requesters. Cer-
tain aspects of the task were the same as for the
MASC data: 45 randomly selected MASC words
were used, sentences were drawn from the same
pool, and the annotation labels were the same Word-
Net 3.0 senses. Instead of collecting a single la-
bel for most instances, however, we collected up to
twenty-five. Other differences from the MASC data
collection were: the annotators were not trained;
the annotation interface differed, though it presented
the same information; the sets of sentences were
not identical; annotators labeled any number of in-
stances for a word up to the limit of 25 labels per
word; finally, the Turkers were not instructed to be-
come familiar with WordNet.
In each HIT, Turkers were presented with ten sam-
ple sentences for each word, with the word’s senses
listed below each sentence. A short paragraph of in-
structions indicated there would be up to 100 HITs
for each word. To encourage Turkers to do multiple
HITs per word, so we could estimate annotator ac-
curacies more tightly, the instructions indicated that
Turkers could expect their time per HIT to decrease
with increasing familiarity with the word’s senses.
Most but not all crowdsourced instances had also
been annotated by the trained annotators. Fig-
ures 7a-7b in Section 5, which compares the ground
truth labels from the trained annotators with the
crowdsourced labels, indicates for each word how
many instances were annotated in common (e.g.,
960 for board (verb)). Sentences were drawn from
</bodyText>
<figure confidence="0.999342923076923">
(a) add (verb) (α = 0.55, agreement=0.72)
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
Sense1 Sense2
(b) date (noun) (α = 0.47, agreement=0.57)
(c) help (verb) (α = 0.26, agreement=0.58)
(d) ask (verb) (α = 0.20, agreement=0.45)
</figure>
<figureCaption confidence="0.96869325">
Figure 4: Prevalence estimates for 4 words: the x-axis
is the sense number, and the y-axis the proportion of
instances assigned that sense. MASC FREQ: frequency
of each sense in the singly-annotated instances from the
trained MASC annotators; AMT MAJ: frequency of each
majority vote sense for instances annotated by ≈25 Turk-
ers; AMT MLE: estimated probability of each sense for
instances annotated by ≈25 Turkers, using MLE.
</figureCaption>
<figure confidence="0.999725454545455">
0.70 Help (Verb) MASC Freq
0.60 AMT Maj
0.50 AMT MLE
0.40
0.30
0.20
0.10
0.00
Sense1 Sense2 Sense3 Sense4 Sense5 Sense6 Sense7 Sense8
MASC FREQ
AMT Maj
AMT MLE
Ask (Verb)
Sense1 Sense2 Sense3 Sense4 Sense5 Sense6 Sense7
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
</figure>
<page confidence="0.993723">
318
</page>
<table confidence="0.999182076923077">
Sense &gt; 0.99 Prop. Sense &gt; 0.99 Prop. Sense &gt; 0.99 Prop. Sense &gt; 0.99 Prop.
0 9 0.01 0 19 0.02 0 0 0.00 0 6 0.01
1 461 0.48 1 68 0.07 1 279 0.30 1 348 0.36
2 135 0.14 2 19 0.02 2 82 0.09 2 177 0.18
3 107 0.11 3 83 0.09 3 201 0.21 3 9 0.01
4 50 0.05 4 173 0.18 4 24 0.03 4 251 0.26
5 50 0.05 5 190 0.20 5 0 0.00 5 0 0
6 93 0.10 6 133 0.14 6 169 0.18 6 0 0
SubTot. 905 0.94 7 236 0.25 7 0 0.00 7 6 0.01
(Rest 62 0.06) 8 5 0.01 8 5 0.01 8 6 0.01
Total 962 1.00 SubTot. 926 0.97 SubTot. 760 0.81 SubTot. 803 0.83
(Rest 33 0.03) (Rest 180 0.19) (Rest 163 0.17)
Total 959 1.00 Total 940 1.00 Total 966 1.00
</table>
<figure confidence="0.856089">
(a) add (verb): 94% (b) date (noun): 97% (c) help (verb): 81% (d) ask (verb): 83%
</figure>
<figureCaption confidence="0.999944">
Figure 5: Proportion of instances where posterior probabilities ≥ 0.99
</figureCaption>
<bodyText confidence="0.999961404761905">
the same pool but in a few cases, the overlap is sig-
nificantly less than the full 900-1,000 instances (e.g.,
work (noun) with 380).
Given 1,000 instances per word for a category
whose prevalence is as low as 0.10 (100 examples
expected), the 95% interval for sample prevalence,
assuming examples are independent, will be 0.10 f
0.06. We collected between 20 and 25 labels per
item to get reasonable confidence intervals for the
true label, and so that future models could incor-
porate item difficulty. The large number of labels
sharpens our estimates of the true category signif-
icantly, as estimated error goes down as O(1/Vn)
with n independent annotations. Confidence inter-
vals must be expanded as correlation among annota-
tor responses increases due to item-level effects such
as difficulty or subject matter.
Requesters can control many aspects of HITs. To
ensure a high proportion of instances with high qual-
ity inferred labels, we piloted the HIT design with
two trials of two and three words each, and dis-
cussed both with Turkers on the Turker Nation mes-
sage board. The HIT title we chose–For American
English Word Mavens–targeted Turkers with an in-
herent interest in words and meanings, and we re-
cruited Turkers with high performance ratings and a
long history of good work. The final procedure and
payment were as follows. To avoid spam workers,
we required Turkers to have a 98% lifetime approval
rating and to have successfully completed 20,000
HITs. HITs were automatically approved after fif-
teen minutes. We monitored performance of Turk-
ers across HITs by comparing individual Turker’s la-
bels to the current majority labels. Turkers with very
poor performance were warned to take more care, or
be blocked from doing further HITs. Of 228 Turk-
ers, five were blocked, with one subsequently un-
blocked. The blocked Turker data is included with
the other Turker data in our analyses and in the full
data release. As noted above, the model-based ap-
proach to annotation is effective at adjusting for in-
accurate annotators.
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.978859">
4.1 Estimates for Prevalence and Labels
</subsectionHeader>
<bodyText confidence="0.985373526315789">
Modeling annotators as having distinct biases and
accuracies should match the intuitions of anyone
who has compared the results of more than one an-
notator on a task. The power of the Dawid and Skene
model, however, shows up in the estimates it yields
for category prevalence and for the true labels on
each instance. Figure 4 contrasts three ways to es-
timate sense prevalence, illustrated with four of the
crowdsourced words. AMT MLE is the model esti-
mate from Turkers’ labels. MASC FREQ is a naive
rate from the trained annotators’ label distributions,
rather than a true estimate. Majority voted labels
for Turkers (AMT MAJ) are closer to the model esti-
mates than MASC FREQ, but do not take annotators’
biases into account.
The plots for the four words in Figure 4 are or-
dered by their α scores for the 100 instances that
were annotated in common by four trained anno-
tators: add (0.55) &gt; date (0.47) &gt; help (0.26) &gt;
</bodyText>
<figure confidence="0.982811">
(a) Four of 57 annotators for add (verb)
</figure>
<figureCaption confidence="0.777941">
(b) Four of 49 annotators for help (verb)
Figure 6: Example confusion matrices of estimated annotator accuracies and biases
</figureCaption>
<bodyText confidence="0.999724290322581">
ask (0.20). The prevalence estimates diverge less
on words where the agreement is higher. Notably,
the plots for the first three words demonstrate one
or more senses where the AMT MLE estimate dif-
fers markedly from all other estimates. In Figure 4a,
the AMT MLE estimate for sense 1 is much lower
(0.51) than the other two measures. In Figure 4b,
the AMT MLE estimate for sense 4 is much closer to
MASC FREQ than AMT MAJ, which sugggests that
some Turkers are biased against sense 4. The AMT
MLE estimates for senses 1, 6 and 7 are distinctive.
For help, the AMT MLE estimates for senses 1 and
6 are particularly distinctive. For ask senses 2 and
4, the divergence of the AMT MAJ estimates is again
evidence of bias in some Turkers.
The estimates of label quality on each item are
perhaps the strongest reason for turning to model-
based approaches to assess annotated data. For the
same four words, Figure 5 shows the proportion of
all instances that had an estimated true label where
the label probability was greater than or equal to
0.99. This proportion ranges from 97% for date
to 81% for help. Even for help, of the remaining
19% of instances of less confident estimated labels,
13% have posterior probabilities greater than 0.75.
Figure 5 also shows that the high quality labels for
each word are distributed across many of the senses.
Of the 45 words studied here, 20 had α scores less
than 0.50 from the trained annotators. For 42 of the
same 45 words, 80% of the inferred true labels have
a probability higher than 0.99.
</bodyText>
<subsectionHeader confidence="0.999762">
4.2 Annotator Accuracy and Bias
</subsectionHeader>
<bodyText confidence="0.99990705">
Figure 6 shows confusion matrices in the form of
heatmaps that plot annotator responses by the esti-
mated true labels. Darker cells have higher probabil-
ities. Perfect response accuracy (agreement with the
inferred true label) would yield black squares on the
diagonal and white on the off-diagonal. Figure 6a
and Figure 6b show heatmaps for four annotators
for the two words of the four that had the highest
and third highest α values.
The two figures show that the Turkers were gener-
ally more accurate on add (verb) than on help (verb),
which is consistent with the differences in the inter-
annotator agreement of trained annotators on these
two words. In contrast to what can be learned from
agreement metrics, inference based on the annota-
tion model provides estimates of bias towards spe-
cific values. Figure 6a shows the bias of these anno-
tators to overuse WordNet sense 1 for help. Further,
there were no assignments of senses 6 or 8 for this
word. The figures provide a succinct visual sum-
</bodyText>
<page confidence="0.993398">
320
</page>
<bodyText confidence="0.999990473684211">
mary that there were more differences across the
four annotators for help than for add, with more
bias towards overuse of not only sense 1, but also
senses 2 (annotators 8 and 41) and 3 (annotator 9).
When annotator 8 uses sense 1, the true label is often
sense 6, thus illustrating how annotators provide in-
formation about the true label even from inaccurate
responses.
Mean accuracies per word ranged from 0.86 to
0.05, with most words showing a large spread across
senses, and higher mean accuracy for the more fre-
quent senses. Mean accuracy for add was 0.90 for
sense 1, 0.79 for sense 2, and much lower for senses
6 (0.29) and 7 (0.19). For help, mean accuracy was
best on sense 1 (0.73), which was also the most fre-
quent, but it was also quite good on sense 4 (0.64),
which was much less frequent. Mean accuracies on
senses of help ranged from 0.11 (senses 5, 7, and
other) to 0.73 (sense 1).
</bodyText>
<sectionHeader confidence="0.997705" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.994811445945946">
For many of the words, the model yields the same la-
bel values as the trained annotator on a large major-
ity of instances, yet for nearly as many words there
is more disparity. After we discuss how the model-
based and trained annotators labels line up with each
other, we argue that the model estimates are bet-
ter. The two sets of labels cannot be differentiated
from one another by mutual information. In contrast
to the model estimates, the trained annotator labels
have no confidence value, and no estimate for the
trained annotator’s accuracy. We conclude the sec-
tion with a cost comparison.
Figure 7 compares how many instances have the
same labels from the trained annotators and of Turk-
ers (blue); from the trained annotators and the model
(red), and from the Turker Plurality and the model
(green). Recall that about ninety percent of the in-
stances labled by trained annotators have a single
label; for the ten percent with two to four annota-
tors, we used the majority label if there was one,
else gave each tied sense a proportional amount of
the vote. Figure 7a shows 22 words where all three
comparisons have about the same relative propor-
tion in common (70%-98% on average). Here sets
with the least overlap are the trained annotators com-
pared with the model, with the exception of win-
dow (noun). The bottom figure shows the 23 words
where the proportion in common is relatively lower
(35%-75% on average), mostly due to the two com-
parisons for the trained annotators. Across the 45
words, the proportion of instances that had the same
labels assigned by the trained annotators and the
model does not correlate with the α scores for the
words, or with pairwise agreement
Previous work has shown that model-based esti-
mates are superior to majority-voting (Snow et al.,
2008). Figure 7 shows that the trained annotators’
labels match the model (red bars) consistently less
often than they match the Turker plurality, which is
often a majority (blue bars). There are a fair number
of cases, however, with a large disparity between the
trained annotators and Turkers. This is most appar-
ent when the green bar is much higher than the red
or blue bars. For the word meet (verb), for exam-
ple, in 19% of cases the trained annotator used sense
4 of WordNet 3.0 (glossed as “fill or meet a want
or need”) where the the plurality of Turkers selected
sense 5 (glossed as “satisfy a condition or restric-
tion”). Notably, in WordNet 3.1, two of the Word-
Net 3.0 senses for meet (verb) have been removed,
including the sense 5 that the Turkers favored in our
data. A similar situation occurs with date (noun):
17% of cases where the trained annotator used sense
4, the plurality of Turkers used sense 5; the former
sense 4 is no longer in WordNet 3.0.
For the trained annotators, interannotator agree-
ment and pairwise agreement varied widely, as
shown in Figure 3. Measures of the information pro-
vided by labels from Turkers and trained annotators
give a similarly wide range across both groups. Fig-
ure 8 shows a histogram of estimated mutual infor-
mation for Turkers and MASC annotators across the
four words. The most striking feature of these plots
is the large variation in mutual information scores
within both groups of annotators for each word (note
that date and help had many more trained annotators
than add or ask). There is no evidence that a label
from a trained annotator provides more information
than a Turker’s. Thus we conclude that a model-
based label derived from many Turkers is preferable
to a label from a single trained annotator.
In contrast to current best practice, an annotation
model yields far more information about the most
essential aspect of annotation efforts, namely how
</bodyText>
<page confidence="0.993767">
321
</page>
<figure confidence="0.89769">
(a) For these 22 words, the three sets of labels (trained annotators, Turker plurality, Turker
model) have a high proportion in common and lower variance.
(b) For these 23 the words, the three sets of labels (trained annotators, Turker plurality, Turker
model) have a lower proportion in common and higher variance.
</figure>
<figureCaption confidence="0.998769">
Figure 7: Proportion of instances labeled by both trained annotators and Turkers (total instances in parentheses) where
the trained annotator label matches the Turker plurality (blue), where the trained annotator label matches the model
(red), and where the Turker plurality matches the model (green)
</figureCaption>
<figure confidence="0.998676041666667">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1 %Trained = Turker Plurality
%Trained = Turker Model
%Turker Plur = Turker Model
0.1
% Trained = Turker Plurality
%Trained = Turker Model
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<bodyText confidence="0.998764742857143">
much uncertainty is associated with each gold stan-
dard label. In our case, the richer information comes
at a lower cost. Over the course of a five-year pe-
riod that included development of the infrastructure,
17 undergraduates who annotated the 116 MASC
words were paid an estimated total of $80,000 for
116 words × 1000 sentences per word, which comes
to a unit cost of $0.70 per ground truth label. In a 12
month period with 6 months devoted to infrastruc-
ture and trial runs, we paid 228 Turkers a total of
$15,000 for 45 words × 1000 sentences per word,
for a unit cost of $0.33 per ground truth label. In
short, the AMT data cost less than half the trained
annotator data.
For annotation tasks such as this one, where each
candidate word has multiple class labels, the com-
parison between the two methods of data collection
shows that the model-based estimates from crowd-
sourced data have at least the same quality, if not
higher, for less cost. The fact that each label has
an associated confidence makes them more valuable
because the end user can choose how to handle la-
bels with lower certainty: for example, to assign
them less weight in evaluating word sense disam-
biguation systems, or to eliminate them from train-
ing for statistical approaches to building such sys-
tems. Each word here has a distinct set of classes,
and the results from both the trained annotators and
model indicate that some sets of sense labels led
to greater agreement or a higher proportion of high
confidence labels. In many cases, results for the
words with fewer high confidence labels could be
improved by revising the sense inventories, as sug-
gested by the examples with meet (verb) and date
(noun).
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999259">
Alternative metrics to measure association of raters
on binary data have been proposed to overcome de-
ficiencies in κ when there is data skew. The G-
index (Holley and Guildford, 1964; Vegelius, 1981),
for example, is argued to improve over the Matthews
Correlation Coefficient (Matthews, 1975). Feinstein
and Cicchetti (1990) outline the undesirable behav-
ior that κ-like metrics will have lower values when
there is high agreement on highly skewed data. κ as-
sumes that chance agreement on the more prevalent
</bodyText>
<page confidence="0.994172">
322
</page>
<bodyText confidence="0.999856235294118">
class becomes high. Gwet (2008) presents a met-
ric that estimates the likelihood of chance agreement
based on the assumption that chance agreement oc-
curs only when annotators assign labels randomly,
which is estimated from the data. Klebanov and
Beigman (2009) make a related assumption that an-
notators agree on easy cases and behave randomly
on hard cases, and propose a model to estimate the
proportion of hard cases.
Model-based gold-standard estimation such as
(Dawid and Skene, 1979) has long been the stan-
dard in epidemiology, and has been applied to dis-
ease prevalence estimation (Albert and Dodd, 2008)
and also to many other problems such as human an-
notation of craters in images of Venus (Smyth et al.,
1995). Smyth et al. (1995), Rogers et al. (2010),
and Raykar et al. (2010) all discuss the advantages
of learning and evaluation with probabilistically an-
notated corpora. Rzhetsky et al. (2009) and White-
hill et al. (2009) estimate annotation models without
gold-standard supervision, but neither models anno-
tator biases, which are critical for estimating true la-
bels.
Perhaps the first application of Dawid and Skene’s
model to NLP data was the Bruce and Wiebe (1999)
investigation of word sense. Much later, Snow et al.
(2008) used the same model to show that combin-
ing noisy crowdsourced annotations produced data
of equal quality to five distinct published gold stan-
dards, including an example of word sense. Both
works estimate the Dawid and Skene model using
supervised gold-standard category data, which al-
lows direct estimation of annotator accuracy and
bias. Hovy et al. (2013) recently presented a much
</bodyText>
<figure confidence="0.8661505">
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5
mutual_info
</figure>
<figureCaption confidence="0.977082">
Figure 8: Histograms of mutual information estimates
for the four example words; trained annotators are in the
top row and Turkers in the bottom.
</figureCaption>
<bodyText confidence="0.99994716">
simpler model to filter out spam annotators. Crowd-
sourcing is now so widespread that NAACL 2010
sponsored a workshop on “Creating Speech and
Language Data with Amazon’s Mechanical Turk”
and in 2011, TREC added a crowdsourcing track.
Active learning is an alternative method to anno-
tate corpora, thus the Troia project (Ipeirotis et al.,
2010) is a web service implementation of a maxi-
mum a posteriori estimator for the Dawid and Skene
model, with a decision-theoretic module for active
learning to select the next item to label. They draw
on the Sheng et al. (2008) model to actively select
the next label to elicit, which provides a very simple
estimate of expected accuracy for a given number of
labels. This essentially provides a statistical power
calculation for annotation tasks. Because it is explic-
itly designed to measure reduction in uncertainty,
mutual information should be the ideal choice for
guiding such active labeling (MacKay, 1992). Such
a strategy of selecting features with maximal mutual
information has proven effective in greedy feature-
selection strategies for classifiers, despite the fact
that the objective function was classification accu-
racy, not entropy (Yang and Pedersen, 1997; For-
man, 2003).
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99996575">
Interannotator agreement applies to a set of annota-
tions, and provides no information about individual
instances. When two or more annotators have very
high interannotator agreement on a task, unless they
have perfect accuracy, there will be instances where
they agreed incorrectly, and no way to predict which
instances these are. Moreover, for many semantic
annotation tasks, high κ is impractical. In addition,
there is often a pragmatic dimension where labels
represent community-established conventions of us-
age. In such cases, no one individual can reliably
assign labels because the ground truth derives from
consensus among the community of language users.
Word sense annotation is such a task.
An annotation model applied to the type of crowd-
sourced labels collected here provides more knowl-
edge and higher quality gold standard labels at
lower cost than the conventional method used in
the MASC project. Those who would use the cor-
pus for training benefit because they can differen-
</bodyText>
<figure confidence="0.997951">
count
15
10
15
10
5
0
5
0
add.v
ask.v
date.n
help.v
masc
turk
</figure>
<page confidence="0.997801">
323
</page>
<bodyText confidence="0.9999557">
tiate high from low confidence labels. Those who
would use such a corpus for cross-site evaluations
of word sense disambiguation systems benefit be-
cause there are more evaluation options. Where the
most probable label is relatively uncertain, systems
can be penalized less for an incorrect but close re-
sponse. Crowdsourcing has already made it possible
to annotate corpora more cheaply, and wider use of
annotation models in NLP should lead to more con-
fidence from users in the corpora we create.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999913875">
The first author was supported by NSF CRI-0708952
and CRI-1059312. The second author was supported
by NSF CNS-1205516 and DoE DE-SC0002099. We
thank Shreya Prasad for work on the data collection,
Mizi Morris and Boyi Xie for results munging and
feedback on the paper, and Marilyn Walker for ad-
vice on collaborating with turkers on the design of
HITs through message boards.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999670390243902">
Paul S. Albert and Lori E. Dodd. 2008. On estimating
diagnostic accuracy from studies with multiple raters
and partial gold standard evaluation. Journal of the
American Statistical Association, 103(481):61–73.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Rebecca F. Bruce and Janyce M. Wiebe. 1998. Word-
sense distinguishability and inter-coder agreement. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing, pages 53–60.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study of manual tagging.
Natural Language Engineering, 1(1):1–16.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Technical report, Alias-i,
Inc.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37–46.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. Wiley-Interscience, New York,
NY, USA.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries C (Applied Statistics), 28(1):20–28.
Barbara di Eugenio and Michael Glass. 2004. The Kappa
statistic: A second look. Computational Linguistics,
30(1):95–101.
Barbara di Eugenio. 2000. On the usage of Kappa to
evaluate agreement on coding tasks. In Proceedings
of the Second International Conference on Language
Resources and Evaluation (LREC).
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):54–77.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low Kappa: I. The problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543 – 549.
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289–1305.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29–48.
J. W. Holley and J. P. Guildford. 1964. A note on the G
index of agreement. Educational and Psychological
Measurement, 24:749–753.
Dirk Hovy, Tayler Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust with
MACE. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1120–1130, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on Amazon Mechanical
Turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation, HCOMP 2010, pages 64–67,
New York, NY, USA. ACM.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Compu-
tational Linguistics, 35(4):495–503.
Klaus Krippendorff. 1980. Content analysis: An intro-
duction to its methodology. Sage Publications, Bev-
erly Hills, CA.
David J. C. MacKay. 1992. Information-based objective
functions for active data selection. Neural Computa-
tion, 4:590–604.
B. W. Matthews. 1975. Comparison of the predicted and
observed secondary structure of t4 phage lysozyme.
Biochimica et Biophysica Acta (BBA) - Protein Struc-
ture, 405(2):442–451.
</reference>
<page confidence="0.988321">
324
</page>
<reference confidence="0.999848263157894">
George A. Miller. 1995. A lexical database for English.
Communications of the ACM, 38(11):39–41.
Rebecca J. Passonneau, Collin F. Baker, Christiane Fell-
baum, and Nancy Ide. 2012a. The MASC word sense
corpus. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC’12), pages 3025–3030, Istanbul, Turkey. Eu-
ropean Language Resources Association (ELRA).
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: Evaluating and learning from multiply la-
beled word sense annotations. Language Resources
and Evaluation, 46(2):219–252.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297–
1322.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistical Computing, 20:317–334.
Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.
2009. How to get the most out of your curation effort.
PLoS Computational Biology., 5(5):1–13.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get another label? Improving data qual-
ity and data mining using multiple, noisy labelers.
In Proceedings of the Fourteenth ACM International
Conference on Knowledge Discovery and Data Min-
ing (KDD).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground truth
from subjectively-labeled images of Venus. In Ad-
vances in Neural Information Processing Systems 7,
pages 1085–1092. MIT Press.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
254–263, Honolulu.
Jan Vegelius. 1981. Significance tests for the G-index.
Educational and Psychological Measurement, 41:99–
108.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Y. Bengio, D. Schu-
urmans, J. Lafferty, C. K. I. Williams, and A. Culotta,
editors, Advances in Neural Information Processing
Systems 22, pages 2035–2043, December.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the Fourteenth International Con-
ference on Machine Learning, ICML ’97, pages 412–
420, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
</reference>
<page confidence="0.9995285">
325
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.509592">
<title confidence="0.999461">The Benefits of a Model of Annotation</title>
<author confidence="0.999802">J Rebecca</author>
<affiliation confidence="0.99976">Center for Computational Learning</affiliation>
<address confidence="0.9449255">Columbia New York, NY USA</address>
<email confidence="0.999641">becky@ccls.columbia.edu</email>
<author confidence="0.987731">Bob</author>
<affiliation confidence="0.999203">Department of</affiliation>
<address confidence="0.814566">Columbia New York, NY USA</address>
<email confidence="0.999852">carp@alias-i.com</email>
<abstract confidence="0.992091846153846">Standard agreement measures for interannotator reliability are neither necessary nor sufficient to ensure a high quality corpus. In a case study of word sense annotation, conventional methods for evaluating labels from trained annotators are contrasted with a probabilistic annotation model applied to crowdsourced data. The annotation model provides far more information, including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul S Albert</author>
<author>Lori E Dodd</author>
</authors>
<title>On estimating diagnostic accuracy from studies with multiple raters and partial gold standard evaluation.</title>
<date>2008</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>103</volume>
<issue>481</issue>
<contexts>
<context position="45859" citStr="Albert and Dodd, 2008" startWordPosition="7909" endWordPosition="7912">revalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. </context>
</contexts>
<marker>Albert, Dodd, 2008</marker>
<rawString>Paul S. Albert and Lori E. Dodd. 2008. On estimating diagnostic accuracy from studies with multiple raters and partial gold standard evaluation. Journal of the American Statistical Association, 103(481):61–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="6526" citStr="Artstein and Poesio, 2008" startWordPosition="1024" endWordPosition="1028">t best practice for creating annotation standards involves iteration over four steps: 1) design or redesign the annotation task, 2) write or revise guidelines to instruct annotators how to carry out the task, possibly with some training, 3) have two or more annotators work independently to annotate a sample of data, 4) measure the interannotator agreement on the data sample. Once the desired agreement has been obtained, the final step is to create a gold standard dataset where each item is annotated by a single annotator. How much chance-adjusted agreement is sufficient has been much debated (Artstein and Poesio, 2008; di Eugenio and Glass, 2004; di Eugenio, 2000; Bruce and Wiebe, 1998). Surprisingly, little attention has been devoted to the question of whether the agreement subset is a representative sample of the corpus. Without such an assurance, there is little justification to take interannotator agreement as a quality measure of the corpus as a whole. Given the influence that a gold standard corpus can have on progress in our field, it is not clear that agreement measures on a corpus subset provide a sufficient guarantee of corpus quality. While it is taken for granted that some annotators perform be</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Wordsense distinguishability and inter-coder agreement.</title>
<date>1998</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--60</pages>
<contexts>
<context position="6596" citStr="Bruce and Wiebe, 1998" startWordPosition="1037" endWordPosition="1040"> four steps: 1) design or redesign the annotation task, 2) write or revise guidelines to instruct annotators how to carry out the task, possibly with some training, 3) have two or more annotators work independently to annotate a sample of data, 4) measure the interannotator agreement on the data sample. Once the desired agreement has been obtained, the final step is to create a gold standard dataset where each item is annotated by a single annotator. How much chance-adjusted agreement is sufficient has been much debated (Artstein and Poesio, 2008; di Eugenio and Glass, 2004; di Eugenio, 2000; Bruce and Wiebe, 1998). Surprisingly, little attention has been devoted to the question of whether the agreement subset is a representative sample of the corpus. Without such an assurance, there is little justification to take interannotator agreement as a quality measure of the corpus as a whole. Given the influence that a gold standard corpus can have on progress in our field, it is not clear that agreement measures on a corpus subset provide a sufficient guarantee of corpus quality. While it is taken for granted that some annotators perform better than others,2 agreement metrics do not differentiate annotators. </context>
</contexts>
<marker>Bruce, Wiebe, 1998</marker>
<rawString>Rebecca F. Bruce and Janyce M. Wiebe. 1998. Wordsense distinguishability and inter-coder agreement. In Proceedings of Empirical Methods in Natural Language Processing, pages 53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Recognizing subjectivity: a case study of manual tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="46429" citStr="Bruce and Wiebe (1999)" startWordPosition="8004" endWordPosition="8007">isease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards, including an example of word sense. Both works estimate the Dawid and Skene model using supervised gold-standard category data, which allows direct estimation of annotator accuracy and bias. Hovy et al. (2013) recently presented a much 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 mutual_info Figure 8: Histograms of mutual information esti</context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recognizing subjectivity: a case study of manual tagging. Natural Language Engineering, 1(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Multilevel Bayesian models of categorical data annotation.</title>
<date>2008</date>
<tech>Technical report,</tech>
<publisher>Alias-i, Inc.</publisher>
<contexts>
<context position="23989" citStr="Carpenter (2008)" startWordPosition="4069" endWordPosition="4070">er an open-source license.7 Other implementations of the Dawid and Skene model should produce the same penalized maximum likelihood (equivalently maximum a posteriori) estimates. The very weak Dirichlet priors added only arithmetic stabilization to the inferences, allowing an identified penalized maximum likelihood estimate in cases where an annotator did not label any instances of some sense for a word. Bayesian posterior means provide similar results for this model; full Bayes would also quantify estimation uncertainty, which as noted above, is substantial for the data sizes discussed here. Carpenter (2008) discusses a more general approach based on a hierarchical model for the accuracy/bias parameters θ. Modeling a random effect per item, such as item difficulty, widens confidence intervals on accuracies/biases, because observed labels may be the result of item ease/difficulty or annotator accuracy/bias. This would have been more realistic, and would have provided additional information, 7URL not given yet to preserve anonymity. 316 Word Pos Senses α Agr. All Used late adj 9 7 0.85 0.90 high adj 7 5 0.84 0.91 long adj 8 7 0.67 0.81 full adj 9 8 0.57 0.69 poor adj 11 9 0.57 0.66 fair adj 10 8 0.</context>
</contexts>
<marker>Carpenter, 2008</marker>
<rawString>Bob Carpenter. 2008. Multilevel Bayesian models of categorical data annotation. Technical report, Alias-i, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="9491" citStr="Cohen, 1960" startWordPosition="1555" endWordPosition="1556">rtion of observed agreements that are above the proportion expected by chance. Given an estimate Am,n of the probability that two annotators m, n E 1:J will agree on a label and an estimate of the probability Cm,n that they will agree by chance, chance-adjusted agreement ZAm,n E [−1, 1] is defined by ZAm,n = Am,n−Cm,n 1−Cm,n . Chance agreement takes into account the prevalence of the individual labels in 1:K. Specifically, it is defined to be the probability that a pair of labels drawn at random for two annotators will agree. There are two common ways to define this draw. Cohen’s κ statistic (Cohen, 1960) assumes each annotator draws uniformly at random from her set of labels. Letting ψj,k = I EIi=1 I(yi,j = k) be the proportion of the label k in annotator j’s labels, this notion of chance agreement for a pair of annotators m, n is estimated as the product of their proportions ψ: EK Cm,n /&apos; /&apos; — �k=1 4&apos;m,k X 4&apos;n,k. Krippendorff’s α, another chance-adjusted metric in wide use, assumes each annotator draws uniformly at random from the pooled set of labels from all annotators (Krippendorff, 1980). Letting φk be the proportion of label k in the entire set of labels, this alternative estimate, C0m,</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>Wiley-Interscience,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="21066" citStr="Cover and Thomas, 1991" startWordPosition="3586" endWordPosition="3589">315 cannot separate the truth from the lies. None of the data sets we have collected showed any evidence of adversarial labeling. 2.2.2 How Much Information is in a Label? By comparing the uncertainty before and after including a new label from an annotator, we can measure the reduction in uncertainty provided by the annotator’s label. By considering the expected reduction in uncertainty due to observing a label from an annotator, we can quantify how much information the label is expected to provide. Entropy. The information-theoretic notion of entropy makes the notion of uncertainty precise (Cover and Thomas, 1991). If Zi is the random variable corresponding to the true label of word instance i with K possible labels and probability mass function pZi, its entropy is H[Zi] = −EKk=1 pZi(k) log pZi(k). Conditional Entropy. Consider a label Yn = k&apos; from annotator j = jjn for item i = iin. The entropy of Zi conditioned on the observed label is H[Zi|Yn=k&apos;] = − EKk=1 pZi|Yn(k|k&apos;) log pZi|Yn(k|k&apos;). Conditional entropy is defined by the expected entropy of Zi after observing Yn, H[Zi|Yn] = EKk,=1 pYn(k&apos;) H[Zi|Yn=k&apos;]. Conditional entropy can be generalized in the obvious way to condition on more than one observed</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm.</title>
<date>1979</date>
<journal>Journal of the Royal Statistical Society. Series C (Applied Statistics),</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="2154" citStr="Dawid and Skene, 1979" startWordPosition="330" endWordPosition="333">led by many annotators. A probabilistic model to fit many annotators’ observed labels produces much more information about the annotated corpus. In particular, there will be a confidence estimate for each ground truth label. Probabilistic models of agreement and goldstandard inference have been used in psychometrics and marketing since the 1950s (e.g., IRT models or Bradley-Terry models) and in epidemiology since the 1970s (e.g., diagnostic disease prevalence models). More recently, crowdsourcing has motivated their application to data annotation for machine learning. The model we apply here (Dawid and Skene, 1979) assumes that annotators differ from one another in their accuracy at identifying the true label values, and that these true values occur at certain rates (their prevalence). To contrast the two approaches to creation of an annotated corpus, we present a case study of word sense annotation. The items that were annotated are occurrences of words in their sentence contexts, and each label is a WordNet sense (Miller, 1995). Each item has sense labels from up to twenty-five different annotators, collected through crowdsourcing. Application of an annotation model does not require this many labels p</context>
<context position="13480" citStr="Dawid and Skene (1979)" startWordPosition="2198" endWordPosition="2201"> sample of size N, a large number of samples of size N are drawn randomly with replacement from the original sample, the statistic of interest is computed for each random draw, and the mean f 1.96 standard deviations gives the estimated value and its approximate 95% confidence interval. 2.2 A Probabilistic Annotation Model A probabilistic model provides a recipe to randomly “generate” a dataset from a set of model parameters and constants.4,5 The utility of such a model lies in its ability to support meaningful inferences from data, such as an estimate of the true prevalence of each category. Dawid and Skene (1979) proposed a model to determine a consensus among patient histories taken by multiple doctors. Inference is driven by accuracies and biases estimated for each annotator on a per-category basis. A graphical sketch of the model is shown in Figure 1. Let K be the number of possible labels or categories for an item, I the number of items to annotate, J the number of annotators, and N the total number of labels provided by annotators, where each annotator may label each instance zero or more times. Because the data is not a simple I x J data matrix where every annotator labels every item exactly onc</context>
<context position="45737" citStr="Dawid and Skene, 1979" startWordPosition="7888" endWordPosition="7891">s will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The Kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>di Eugenio, Glass, 2004</marker>
<rawString>Barbara di Eugenio and Michael Glass. 2004. The Kappa statistic: A second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara di Eugenio</author>
</authors>
<title>On the usage of Kappa to evaluate agreement on coding tasks.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>di Eugenio, 2000</marker>
<rawString>Barbara di Eugenio. 2000. On the usage of Kappa to evaluate agreement on coding tasks. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy.</title>
<date>1986</date>
<journal>Statistical Science,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="12840" citStr="Efron and Tibshirani, 1986" startWordPosition="2090" endWordPosition="2093">umptions of no annotator bias or item-level effects. Given a sample of 100 annotations, if the true gold standard categories were known (as opposed to being themselves estimated as 313 in our setup here), an annotator getting 80 out of 100 items correct would produce a 95% interval for accuracy of roughly (74%,86%).3 Agreement statistics have even wider error bounds. This introduces enough uncertainty to span the rather arbitrary decision boundaries for acceptability employed for interannotator agreement statistics. Note that bootstrapping is a reliable method to compute confidence intervals (Efron and Tibshirani, 1986). Briefly, given a sample of size N, a large number of samples of size N are drawn randomly with replacement from the original sample, the statistic of interest is computed for each random draw, and the mean f 1.96 standard deviations gives the estimated value and its approximate 95% confidence interval. 2.2 A Probabilistic Annotation Model A probabilistic model provides a recipe to randomly “generate” a dataset from a set of model parameters and constants.4,5 The utility of such a model lies in its ability to support meaningful inferences from data, such as an estimate of the true prevalence </context>
</contexts>
<marker>Efron, Tibshirani, 1986</marker>
<rawString>B. Efron and R. Tibshirani. 1986. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science, 1(1):54–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvan R Feinstein</author>
<author>Domenic V Cicchetti</author>
</authors>
<title>High agreement but low Kappa: I. The problems of two paradoxes.</title>
<date>1990</date>
<journal>Journal of Clinical Epidemiology,</journal>
<volume>43</volume>
<issue>6</issue>
<pages>549</pages>
<contexts>
<context position="10904" citStr="Feinstein and Cicchetti (1990)" startWordPosition="1792" endWordPosition="1796">an compare to a voted consensus or average over multiple pairwise agreements. (2) In agreement-based analyses, two wrongs make a right in the sense that if two annotators both make the same mistake, they agree. If annotators are 80% accurate on a binary task, then chance agreement on the wrong category occurs at a 4% rate. (3) Chance-adjusted agreement reduces to simple agreement as chance agreement approaches zero. When chance agreement is high, even high-accuracy annotators can have low chance-adjusted agreement, as when the data is skewed towards a few values, a typical case for NLP tasks. Feinstein and Cicchetti (1990) referred to this as the paradox of κ (see section 6). For example, in a binary task with 95% prevalence of one category, two 90% accurate annotators would have negative chanceadjusted agreements of 0.9−(.952+.052) = −.053. 1−(.952+.052) Thus high chance-adjusted interannotator agreement is not a necessary condition for a high-quality corpus. An alternative metric discussed in Section 6 addresses skewed prevalence of label values, but has not been adopted in the NLP community (Gwet, 2008). (4) Interannotator agreement statistics implicitly assume annotators are unbiased; if they are biased in </context>
<context position="45063" citStr="Feinstein and Cicchetti (1990)" startWordPosition="7778" endWordPosition="7781">hat some sets of sense labels led to greater agreement or a higher proportion of high confidence labels. In many cases, results for the words with fewer high confidence labels could be improved by revising the sense inventories, as suggested by the examples with meet (verb) and date (noun). 6 Related Work Alternative metrics to measure association of raters on binary data have been proposed to overcome deficiencies in κ when there is data skew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard c</context>
</contexts>
<marker>Feinstein, Cicchetti, 1990</marker>
<rawString>Alvan R. Feinstein and Domenic V. Cicchetti. 1990. High agreement but low Kappa: I. The problems of two paradoxes. Journal of Clinical Epidemiology, 43(6):543 – 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="48351" citStr="Forman, 2003" startWordPosition="8319" endWordPosition="8321">licit, which provides a very simple estimate of expected accuracy for a given number of labels. This essentially provides a statistical power calculation for annotation tasks. Because it is explicitly designed to measure reduction in uncertainty, mutual information should be the ideal choice for guiding such active labeling (MacKay, 1992). Such a strategy of selecting features with maximal mutual information has proven effective in greedy featureselection strategies for classifiers, despite the fact that the objective function was classification accuracy, not entropy (Yang and Pedersen, 1997; Forman, 2003). 7 Conclusion Interannotator agreement applies to a set of annotations, and provides no information about individual instances. When two or more annotators have very high interannotator agreement on a task, unless they have perfect accuracy, there will be instances where they agreed incorrectly, and no way to predict which instances these are. Moreover, for many semantic annotation tasks, high κ is impractical. In addition, there is often a pragmatic dimension where labels represent community-established conventions of usage. In such cases, no one individual can reliably assign labels because</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilem Li Gwet</author>
</authors>
<title>Computing inter-rater reliability and its variance in the presence of high agreement.</title>
<date>2008</date>
<journal>British Journal of Mathematical and Statistical Psychology,</journal>
<volume>61</volume>
<issue>1</issue>
<contexts>
<context position="11397" citStr="Gwet, 2008" startWordPosition="1872" endWordPosition="1873">reement, as when the data is skewed towards a few values, a typical case for NLP tasks. Feinstein and Cicchetti (1990) referred to this as the paradox of κ (see section 6). For example, in a binary task with 95% prevalence of one category, two 90% accurate annotators would have negative chanceadjusted agreements of 0.9−(.952+.052) = −.053. 1−(.952+.052) Thus high chance-adjusted interannotator agreement is not a necessary condition for a high-quality corpus. An alternative metric discussed in Section 6 addresses skewed prevalence of label values, but has not been adopted in the NLP community (Gwet, 2008). (4) Interannotator agreement statistics implicitly assume annotators are unbiased; if they are biased in the same direction, e.g., the most prevalent category, then agreement is an overestimate of their accuracy. In the extreme case, in a binary labeling task, two adversarial annotators who always provide the wrong answer have a chance-adjusted agreement of 100%. (5) Item-level effects such as difficulty can inflate levels of agreement-in-error. For example, in a named-entity corpus one of the co-authors helped collect for MUC, hard-to-identify names have correlated false negatives among ann</context>
<context position="45281" citStr="Gwet (2008)" startWordPosition="7817" endWordPosition="7818">sted by the examples with meet (verb) and date (noun). 6 Related Work Alternative metrics to measure association of raters on binary data have been proposed to overcome deficiencies in κ when there is data skew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many othe</context>
</contexts>
<marker>Gwet, 2008</marker>
<rawString>Kilem Li Gwet. 2008. Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Holley</author>
<author>J P Guildford</author>
</authors>
<date>1964</date>
<booktitle>A note on the G index of agreement. Educational and Psychological Measurement,</booktitle>
<pages>24--749</pages>
<contexts>
<context position="44920" citStr="Holley and Guildford, 1964" startWordPosition="7759" endWordPosition="7762"> building such systems. Each word here has a distinct set of classes, and the results from both the trained annotators and model indicate that some sets of sense labels led to greater agreement or a higher proportion of high confidence labels. In many cases, results for the words with fewer high confidence labels could be improved by revising the sense inventories, as suggested by the examples with meet (verb) and date (noun). 6 Related Work Alternative metrics to measure association of raters on binary data have been proposed to overcome deficiencies in κ when there is data skew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a rel</context>
</contexts>
<marker>Holley, Guildford, 1964</marker>
<rawString>J. W. Holley and J. P. Guildford. 1964. A note on the G index of agreement. Educational and Psychological Measurement, 24:749–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Tayler Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1120--1130</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="46847" citStr="Hovy et al. (2013)" startWordPosition="8071" endWordPosition="8074">ard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards, including an example of word sense. Both works estimate the Dawid and Skene model using supervised gold-standard category data, which allows direct estimation of annotator accuracy and bias. Hovy et al. (2013) recently presented a much 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 mutual_info Figure 8: Histograms of mutual information estimates for the four example words; trained annotators are in the top row and Turkers in the bottom. simpler model to filter out spam annotators. Crowdsourcing is now so widespread that NAACL 2010 sponsored a workshop on “Creating Speech and Language Data with Amazon’s Mechanical Turk” and in 2011, TREC added a crowdsourcing track. Active learning is an alternative method to annotate corpora, thus the Troia project (</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Tayler Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1120–1130, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
<author>Foster Provost</author>
<author>Jing Wang</author>
</authors>
<title>Quality management on Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP 2010,</booktitle>
<pages>64--67</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="47470" citStr="Ipeirotis et al., 2010" startWordPosition="8179" endWordPosition="8182"> recently presented a much 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 mutual_info Figure 8: Histograms of mutual information estimates for the four example words; trained annotators are in the top row and Turkers in the bottom. simpler model to filter out spam annotators. Crowdsourcing is now so widespread that NAACL 2010 sponsored a workshop on “Creating Speech and Language Data with Amazon’s Mechanical Turk” and in 2011, TREC added a crowdsourcing track. Active learning is an alternative method to annotate corpora, thus the Troia project (Ipeirotis et al., 2010) is a web service implementation of a maximum a posteriori estimator for the Dawid and Skene model, with a decision-theoretic module for active learning to select the next item to label. They draw on the Sheng et al. (2008) model to actively select the next label to elicit, which provides a very simple estimate of expected accuracy for a given number of labels. This essentially provides a statistical power calculation for annotation tasks. Because it is explicitly designed to measure reduction in uncertainty, mutual information should be the ideal choice for guiding such active labeling (MacKa</context>
</contexts>
<marker>Ipeirotis, Provost, Wang, 2010</marker>
<rawString>Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality management on Amazon Mechanical Turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP 2010, pages 64–67, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="45509" citStr="Klebanov and Beigman (2009)" startWordPosition="7851" endWordPosition="7854">The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistic</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational Linguistics, 35(4):495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content analysis: An introduction to its methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="9989" citStr="Krippendorff, 1980" startWordPosition="1644" endWordPosition="1645">at random for two annotators will agree. There are two common ways to define this draw. Cohen’s κ statistic (Cohen, 1960) assumes each annotator draws uniformly at random from her set of labels. Letting ψj,k = I EIi=1 I(yi,j = k) be the proportion of the label k in annotator j’s labels, this notion of chance agreement for a pair of annotators m, n is estimated as the product of their proportions ψ: EK Cm,n /&apos; /&apos; — �k=1 4&apos;m,k X 4&apos;n,k. Krippendorff’s α, another chance-adjusted metric in wide use, assumes each annotator draws uniformly at random from the pooled set of labels from all annotators (Krippendorff, 1980). Letting φk be the proportion of label k in the entire set of labels, this alternative estimate, C0m,n = EKk=1 φ2k, does not depend on the identity of the annotators m and n. Agreement coefficients suffer from multiple shortcomings. (1) They are intrinsically pairwise, although one can compare to a voted consensus or average over multiple pairwise agreements. (2) In agreement-based analyses, two wrongs make a right in the sense that if two annotators both make the same mistake, they agree. If annotators are 80% accurate on a binary task, then chance agreement on the wrong category occurs at a</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Information-based objective functions for active data selection.</title>
<date>1992</date>
<journal>Neural Computation,</journal>
<pages>4--590</pages>
<contexts>
<context position="48078" citStr="MacKay, 1992" startWordPosition="8280" endWordPosition="8281">2010) is a web service implementation of a maximum a posteriori estimator for the Dawid and Skene model, with a decision-theoretic module for active learning to select the next item to label. They draw on the Sheng et al. (2008) model to actively select the next label to elicit, which provides a very simple estimate of expected accuracy for a given number of labels. This essentially provides a statistical power calculation for annotation tasks. Because it is explicitly designed to measure reduction in uncertainty, mutual information should be the ideal choice for guiding such active labeling (MacKay, 1992). Such a strategy of selecting features with maximal mutual information has proven effective in greedy featureselection strategies for classifiers, despite the fact that the objective function was classification accuracy, not entropy (Yang and Pedersen, 1997; Forman, 2003). 7 Conclusion Interannotator agreement applies to a set of annotations, and provides no information about individual instances. When two or more annotators have very high interannotator agreement on a task, unless they have perfect accuracy, there will be instances where they agreed incorrectly, and no way to predict which i</context>
</contexts>
<marker>MacKay, 1992</marker>
<rawString>David J. C. MacKay. 1992. Information-based objective functions for active data selection. Neural Computation, 4:590–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Matthews</author>
</authors>
<title>Comparison of the predicted and observed secondary structure of t4 phage lysozyme.</title>
<date>1975</date>
<journal>Biochimica et Biophysica Acta (BBA) - Protein Structure,</journal>
<volume>405</volume>
<issue>2</issue>
<contexts>
<context position="45031" citStr="Matthews, 1975" startWordPosition="7776" endWordPosition="7777"> model indicate that some sets of sense labels led to greater agreement or a higher proportion of high confidence labels. In many cases, results for the words with fewer high confidence labels could be improved by revising the sense inventories, as suggested by the examples with meet (verb) and date (noun). 6 Related Work Alternative metrics to measure association of raters on binary data have been proposed to overcome deficiencies in κ when there is data skew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to e</context>
</contexts>
<marker>Matthews, 1975</marker>
<rawString>B. W. Matthews. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA) - Protein Structure, 405(2):442–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2577" citStr="Miller, 1995" startWordPosition="401" endWordPosition="402">, diagnostic disease prevalence models). More recently, crowdsourcing has motivated their application to data annotation for machine learning. The model we apply here (Dawid and Skene, 1979) assumes that annotators differ from one another in their accuracy at identifying the true label values, and that these true values occur at certain rates (their prevalence). To contrast the two approaches to creation of an annotated corpus, we present a case study of word sense annotation. The items that were annotated are occurrences of words in their sentence contexts, and each label is a WordNet sense (Miller, 1995). Each item has sense labels from up to twenty-five different annotators, collected through crowdsourcing. Application of an annotation model does not require this many labels per item, and crowdsourced annotation data does not require a probabilistic model. The case study, however, shows how the two benefit each other. MASC (Manually Annotated Sub-Corpus of the Open American National Corpus) contains a subsidiary word sense sentence corpus that consists of approximately one thousand sentences per word for 116 words. Word senses were annotated in their sentence contexts using WordNet sense lab</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. A lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Collin F Baker</author>
<author>Christiane Fellbaum</author>
<author>Nancy Ide</author>
</authors>
<title>The MASC word sense corpus.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>3025--3030</pages>
<location>Istanbul,</location>
<contexts>
<context position="3324" citStr="Passonneau et al., 2012" startWordPosition="513" endWordPosition="516"> annotation model does not require this many labels per item, and crowdsourced annotation data does not require a probabilistic model. The case study, however, shows how the two benefit each other. MASC (Manually Annotated Sub-Corpus of the Open American National Corpus) contains a subsidiary word sense sentence corpus that consists of approximately one thousand sentences per word for 116 words. Word senses were annotated in their sentence contexts using WordNet sense labels. Chanceadjusted agreement levels ranged from very high to chance levels, with similar variation for pairwise agreement (Passonneau et al., 2012a). As a result, the annotations for certain words appear to be low 311 Transactions of the Association for Computational Linguistics, 2 (2014) 311–326. Action Editor: Chris Callison-Burch. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. quality.1 Our case study shows how we created a more reliable word sense corpus for a randomly selected subset of 45 of the same words, through crowdsourcing and application of the Dawid and Skene model. The model yields a certainty measure for each labeled instance. For most instances, the certainty of th</context>
<context position="26474" citStr="Passonneau et al., 2012" startWordPosition="4553" endWordPosition="4556">lexity, especially with multivariate outputs, would distract from our main point in contrasting model-based inference with agreement statistics. 3 Two Data Collections 3.1 MASC Word Sense Sentence Corpus To motivate our case study, we briefly discuss some of the limitations of the MASC word sense sentence corpus, which is an addendum to the MASC corpus.8 For convenience, we refer here to the word sense sentence corpus as the MASC corpus. This is a 1.3 million word corpus with approximately one thousand sentences per word, for 116 words nearly evenly balanced among nouns, adjectives and verbs (Passonneau et al., 2012a). Each sentence is drawn from the MASC corpus or the Open American National Corpus, exemplifies at least one of the 116 MASC words, and has been annotated by trained annotators who used WordNet senses as annotation labels. The annotation process is described in detail in (Passonneau et al., 2012a; Passonneau et al., 2012b). The annotators were college students from Vassar, Barnard, and Columbia who were given general training in the annotation process, then were trained together on each word with a sample of fifty sentences, which included discussion with Christiane Fellbaum, one of the desi</context>
<context position="28025" citStr="Passonneau et al., 2012" startWordPosition="4802" endWordPosition="4806">assess inter-annotator reliability. Figure 3 shows 45 randomly selected MASC words that were re-annotated using crowdsourcing. Shown are the part of speech, the number of WordNet senses, the number of senses used by annotators, the α value, and pairwise agreement. While the MASC word sense data demonstrates that annotators can agree on words with many senses, there are many words with low agreement, and correspondingly questionable ground truth labels. There is no correlation between the agreement and number of 8http://www.anc.org/data/masc/ 317 available senses, or senses used by annotators (Passonneau et al., 2012a). Due to limited resources, the project deviated from best practice in having only a single round of annotation per word, and no iteration to achieve an agreement threshold. All annotators, however, had at least two phases of training, and most annotated several rounds. Below we use mutual information to show that the quality of the crowdsourced labels is equivalent to or superior than labels from the trained MASC annotators. 3.2 Crowdsourced Word Sense Annotation To collect the data, we relied on Amazon Mechanical Turk, a crowdsourcing marketplace that is used extensively in the NLP communi</context>
</contexts>
<marker>Passonneau, Baker, Fellbaum, Ide, 2012</marker>
<rawString>Rebecca J. Passonneau, Collin F. Baker, Christiane Fellbaum, and Nancy Ide. 2012a. The MASC word sense corpus. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3025–3030, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Vikas Bhardwaj</author>
<author>Ansaf SallebAouissi</author>
<author>Nancy Ide</author>
</authors>
<title>Multiplicity and word sense: Evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>46--2</pages>
<contexts>
<context position="3324" citStr="Passonneau et al., 2012" startWordPosition="513" endWordPosition="516"> annotation model does not require this many labels per item, and crowdsourced annotation data does not require a probabilistic model. The case study, however, shows how the two benefit each other. MASC (Manually Annotated Sub-Corpus of the Open American National Corpus) contains a subsidiary word sense sentence corpus that consists of approximately one thousand sentences per word for 116 words. Word senses were annotated in their sentence contexts using WordNet sense labels. Chanceadjusted agreement levels ranged from very high to chance levels, with similar variation for pairwise agreement (Passonneau et al., 2012a). As a result, the annotations for certain words appear to be low 311 Transactions of the Association for Computational Linguistics, 2 (2014) 311–326. Action Editor: Chris Callison-Burch. Submitted 2/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. quality.1 Our case study shows how we created a more reliable word sense corpus for a randomly selected subset of 45 of the same words, through crowdsourcing and application of the Dawid and Skene model. The model yields a certainty measure for each labeled instance. For most instances, the certainty of th</context>
<context position="26474" citStr="Passonneau et al., 2012" startWordPosition="4553" endWordPosition="4556">lexity, especially with multivariate outputs, would distract from our main point in contrasting model-based inference with agreement statistics. 3 Two Data Collections 3.1 MASC Word Sense Sentence Corpus To motivate our case study, we briefly discuss some of the limitations of the MASC word sense sentence corpus, which is an addendum to the MASC corpus.8 For convenience, we refer here to the word sense sentence corpus as the MASC corpus. This is a 1.3 million word corpus with approximately one thousand sentences per word, for 116 words nearly evenly balanced among nouns, adjectives and verbs (Passonneau et al., 2012a). Each sentence is drawn from the MASC corpus or the Open American National Corpus, exemplifies at least one of the 116 MASC words, and has been annotated by trained annotators who used WordNet senses as annotation labels. The annotation process is described in detail in (Passonneau et al., 2012a; Passonneau et al., 2012b). The annotators were college students from Vassar, Barnard, and Columbia who were given general training in the annotation process, then were trained together on each word with a sample of fifty sentences, which included discussion with Christiane Fellbaum, one of the desi</context>
<context position="28025" citStr="Passonneau et al., 2012" startWordPosition="4802" endWordPosition="4806">assess inter-annotator reliability. Figure 3 shows 45 randomly selected MASC words that were re-annotated using crowdsourcing. Shown are the part of speech, the number of WordNet senses, the number of senses used by annotators, the α value, and pairwise agreement. While the MASC word sense data demonstrates that annotators can agree on words with many senses, there are many words with low agreement, and correspondingly questionable ground truth labels. There is no correlation between the agreement and number of 8http://www.anc.org/data/masc/ 317 available senses, or senses used by annotators (Passonneau et al., 2012a). Due to limited resources, the project deviated from best practice in having only a single round of annotation per word, and no iteration to achieve an agreement threshold. All annotators, however, had at least two phases of training, and most annotated several rounds. Below we use mutual information to show that the quality of the crowdsourced labels is equivalent to or superior than labels from the trained MASC annotators. 3.2 Crowdsourced Word Sense Annotation To collect the data, we relied on Amazon Mechanical Turk, a crowdsourcing marketplace that is used extensively in the NLP communi</context>
</contexts>
<marker>Passonneau, Bhardwaj, SallebAouissi, Ide, 2012</marker>
<rawString>Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf SallebAouissi, and Nancy Ide. 2012b. Multiplicity and word sense: Evaluating and learning from multiply labeled word sense annotations. Language Resources and Evaluation, 46(2):219–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>1322</pages>
<contexts>
<context position="46036" citStr="Raykar et al. (2010)" startWordPosition="7943" endWordPosition="7946">otators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. Journal of Machine Learning Research, 11:1297– 1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Rogers</author>
<author>Mark Girolami</author>
<author>Tamara Polajnar</author>
</authors>
<title>Semi-parametric analysis of multi-rater data.</title>
<date>2010</date>
<journal>Statistical Computing,</journal>
<pages>20--317</pages>
<contexts>
<context position="46010" citStr="Rogers et al. (2010)" startWordPosition="7938" endWordPosition="7941">ement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinc</context>
</contexts>
<marker>Rogers, Girolami, Polajnar, 2010</marker>
<rawString>Simon Rogers, Mark Girolami, and Tamara Polajnar. 2010. Semi-parametric analysis of multi-rater data. Statistical Computing, 20:317–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrey Rzhetsky</author>
<author>Hagit Shatkay</author>
<author>W John Wilbur</author>
</authors>
<title>How to get the most out of your curation effort.</title>
<date>2009</date>
<journal>PLoS Computational Biology.,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="19918" citStr="Rzhetsky et al., 2009" startWordPosition="3398" endWordPosition="3401"> the true label. For example, in a binary task, a positively biased annotator will return relatively more false positives and relatively fewer false negatives compared to an unbiased one. As shown in Section 4.2, our word sense task had fairly small estimated biases toward the high-frequency senses in most cases. Other tasks, such as ordinal ranking of author certainty for assertions, show systematically biased annotators. Annotators may be biased toward one end of an ordinal scale, or toward the center. These kinds of biases are apparent in the annotators in the annotation task described in (Rzhetsky et al., 2009), where biologists labeled sentences in biomedical research articles on a 1 to 7 scale of polarity and certainty. Adversarial Annotators. An adversarial annotator who always returns the wrong answer exhibits an extreme bias. In a binary annotation case, it is clear how perfectly adversarial answers provide the same information as perfectly cooperative answers. Although it is possible to estimate the response matrix of an adversarial annotator, if too many of the annotators are adversarial, the Dawid and Skene model θ1= 315 cannot separate the truth from the lies. None of the data sets we have </context>
<context position="46155" citStr="Rzhetsky et al. (2009)" startWordPosition="7960" endWordPosition="7963">on that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards, including an example of word sense. Both works estimate the Dawid and Skene model using supervised gold-standard cate</context>
</contexts>
<marker>Rzhetsky, Shatkay, Wilbur, 2009</marker>
<rawString>Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur. 2009. How to get the most out of your curation effort. PLoS Computational Biology., 5(5):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get another label? Improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fourteenth ACM International Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="47693" citStr="Sheng et al. (2008)" startWordPosition="8219" endWordPosition="8222">otators are in the top row and Turkers in the bottom. simpler model to filter out spam annotators. Crowdsourcing is now so widespread that NAACL 2010 sponsored a workshop on “Creating Speech and Language Data with Amazon’s Mechanical Turk” and in 2011, TREC added a crowdsourcing track. Active learning is an alternative method to annotate corpora, thus the Troia project (Ipeirotis et al., 2010) is a web service implementation of a maximum a posteriori estimator for the Dawid and Skene model, with a decision-theoretic module for active learning to select the next item to label. They draw on the Sheng et al. (2008) model to actively select the next label to elicit, which provides a very simple estimate of expected accuracy for a given number of labels. This essentially provides a statistical power calculation for annotation tasks. Because it is explicitly designed to measure reduction in uncertainty, mutual information should be the ideal choice for guiding such active labeling (MacKay, 1992). Such a strategy of selecting features with maximal mutual information has proven effective in greedy featureselection strategies for classifiers, despite the fact that the objective function was classification acc</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? Improving data quality and data mining using multiple, noisy labelers. In Proceedings of the Fourteenth ACM International Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Michael Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjectively-labeled images of Venus.</title>
<date>1995</date>
<booktitle>In Advances in Neural Information Processing Systems 7,</booktitle>
<pages>1085--1092</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="45967" citStr="Smyth et al., 1995" startWordPosition="7930" endWordPosition="7933">t based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations pro</context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjectively-labeled images of Venus. In Advances in Neural Information Processing Systems 7, pages 1085–1092. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>254--263</pages>
<location>Honolulu.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 254–263, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Vegelius</author>
</authors>
<title>Significance tests for the G-index. Educational and Psychological Measurement,</title>
<date>1981</date>
<pages>41--99</pages>
<contexts>
<context position="44937" citStr="Vegelius, 1981" startWordPosition="7763" endWordPosition="7764"> word here has a distinct set of classes, and the results from both the trained annotators and model indicate that some sets of sense labels led to greater agreement or a higher proportion of high confidence labels. In many cases, results for the words with fewer high confidence labels could be improved by revising the sense inventories, as suggested by the examples with meet (verb) and date (noun). 6 Related Work Alternative metrics to measure association of raters on binary data have been proposed to overcome deficiencies in κ when there is data skew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent 322 class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption t</context>
</contexts>
<marker>Vegelius, 1981</marker>
<rawString>Jan Vegelius. 1981. Significance tests for the G-index. Educational and Psychological Measurement, 41:99– 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22,</booktitle>
<pages>2035--2043</pages>
<editor>In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors,</editor>
<contexts>
<context position="46183" citStr="Whitehill et al. (2009)" startWordPosition="7965" endWordPosition="7969"> easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistically annotated corpora. Rzhetsky et al. (2009) and Whitehill et al. (2009) estimate annotation models without gold-standard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards, including an example of word sense. Both works estimate the Dawid and Skene model using supervised gold-standard category data, which allows dire</context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2035–2043, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>412--420</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="48336" citStr="Yang and Pedersen, 1997" startWordPosition="8315" endWordPosition="8318">elect the next label to elicit, which provides a very simple estimate of expected accuracy for a given number of labels. This essentially provides a statistical power calculation for annotation tasks. Because it is explicitly designed to measure reduction in uncertainty, mutual information should be the ideal choice for guiding such active labeling (MacKay, 1992). Such a strategy of selecting features with maximal mutual information has proven effective in greedy featureselection strategies for classifiers, despite the fact that the objective function was classification accuracy, not entropy (Yang and Pedersen, 1997; Forman, 2003). 7 Conclusion Interannotator agreement applies to a set of annotations, and provides no information about individual instances. When two or more annotators have very high interannotator agreement on a task, unless they have perfect accuracy, there will be instances where they agreed incorrectly, and no way to predict which instances these are. Moreover, for many semantic annotation tasks, high κ is impractical. In addition, there is often a pragmatic dimension where labels represent community-established conventions of usage. In such cases, no one individual can reliably assign</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pages 412– 420, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
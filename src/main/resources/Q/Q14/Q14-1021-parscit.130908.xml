<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.839718">
Entity Linking on Microblogs with Spatial and Temporal Signals
</title>
<author confidence="0.999326">
Yuan Fang ∗ †
</author>
<affiliation confidence="0.999165">
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.8590005">
201 N. Goodwin Avenue
Urbana, IL 61801, USA
</address>
<email confidence="0.998415">
fang2@illinois.edu
</email>
<author confidence="0.915607">
Ming-Wei Chang
</author>
<affiliation confidence="0.902516">
Microsoft Research
</affiliation>
<address confidence="0.953239">
1 Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999439">
minchang@microsoft.com
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999486888888889">
Microblogs present an excellent opportunity
for monitoring and analyzing world happen-
ings. Given that words are often ambiguous,
entity linking becomes a crucial step towards
understanding microblogs. In this paper, we
re-examine the problem of entity linking on
microblogs. We first observe that spatiotem-
poral (i.e., spatial and temporal) signals play
a key role, but they are not utilized in exist-
ing approaches. Thus, we propose a novel en-
tity linking framework that incorporates spa-
tiotemporal signals through a weakly super-
vised process. Using entity annotations1 on
real-world data, our experiments show that the
spatiotemporal model improves F1 by more
than 10 points over existing systems. Finally,
we present a qualitative study to visualize the
effectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9694838">
Microblogging services provide an immense plat-
form for intelligence gathering, such as market re-
search (Asur and Huberman, 2010), disaster mon-
itoring (Sadilek et al., 2012) and political analy-
sis (Tumasjan et al., 2010). Extracting entities from
microblogs is an essential step for many such ap-
plications. Suppose that a marketing firm is inter-
ested in the sentiment about some product on Twit-
ter. However, any sentiment analysis is potentially
∗Work done during an internship at Microsoft Research.
</bodyText>
<footnote confidence="0.93691425">
†Also affiliated with Agency for Science, Technology and
Research, 1 Fusionopolis Way, Singapore 138632.
1Can be downloaded at http://research.microsoft.com/en-
us/downloads/84ac9d88-c353-4059-97a4-87d129db0464/.
</footnote>
<bodyText confidence="0.999819558823529">
misleading if we cannot correctly retrieve the tweets
mentioning the target product.
To retrieve tweets for a given entity (e.g., a prod-
uct or an organization), a straightforward approach
is to formulate a keyword query. However, simple
keyword matching is largely ineffective, since key-
words are often ambiguous. For instance, “spurs”
can refer to two distinct sports teams (San Antonio
Spurs, which is a basketball team in the US, and Tot-
tenham Hotspur F.C., which is a soccer team in the
UK), besides being a non-entity verb or noun. Thus,
retrieving tweets via keyword queries inevitably
mixes different entities.
Given the ambiguity of keywords, in this paper,
we study the task of entity linking (Bunescu and
Pasca, 2006) on microblogs. As input, we are given
a short message (e.g., a tweet) and an entity database
(e.g., Wikipedia where each article is an entity). As
output, we map every surface form (e.g., “spurs”)
in the message to an entity (e.g., San Antonio Spurs)
or to ∅ (i.e., a non-entity). This task is particularly
challenging for microblogs due to their short, noisy
and colloquial nature. Fortunately, we also observe
two new opportunities, which are often missing in
traditional data.
First, microblogs usually embed rich meta-data,
most notably spatiotemporal (i.e., spatial and tem-
poral) signals. Specifically, all tweets are associ-
ated with a timestamp, and many can be mapped
to a location. We observe that entity priors often
change across time and location. Consider the ex-
ample tweets in Fig. 1. To understand the meaning
of the word “spurs,” it is challenging to only rely on
the textual features. However, with the help of lo-
</bodyText>
<page confidence="0.975569">
259
</page>
<note confidence="0.77764975">
Transactions of the Association for Computational Linguistics, 2 (2014) 259–272. Action Editor: Noah Smith.
Submitted 11/2013; Revised 3/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
Tweets mentioning Tottenham Hotspur F.C., a soccer team in the UK
UK: Who scored the 2 goals for spurs I had to go out or 5 mins and missed it all
UK: Defoe again 3 nil to spurs how you doing @USER at Arsenal
Tweets mentioning San Antonio Spurs, a basketball team in the US
US: @USER who cares, nobody wanna see the spurs play. Remember they’re boring. He’s a great coach ...
US: The fine on the spurs for 250k is ridiculous! I’m a laker fan; still think this is too much
</note>
<figureCaption confidence="0.994487">
Figure 1: Examples illustrating the importance of spatiotemporal signals to entity linking. The “UK” or “US”
tag indicates the location of the posting user. Based on deep semantic understanding of lexical items and additional
resources (such as the entire conversation and attached URL), the annotators can label “spurs” in the first two tweets as
the soccer team, and in the other two as the basketball team. While most entity linking systems handle merely textual
features, the task obviously becomes easier with location information.
</figureCaption>
<bodyText confidence="0.981797583333333">
cation information, the two teams can be easily dis-
tinguished. In particular, based on our labeled data,
San Antonio Spurs accounts for 91% of the “spurs”
tweets in the US, whereas it only accounts for 8%
in the UK. Similar trends also exist across different
time periods. Therefore, exploiting spatiotemporal
signals is crucial to entity linking.
Second, we can leverage the predictions from the
tweets carrying an easier surface form to help link
the tweets carrying an ambiguous surface form at a
similar time or location. The intuition is that cer-
tain surface forms are easier to disambiguate than
others. For instance, while only saying “spurs” in
a tweet can be quite ambiguous, many other tweets
around the same time or location might carry an eas-
ier surface form, such as “SA spurs” or “san antonio
spurs,” which are not ambiguous.
In this work, we focus on the offline mining set-
ting, where a corpus has been collected for analy-
sis offline. Developing algorithms for the streaming
setting is an important direction of future work. Our
contributions are summarized as follows:
• We propose a spatiotemporal framework for en-
tity linking on microblogs. To our best knowl-
edge, this is the first work to model spatiotempo-
ral signals for entity linking.
• We demonstrate the effectiveness of our frame-
work through extensive quantitative experiments.
In particular, we improve F1 by more than 10
points over the existing state of the art.
• We point out that entity linking should be evalu-
ated for both information extraction and retrieval
needs. In the former, we evaluate the extraction of
entities from tweets, while in the latter we evalu-
ate the retrieval of tweets for a query entity. A
qualitative study is also presented for the latter.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999243514285714">
Earlier research on entity linking (Bunescu and
Pasca, 2006; Cucerzan, 2007; Milne and Witten,
2008) has been focused on well-written documents
such as news and encyclopedia articles. The TAC
KBP track (Ji et al., 2010; Ji and Grishman, 2011)
also includes an entity linking task with a slightly
different setting—to link a given mention based on a
background document. These efforts exploit the sta-
tistical power aggregated by a semantic knowledge
bases, most notably Wikipedia. Various features in
the target document are also leveraged (Han et al.,
2011; Kulkarni et al., 2009; Hoffart et al., 2011;
Shen et al., 2012) to assess both local compatibility
and global coherence.
These techniques have also been adapted and tai-
lored to short texts including tweets, for the prob-
lem of entity linking (Ferragina and Scaiella, 2010;
Meij et al., 2012; Guo et al., 2013) as well as the
related problem of named entity recognition (NER)
(Ritter et al., 2011; Li et al., 2012a). However,
given the short and noisy nature of microblogs, these
approaches, which largely depend on textual fea-
tures, often result in unsatisfactory performance.
Fortunately, additional non-textual meta-data in mi-
croblogs can often help. A recent study (Shen et
al., 2013) improves entity linking by utilizing user
account information, based on the intuition that all
tweets posted by the same user share an underlying
topic distribution.
Inspired by the use of non-textual features, we
explore the spatiotemporal signals associated with
tweets. Although the spatiotemporal aspect of so-
cial media has been studied in many papers, includ-
ing temporal cycle tracking (Leskovec et al., 2009),
spatial object matching (Dalvi et al., 2012), min-
</bodyText>
<page confidence="0.988848">
260
</page>
<bodyText confidence="0.999110464285714">
ing emerging topics (Mei et al., 2006; Cataldi et al.,
2010; Yin et al., 2011), event monitoring (Sakaki et
al., 2010; Xu et al., 2012), and identifying geograph-
ical linguistic variations (Eisenstein et al., 2010;
Wing and Baldridge, 2011), none of them addresses
the problem of entity linking. In this paper, we pro-
pose a novel spatiotemporal framework for entity
linking, building upon some of the previously ob-
served patterns in social media.
While we believe that entity linking is the first
step towards intelligence gathering, many existing
studies filter or cluster tweets based on merely key-
words. On the one hand, manual selection of key-
words (Sakaki et al., 2010; Tumasjan et al., 2010)
requires significant labor, and thus is not scalable
to the vast number of entities. On the other hand,
automatic approaches (Li et al., 2013) only identify
coarse-grained topics (e.g., crime or sports), falling
short of recognizing specific entities.
Lastly, there is a line of research on record ex-
traction from social media (Benson et al., 2011; Rit-
ter et al., 2012). Although the problem is different
from entity linking, they present an interesting in-
sight into social media. They observe that the same
record is often referenced by multiple messages, and
exploit this redundancy to help with extraction. The
redundant nature of social media can be potentially
leveraged to improve entity linking as well.
</bodyText>
<sectionHeader confidence="0.970024" genericHeader="method">
3 Spatiotemporal Entity Linking
</sectionHeader>
<bodyText confidence="0.9999368">
Our spatiotemporal framework for entity linking re-
quires an input tweet m, as well as its associated
timestamp t and location l. For each tweet m, the
goal is to predict an output set {e1, e2, ...I of enti-
ties that are mentioned in m.
</bodyText>
<subsectionHeader confidence="0.99798">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.999971428571429">
To build an entity linking system, we need both a
database and a lexicon.
A database is a set of entities that a tweet can link
to. Following earlier work (Sil and Yates, 2013), our
database consists of the intersection of Freebase and
Wikipedia. We then select the entities belonging to
the core types (Guo et al., 2013)2 based on the Free-
</bodyText>
<footnote confidence="0.697037666666667">
2Their core types include person, organization (org), loca-
tion (loc), book, tvshow and movie. We deal with two additional
types, namely, event and product.
</footnote>
<bodyText confidence="0.999974952380952">
base type information. In total, there are 2.7 million
entities in our database.
A lexicon is a dictionary that maps a candidate an-
chor a (i.e., a surface form) to its possible entity set
£(a). Similar to existing studies (Cucerzan, 2007;
Guo et al., 2013), our lexicon comprises information
from disambiguation pages, redirect pages and an-
chor texts in Wikipedia3. To handle the case where
a is not an entity, we add a special symbol 0 to every
£(a). We have 6 million anchors in total.
Given a tweet, the system generates a set of can-
didate anchors based on the lexicon. Specifically,
each tweet is tokenized into individual words, keep-
ing each punctuation as a separate token. To iden-
tify if any sequence of tokens matches an anchor in
the lexicon, we use exact matching, but allow for
case-insensitivity. Furthermore, we employ an NER
system trained using structural perceptron (Collins,
2002) to filter the anchors of type person, loc or org,
such that only the anchors recognized by the NER
are retained.
</bodyText>
<subsectionHeader confidence="0.999946">
3.2 Incorporating Spatiotemporal Signals
</subsectionHeader>
<bodyText confidence="0.999517666666667">
Consider a tweet m with timestamp t and location l.
Given anchor a in tweet m, our system links a to the
candidate entity e* as follows:
</bodyText>
<equation confidence="0.9476685">
e* = arg maxeES(a) P(e|m, a, t, l)
= arg maxeES(a) P(e, m, a, t, l) (1)
</equation>
<bodyText confidence="0.8695725">
We further adopt the following conditional inde-
pendence assumption in our model.
ASSUMPTION: Given an entity e, how e is expressed
(m, a), and when or where e is published (t, l), are
conditionally independent. In other words, we have
P(m, a|t, l, e) = P(m, a|e).
Intuitively, the expression (m, a) of a given en-
tity e is stable across most times and locations (t, l),
which could be attributed to the imitation nature
(Leskovec et al., 2009) of social media. That is, as
stories of e propagate on the social media, users over
different t and l often imitate each other (disregard-
ing cross-lingual scenarios). While there could be
a “burn-in” period for recent events of e, the distri-
bution of m and a would eventually “stabilize” over
most t and l.
</bodyText>
<footnote confidence="0.948868">
3We use a snapshot of Wikipedia taken on 04/03/2013.
</footnote>
<page confidence="0.996579">
261
</page>
<figure confidence="0.998183571428571">
Existing Work
Non-spatiotemporal (Sec 4)
Example: (Guo et al., 2013)
Popularity of Entity
Example: estimate using
Wikipedia pageviews
Spatio
</figure>
<figureCaption confidence="0.999655">
Figure 2: Overall framework of spatiotemporal entity linking.
</figureCaption>
<bodyText confidence="0.999790333333333">
Subsequently, we decompose the model below,
to enable the reuse of existing non-spatiotemporal
models for P(e|m, a), as we shall see later.
</bodyText>
<equation confidence="0.99679875">
P(e, m, a, t, l)
= P(m, a|t, l, e)P(t,l, e)
= P(m, a|e)P(t,l, e)
= P(e|m, a)P(m, a)P(e|t, l)P(t, l)/P(e) (2)
</equation>
<bodyText confidence="0.972247555555556">
Note that in (2), we choose to rewrite P(t,l, e) as
P(e|t,l)P(t, l) instead of P(t, l|e)P(e). The choice
is due to computational issues. In the latter, we need
to estimate one distribution for every e; in the for-
mer, we need to estimate one distribution for every
t,l. There are 2.7 million entities in our database,
but with appropriate quantization the number of t, l
“bins” are only on the order of thousands.
Thus, the prediction model (1) is equivalent to
</bodyText>
<equation confidence="0.998981">
e* = argmaxeEg(a) P(e|m, a)P(e|t,l)/P(e). (3)
</equation>
<bodyText confidence="0.991027227272727">
Intuitively, if e at some t, l is more popular than
usual, i.e., P(e|t, l) &gt; P(e), we should promote e
for a tweet at t, l; otherwise we should demote e.
The overall framework is summarized in Fig. 2,
which boils down to the three factors in (3), namely,
P(e|m, a), P(e|t, l) and P(e). In Sect. 4, we will
discuss briefly how P(e|m, a) is modeled based on
previous work. In Sect. 5, we will estimate spa-
tiotemporal signals in the form of P(e|t,l), which
is jointly optimized with entity assignments. Lastly,
we discuss P(e) here.
P(e) measures the popularity of e, and is esti-
mated by making it proportional to the Wikipedia
pageviews of e. While this is a fair proxy for a gen-
eral social media corpus, we acknowledge that more
advanced method is required for a specialized cor-
pus (e.g., for users of a sub-community). Note that
P(e) is not estimated based on the joint optimization
technique for P(e|t, l) in Sect. 5. The reason is that
our tweets used in the experiments only cover a one-
month period, which does not necessarily reflect the
general popularity of the entities.
</bodyText>
<sectionHeader confidence="0.963146" genericHeader="method">
4 End-to-End Entity Linking
</sectionHeader>
<bodyText confidence="0.9999613">
The goal of this section is to describe our base sys-
tem that does not consider spatiotemporal signals,
i.e., to model P(e|m, a). Specifically, we adopt an
end-to-end entity linking system (E2E), which is de-
signed to jointly detect mentions and disambiguate
entities. E2E is a supervised method largely based
on a previous study (Guo et al., 2013).
For efficiency, we only adopt the first order
model. Therefore, the prediction function can be de-
composed for each anchor a independently,
</bodyText>
<equation confidence="0.987179">
e* = arg maxeEg(a) wTΦ(m, a, e). (4)
</equation>
<bodyText confidence="0.994167166666667">
where w is a linear model trained using structural
SVM, and Φ is a feature function over message m,
anchor a, and candidate output e.
We use all the basic features and the cohesiveness
score feature (Guo et al., 2013)4. Additional features
are also included. First, for each mention and candi-
date entity pair, we add a feature to capture the num-
ber of highly correlated candidates carried by other
mentions in the same tweet with respect to the cur-
rent candidate. Second, we include a binary feature
that will be active if the type of the mention (when
recognized by our NER system) and the type of the
candidate entity (according to the Freebase type in-
formation) agree with each other.
Probability conversion. It is crucial to have a
well calibrated probability distribution for the pre-
dictions. In order to convert the output of the struc-
tural SVM model, we adapt an existing approach
</bodyText>
<footnote confidence="0.949673">
4Described in Table 4 and Sect. 4.3 of the reference.
</footnote>
<page confidence="0.99234">
262
</page>
<bodyText confidence="0.921555">
(Platt, 2000) to our case. We define
</bodyText>
<equation confidence="0.7072485">
exp(b1 + b2WTΦ(m, a, e))
P(e|m, a)= Ee,ES(a) exp(b1+b2WTΦ(m, a, e&apos;)),
</equation>
<bodyText confidence="0.989464">
where b1 and b2 are the calibration parameters that
will be tuned using labeled data.
Given a labeled development set, let G(e|m, a) =
1 if and only if anchor a in tweet m is labeled to
link to entity e, and let G(0|m, a) = 1 if and only
if a in m is not labeled to link to any entity. Note
that EeES(a) G(e|m, a) = 1. Thus, G represents the
ground-truth distribution, and we want P(e|m, a) to
be as close to G(e|m, a) as possible. To this end, we
optimize b1 and b2 by minimizing the cross entropy
between G and P:
</bodyText>
<equation confidence="0.763423">
minb1,b2 — Em,a G(e|m, a) log P(e|m, a).
</equation>
<bodyText confidence="0.999473647058824">
Alternative base system. We also consider Link-
Probability (LP) as an additional base system. As
pointed out earlier (Guo et al., 2013), mention detec-
tion is an important step for end-to-end entity link-
ing, and its design is crucial to the ultimate perfor-
mance. Hence, to detect candidate anchors, LP uses
the same design of database and lexicon discussed
in this paper and elsewhere (Cucerzan, 2007; Guo et
al., 2013), which is believed to be effective. Given a
potential anchor a in message m, P(e|m, a) is sim-
ply modeled as P(e|a), which can be estimated from
Wikipedia anchor statistics. In fact, anchor statistics
constitute one of the most useful features in more
sophisticated systems (Shen et al., 2012; Guo et al.,
2013). Given its robust mention detection mecha-
nism and the utility of anchor statistics, the simple
LP turns out surprisingly well.
</bodyText>
<sectionHeader confidence="0.780955" genericHeader="method">
5 Estimating Spatiotemporal Signals
</sectionHeader>
<bodyText confidence="0.9999917">
There are two critical challenges for successfully es-
timating the spatiotemporal signals in the form of
P(e|t,l). First, it is impractical to collect sufficient
labeled data to directly estimate it. Second, we need
to properly handle the continuous space of the spa-
tiotemporal signals.
In the following, we detail the overall model for
learning spatiotemporal signals in a weakly super-
vised fashion (Sect. 5.1), and then discuss two ways
of handling continuous signals (Sect. 5.2).
</bodyText>
<subsectionHeader confidence="0.989413">
5.1 Spatiotemporal Learning Model
</subsectionHeader>
<bodyText confidence="0.999964">
We model the spatiotemporal signals by a genera-
tive model P(e|t, l) — Multi(θtl), where θtl is the
parameter for the multinomial distribution over all
entities at t,l. Since there is no ground truth of the
entity assignment, our model will jointly optimize
the entity assignment and θtl. Based on (3), we use
the following objective function Q({e}, θtl):
</bodyText>
<equation confidence="0.537011">
Em,a(log P(e|m, a) + log P(e|θtl) — log P(e)),
</equation>
<bodyText confidence="0.999770333333333">
where m is an unlabeled message at time t and loca-
tion l, a is an anchor in m, and {e} is a set of entity
assignments for the set of m.
We use a block-coordinate ascent method to find
the best θtl and {e} iteratively. For each iteration,
the following two steps will be executed.
</bodyText>
<listItem confidence="0.894538909090909">
• Fix θtl. Find the entity assignments {e} that max-
imizes Q. Note that if θtl is fixed, the most likely
assignment can be found using
arg maxe(logP(e|m, a)+logP(e|θtl)—logP(e)).
In fact, this equation is the same as (3), where
P(e|m, a) can be estimated by a supervised base
system (e.g., E2E or LP, see Sect. 4).
• Fix {e}. Re-estimate θtl by maximizing Q. Once
the assignment of entities has been generated by
the previous step, we can re-estimate it by maxi-
mizing the objective function with
</listItem>
<equation confidence="0.657199">
E
</equation>
<bodyText confidence="0.942225666666667">
arg maxotl m,a log P(e|θtl).
In other words, we are looking for the maximum
likelihood estimate (MLE) of θtl, given the (previ-
ously inferred) entity assignments {e}. Since θtl
is multinomial, its MLE can be computed as the
relative frequency of each e, that is,
</bodyText>
<equation confidence="0.762793">
# tweets containing e at t, l
θtl(e) = Ee, # tweets containing e&apos; at t, l.
</equation>
<bodyText confidence="0.999628857142857">
Of course, t, l are continuous, so direct counting
is infeasible. Instead, we resort to discrete bins,
which will be treated in Sect. 5.2.
This process will be executed repeatedly, and can
be considered as a variant of the Hard EM algorithm.
In practice, we run it for up to five iterations, as Hard
EM often converges fast.
</bodyText>
<page confidence="0.989824">
263
</page>
<bodyText confidence="0.999803888888889">
We call this learning process a “weakly super-
vised” model. For some time t and location l, the ob-
jective function Ω({e}, θtl) sums over all messages
at t,l. These messages themselves are unlabeled,
but some supervision is provided by the base sys-
tem indirectly. In other words, even though we do
not know the ground truth entity assignments in the
messages, we can leverage the predictions from the
base system to update the entity assignments.
</bodyText>
<subsectionHeader confidence="0.999575">
5.2 Handling Continuous Time and Location
</subsectionHeader>
<bodyText confidence="0.989658428571429">
Given the continuous space of time t and location
l, we propose two methods to estimate θtl. While
it could be cast as an instance of the well-studied
density estimation problem (Vapnik and Mukherjee,
1999; Scott, 2009), we resort to a simple binning
method, which has also been applied to other spa-
tiotemporal contexts (Wing and Baldridge, 2011; Xu
et al., 2012). The binning approach can already
demonstrate the significance of spatiotemporal sig-
nals in entity linking.
Binning. Our first approach segments the continu-
ous space into discrete bins. Time is divided into a
set of equal intervals ΔT (e.g., every one hour), and
location is divided into a set of equal squares ΔL
(e.g., each 100×100 sqkm area5). Let Δ = ΔT×ΔL
denote the set of bins over time and location. We
further index the bins by I = {1, ... , n} where
n = |Δ|, and refer to each bin δi through its index
i ∈ I. Correspondingly, we denote the multinomial
parameter at bin δi by θi, which can be estimated us-
ing maximum likelihood (i.e., relative frequency of
entities in δi).
It can be shown that with sufficiently small bins,
θtl can be approximated accurately by θi when t, l ∈
δi. In practice, if the bins are too small, there are
often inadequate tweets in a bin, and thus θi cannot
be well estimated. On the contrary, if the bins are
too large, θi cannot reliably model θtl. In this paper,
we tune the bin size using development data.
Graph-based smoothing. Our algorithm contains
two steps: first, we estimate ˆθi for each bin δi as
in the above binning approach; next, we smooth the
initial estimate to obtain the final estimate θi using
graph-based regularization.
5The location grid is actually defined in longitude and lati-
tude. Here we show the equivalent distance on Earth’s surface.
As a common insight in graph-based learning
(Zhu et al., 2003; Fang et al., 2012; Fang et al.,
2014), if δi and δj are close to each other, θi and θj
should be similar. Moreover, θi should not deviate
too much from the initial estimate ˆθi. The intuition
can be captured by the following optimization:
</bodyText>
<equation confidence="0.901828333333333">
2(1 − E) E
1 i,j∈I Wijkθi − θjk2
+ E Ei∈I Diikθi − ˆθik2, (5)
</equation>
<bodyText confidence="0.9983718">
where c ∈ (0, 1) is a regularization parameter, W is
an affinity matrix such that Wij measures the “close-
ness” of δi and δj, and D is a diagonal matrix such
that Dii = Ej∈I Wij. In particular, we design the
affinity matrix as follows:
</bodyText>
<equation confidence="0.999609666666667">
� (d0 + d(δi, δj))γ i =6 j
Wij = (6)
0 i = j,
</equation>
<bodyText confidence="0.999258142857143">
where d0 &gt; 0 and γ &lt; 0 are parameters, and d(·) is
a symmetric distance function for bins. Given that
γ &lt; 0, Wij follows a polynomial decay when δi
and δj become farther apart, as previously suggested
(Dalvi et al., 2012).
It can be shown that the optimization problem (5)
is equivalent to finding θi such that ∀i ∈ I,
</bodyText>
<equation confidence="0.695608">
θi=(1−E) Ej∈I Dii θj + A. (7)
</equation>
<bodyText confidence="0.999775947368421">
Interestingly, we can consider (7) as a generaliza-
tion of previous observations on social media. It is
proposed that the temporal model on social media
needs to account for two factors: imitation and re-
cency (Leskovec et al., 2009). For imitation, users
often imitate one another, so that a past story can
be picked up and propagated by other users by writ-
ing new articles about the same story. For recency,
more recent stories are more likely to be imitated.
We generalize their idea to both temporal and spatial
signals, and extend it to entity linking. Specifically,
(7) can be interpreted as a result of imitation: tweets
mentioning entity e in δi are imitated from those in
δj. In addition, a closer δj has a higher chance to be
imitated, which captures recency.
To solve (7), let Q = [θ1, ... , θn]T and Qˆ =
ˆθn]T, treating θi and ˆθi as column vectors.
Through the following iterative updates (Fang et al.,
2013), Q(t) converges to Q as t → ∞, starting from
</bodyText>
<equation confidence="0.903658333333333">
min
(θ1,...,θn)
[ ˆθ1,...,
</equation>
<page confidence="0.672837">
264
</page>
<bodyText confidence="0.490372">
an arbitrary Q(0):
</bodyText>
<equation confidence="0.999351">
Q(t+1) = (1 − c)(D−1W)Q(t) + c ˆQ. (8)
</equation>
<bodyText confidence="0.997798733333333">
The time complexity is O(tnk), where k is the
number of neighboring bins on the graph. Such cost
is reasonable, given that the update generally con-
verges for t &lt; 50, and k is a constant if we only
consider k nearest neighbors on the graph.
Joint vs. separate modeling. Finally, while model-
ing time and location jointly is more expressive than
modeling each signal separately, the joint approach
also creates much more bins (and hence more multi-
nomial parameters), making data scarcity a severe
issue. In particular, jointly we have |OT |× |OL|
bins, but separately we only have |OT  |+ |OL |bins.
By assuming the conditional independence of t and
l given e, we can rewrite (3) to model time and loca-
tion separately:
</bodyText>
<equation confidence="0.996379">
e∗ = arg max P(e|m, a)P(e|t)P(e|l)/P(e)2 (9)
eEE(a)
</equation>
<bodyText confidence="0.9998524">
We refer to the joint model (3) as “T×L”, and the
separate model as “T+L” (9). For “T+L”, graph-
based smoothing can be carried out separately for
the time bins and the location bins. The two models
will be compared in our experiments.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9995665">
In this section, we assess the effectiveness of our ap-
proach through quantitative experiments.
</bodyText>
<subsectionHeader confidence="0.992908">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.988372775862069">
Dataset. Our experiments are conducted on Twit-
ter data. We collect all English tweets from verified
accounts in December 2012, excluding retweets.6
These tweets amount to 7 million. As we assume an
offline analysis scenario instead of an online stream-
ing scenario, these tweets are fetched and stored lo-
cally in advance.
We further select tweets with both spatial and
temporal information. For time, we take the post-
ing time of each tweet. Locations are harder to ob-
tain, given that fewer than 5% of our tweets are geo-
tagged. For tweets without geo-coordinates, we use
6The identities of verified accounts are validated by Twit-
ter, and thus their tweets contain little spam. Moreover, we ig-
nore retweets here as their spatiotemporal behavior might sig-
nificantly differ from that of the original tweets.
the location in the user profile and map it to coor-
dinates based on a lookup table containing major
cities in the US7. Such mapping exists for about 25%
of the tweets. For the remaining tweets, their user
profiles are either uninformative (e.g., “home”) or
report a non-US location. In the end, 1.8 million
tweets remain in our dataset. Note that some stud-
ies (Dalvi et al., 2012; Li et al., 2012b) enable the
inference of missing locations based on user gener-
ated content or user network. We do not apply these
methods, which are beyond our focus.
Lastly, we collect the Wikipedia pageviews in the
year 2012 in order to estimate P(e) in (3).
Development set. We randomly sample 250 tweets
as the development set and label their core entities
(see Sect. 3.1). There are two human annotators.
Each annotator labels half of the tweets, which are
then counter-checked by the other to reach an agree-
ment. To label a tweet, the annotators are given all
available information to understand its content, e.g.,
the attached URL and the conversation.
On the other hand, the base system E2E is trained
on an independent set of 1000 tweets randomly sam-
pled from the year 2010.
Algorithms. For the binning approach, we tune the
bin size on the development set, ranging from 10
minutes to 1 day for time, and from 10 × 10 sqkm to
1500 × 1500 sqkm for location. Although the opti-
mal bin size may be entity specific, we use the same
size optimized over all entities for scalability con-
siderations. That said, if we are only interested in a
small set of target entities, it is possible to optimize
the bin size for each entity, in order to attain better
performance. Unless otherwise stated, in the rest of
the paper, binning is the default method to estimate
spatiotemporal signals.
For graph-based smoothing, we use the finest bin
for both time and location, and tune its parame-
ters c ∈ {.01, .05, .1, .5}, d0 = {1,10} and γ ∈
{-.5,-1,-2,-4,-8} on the development set. To con-
struct the graph, we also need to define the dis-
tance function d(·) in (6). For time bins, we use the
</bodyText>
<footnote confidence="0.996399">
7The lookup table is compiled from geonames.org. We only
consider city and state names in the US, discarding all oth-
ers. As city names may be ambiguous, we apply conservative
matching using city-state pair. We also account for popular ab-
breviations (e.g., IL for Illinois and NYC for New York City).
</footnote>
<page confidence="0.996173">
265
</page>
<bodyText confidence="0.9999527">
elapsed time between the middle timestamps of two
bins. For location bins, we use the distance between
the geographical centers of two bins on Earth’s sur-
face. For joint time-location bins, we take the aver-
age of time (min) and location (km) distances.
Finally, in both of the above, we use the inferred
entity assignments {e} to estimate Btl (see Sect 5.1).
Since the inferred assignments can be quite noisy,
we only count confident ones such that their inferred
probability is at least 0.5.
</bodyText>
<subsectionHeader confidence="0.998978">
6.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999945962962963">
We adopt two different evaluation policies, which
are driven by information extraction (IE) and re-
trieval (IR) needs, respectively.
IE-driven evaluation. The IE-driven evaluation is
similar to the standard evaluation for an end-to-end
entity linking system, which evaluates the entities
“extracted” by linking. We randomly sample 250
tweets as the test set, which are labeled in the same
manner as the development set. For evaluation, we
compute the F1 score over the test set, as defined
earlier (Guo et al., 2013).
According to the labels, 40% of the tweets in the
test set contain at least one entity. A total of 179
entity instances are identified, or an average of 0.72
per tweet. These entities belong to different types,
including person (24%), org (36%), loc (16%), event
(9%), and others (15%).
IR-driven evaluation. One key application of en-
tity linking is to enable intelligence gathering for a
query entity, where the first step is to “retrieve” the
tweets mentioning the query entity. As listed in Ta-
ble 1, we sample ten query entities of different types,
where each entity is known to be influenced by spa-
tiotemporal signals, and has one or more ambiguous
anchors. For instance, Hillary Rodham Clinton may be
mentioned by “clinton” only, which could be mis-
takenly linked to Bill Clinton.
</bodyText>
<table confidence="0.775958">
Type Query entity
person Hillary Rodham Clinton* Catherine, Duchess of Cambridge*
San Antonia Spurs* Big Bang (South Korean band)*
Washington (state)* Newtown, Connecticut
Hanukkah* Winter solstice
Les Mis´erables (2012 film) Django unchained (2012 film)
</table>
<tableCaption confidence="0.999747">
Table 1: Query entities for IR-driven evaluation.
</tableCaption>
<bodyText confidence="0.999934769230769">
For each query entity, we randomly sample 100
tweets as the test set (totaling 1000 tweets), such that
each tweet contains an ambiguous anchor of the en-
tity (through a lookup from our lexicon). Each tweet
is labeled to indicate whether it mentions the query
entity or not. About 37% of the sampled tweets
did not actually mention the query entity since the
anchors are ambiguous. For evaluation, we test if
the system can correctly identify the presence or ab-
sence of the query entity in every tweet. In particu-
lar, we compute the F1 score over the test set.
Note that this task is harder than it seems. Besides
ambiguous anchors, many entities are not dominant
for their anchor. For instance, for the anchor “big
bang”, Big Bang (South Korean band) is less popular
than The Big Bang Theory (a TV show). In fact, six of
the entities are not dominant, marked by * in Table 1.
Thus, merely linking to the most popular one (i.e.,
the base system LP) is not a good strategy.
Significance test. To establish the statistical signifi-
cance of our results, we randomly divide the test set
into 10 splits of equal number of tweets, and com-
pute the F1 score on each split for each algorithm.
Two-tail paired t-test is then applied to determine if
the F1 scores of two algorithms over the 10 splits are
significantly different.
</bodyText>
<subsectionHeader confidence="0.948568">
6.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9993495">
We present the empirical findings for the following
research questions.
</bodyText>
<listItem confidence="0.90837525">
Q1: How do our base systems perform?
Q2: Are spatiotemporal signals indeed useful?
Q3: Does the graph-based smoothing help?
Q4: What causes the errors? How to recover them?
</listItem>
<bodyText confidence="0.80822">
Base system comparison (Q1). To show that our
base systems, in particular E2E, already outperform
other systems, we compare with Wikiminer (Milne
and Witten, 2008) and Illinois (Ratinov et al., 2011)
systems.8 As existing systems are more geared for
the IE scenario, we report in Table 2 the IE-drive F1
on the test set.
8We use the authors’ implementations. AIDA (Hoffart et
al., 2011) is not compared, as it mostly links to person, org and
loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan
(Cucerzan, 2007) were already compared in an earlier paper
(Guo et al., 2013) which E2E is largely based on. To be fair,
we discard non-core entities linked by Wikiminer or Illinois.
org
loc
event
movie
</bodyText>
<page confidence="0.989832">
266
</page>
<table confidence="0.979144428571429">
E2E LP
IE IR IE IR
base 57.0 – 58.4 – 48.3 – 48.5 –
+T 64.9 *** 71.4 *** 52.4 * 59.7 ***
+L 65.0 ** 76.1 *** 50.3 * 61.8 ***
+T+L 68.6 *** 79.0 *** 49.0 53.3 ***
+T×L 66.2 *** 74.1 *** 50.6 61.2 ***
</table>
<figure confidence="0.999063333333333">
(a) F1 scores
(b) Precision
(c) Recall
E2E +T +L +T+L
Precision
90
80
70
60
85.5 86.9
82.4
85.6
E2E +T +L +T+L
Recall
40
60
50
30
42.8
51.8 53.6
57.2
</figure>
<figureCaption confidence="0.9995925">
Figure 3: Effect of using spatiotemporal signals. (a) F1 scores. ***, **, *: Significantly different from the base
system at .01, .05, .1 levels. (b, c) IE-driven evaluation of precision and recall, using E2E as the base system.
</figureCaption>
<bodyText confidence="0.99814675">
The results clearly show that E2E performs better
than other base systems. Also note that LP obtains
similar or better F1 than Illinois or Wikiminer, al-
though its precision is lower.
</bodyText>
<table confidence="0.9999622">
Precision Recall F1 Significance
Wikiminer 78.9 24.7 37.6 ***
Illinois 77.3 34.9 48.1 **
LP 49.7 47.0 48.3 **
E2E 85.5 42.8 57.0 –
</table>
<tableCaption confidence="0.9953435">
Table 2: IE-driven comparison of base systems. ***, **,
*: Significantly different from E2E at .01, .05, .1 levels.
</tableCaption>
<bodyText confidence="0.999879934782609">
Spatiotemporal signals (Q2). To showcase our key
insight that spatiotemporal signals are crucial for en-
tity linking, we compare the base systems with their
time or location-aware counterparts.
As reported in Fig. 3a, using either temporal (+T)
or spatial (+L) signal can indeed improve entity link-
ing over the base systems. In particular, the im-
provements in IR-driven evaluation are more signif-
icant since the chosen entities are known to be influ-
enced by time or location.
Next, we combine both temporal and spatial sig-
nals. As Sect. 5 discussed, while the joint model
(+TxL) is ideal in theory, it is not necessarily better
than the separate model (+T+L) due to data scarcity,
as shown in the last two rows of Fig. 3a. In the fol-
lowing, we will use “+T+L” as the default setting.
Also note that using both time and location on LP
may be worse than using either alone, given that LP
has low precision and thus employing both signals
may aggregate more errors.
Let us also examine the precision and recall in
Fig. 3b and 3c. Interestingly, while precision is not
compromised, recall is greatly increased with spa-
tiotemporal signals. The reason is that E2E has high
precision, but misses a lot of mentions (i.e., linking
them to 0). However, if an entity e is “trending”
at some time t or location l, that is, e is mentioned
by more tweets than usual at t or l, we shall expect
P(e|t, l) &gt; P(e). Thus, the system is more likely
to link to e instead of 0 based on our spatiotempo-
ral model (3), resulting in higher recall. A more in-
depth error analysis will be presented in Q4.
Graph-based smoothing (Q3). Next, we evalu-
ate the utility of smoothing in Fig. 4. The model
called “Fine” discretizes time and location using the
finest bins. The one called “Fine+Smoothing” is
our graph-based smoothing algorithm applied to the
“Fine” model. We also compare with “Optimal”,
which discretizes time and location using the opti-
mal bins tuned on the development set.
As we can see, smoothing is crucial when a
small bin size is used. Compared to “Optimal”, our
smoothing is better on LP but not on E2E. However,
smoothing is still useful, as “Optimal” depends a lot
on choosing the right bin size, while smoothing is
less sensitive to its parameters.
</bodyText>
<figureCaption confidence="0.9865465">
Figure 4: F1 scores by different ways of estimating spa-
tiotemporal signals in the +T+L setting.
</figureCaption>
<bodyText confidence="0.99983275">
Error analysis (Q4). Let us investigate what causes
the errors. In Table 3, we first break down the errors
into false positives and false negatives. Each type is
further categorized as “mention” (the mention itself
</bodyText>
<figure confidence="0.99211078125">
IE-driven IR-driven IE-driven IR-driven
(a) Base system: E2E (b) Base system: LP
F1 score
90
80
70
60
50
57.0
Base
Optimal
Fine
Fine+Smoothing
68.6
59.6
66.0
58.5
79.0
67.6
76.6
F1 score
45
65
60 Fine
Fine+Smoothing
55
50 48.3 49.2 49.2 49.7
48.6
Base
Optimal
53.4 52.9
59.8
</figure>
<page confidence="0.992031">
267
</page>
<bodyText confidence="0.988312">
is wrong) or “linking” (the mention is correct but
linking is wrong). Note that if the mention is wrong,
the linking would also be wrong.
</bodyText>
<table confidence="0.999608">
False Positives False Negatives
mention linking total mention linking total
E2E 7 5 12 90 5 95
+T+L 9 7 16 64 7 71
</table>
<tableCaption confidence="0.999857">
Table 3: Sources of error in IE-driven evaluation.
</tableCaption>
<bodyText confidence="0.999951939393939">
On the one hand, both systems make few false
positives, suggesting high precision (see Fig. 3b).
On the other hand, while the base system (E2E)
makes many false negatives, the spatiotemporal
model (+T+L) substantially reduces these errors,
resulting in a significant increase in recall (see
Fig. 3c). +T+L is particularly effective in recovering
false negatives due to incorrect mention, i.e., linking
to 0 when it should not, which account for the vast
majority (84%) of all the errors in E2E.
Next, we examine how these errors can be recov-
ered in +T+L. We zoom into entity types of person,
org, loc and event, which collectively cover 85% of
the entities in the test set. Fig. 5a reveals that +T+L
is consistently helpful to different types of entities.
In particular, +T+L improves the event entities the
most, since events are generally more correlated to
time and location, which are useful signals for re-
covering the errors.
We illustrate in Fig. 5b some entities that are
missed by E2E, but recovered by useful spatial or
temporal signals. Taking tweet #2 for illustration,
E2E is not confident linking to the entity Califor-
nia State University, Fullerton based on textual content
alone, and thus incorrectly links to 0. This is a false
negative due to incorrect mention, the most frequent
kind in E2E (see Table 3). Fortunately, +T+L is con-
fident linking to the entity correctly, given that 1)
the tweet was posted during a campus emergency,
when many other tweets were also discussing this
entity; and 2) the posting user was in California,
where the users of many other tweets discussing this
entity were also located.
</bodyText>
<sectionHeader confidence="0.97361" genericHeader="method">
7 Qualitative Study: Entity vs. Keyword
</sectionHeader>
<bodyText confidence="0.998267">
The goal of this section is twofold. First, we provide
a qualitative analysis to visualize the performance
of our entity linking system. The results show that
</bodyText>
<figure confidence="0.631675">
person org loc event
(a) F1 scores
</figure>
<table confidence="0.7552975">
Tweet Example entity Posting time User location
#1 person: Colin Kaepernick during his game (not useful)
#2 org: Cali. State U. Fullerton in campus emergency California
#3 loc: Los Angeles (not useful) California
#4 event: New Year’s Eve on 31 December (not useful)
(b) Example entities recovered by spatiotemporal signals
</table>
<figureCaption confidence="0.990893">
Figure 5: IE-driven evaluation by entity type.
</figureCaption>
<bodyText confidence="0.99962971875">
our system can consistently recover the real-world
happenings or physical locations of the query enti-
ties. Second, we show that entity linking is more ef-
fective than keyword matching in the retrieving sce-
nario (see Sect. 6.2).
Approach. We can retrieve a tweet by entity link-
ing or keywords. For entity linking, we classify all
the tweets using E2E+T+L, and a tweet is positive
(i.e., it will be retrieved) if its predictions contain the
query entity. For keyword matching, a tweet is posi-
tive if it contains the given keywords which describe
the query entity.
To visualize the retrieved tweets, we discretize
time into 24-hour bins, and location into 100 × 100
sqkm bins. In a bin Si, define the intensity of an en-
tity e as #(e, Si)/#(Si), where #(e, Si) is the number
of positive tweets for e in Si, and #(Si) is the total
number of tweets in Si. To visualize the intensity of
multiple entities simultaneously, we further compute
the normalized intensity of e, which normalizes e’s
intensity by its maximum over all the bins.
Case study 1: “washington.” We retrieve tweets
with keyword “washington”, a common anchor for
three entities: Washington (state), Wasghinton D.C. and
Washington Redskins. We also retrieve tweets for
each of them with entity linking.
We first examine the intensity of the entities over
December 2012. In Fig. 6a, the intensity based on
keyword “washington” inevitably represents a mix-
ture of different entities—it is unclear which “wash-
ington” entity corresponds to each intensity peak.
Fortunately, entity linking separates the three enti-
</bodyText>
<figure confidence="0.991522214285714">
66.7
78.1 73.1
68.0
60.9
59.3
E2E +T+L
11.1
58.3
F1 score 100
80
60
40
20
0
</figure>
<page confidence="0.730488">
268
</page>
<figureCaption confidence="0.995032666666667">
Figure 6: Intensity of “washington” over December 2012. In particular, entity linking (b) reveals the correspondence
to several real-world happenings: legalization of marijuana in Washington (state) [1.1] and Obama’s response [1.2], fiscal
cliff and weather alert in Washington D.C. [2], games of Washington Redskins [3].
</figureCaption>
<figure confidence="0.999227">
(b) with entity linking
(c) with other keywords for Washington (state)
1
0.8
0.6
0.4
0.2
0
1 6 11 16 21 26 31
Day
Washington, D.C.
Washington Redskins
Washington (state)
0.4
0.2
0
1 6 11 16 21 26 31
Day
keywords (&amp;quot;wa&amp;quot;)
keywords (&amp;quot;washington state&amp;quot;)
0.8
0.6
0.4
0.2
0
1 6 11 16 21 26 31
Day
1
0.8
0.6
1
[1.1]
[2]
[3]
[3]
[3]
[1.2]
[3] [3]
(a) with keywords (“washington”)
</figure>
<bodyText confidence="0.9976664">
ties in Fig. 6b, where major intensity peaks corre-
spond to real-world happenings for each entity. We
also use alternative keywords specifically targeted
for Washington (state), namely, “washington state”
and “wa”. While they are more precise than “wash-
ington” for the given entity, they have lower recall,
resulting in different intensity profiles in Fig. 6c.
Lastly, we examine their intensity over the US.
Using keywords, Fig. 6a does not isolate different
entities. In contrast, using entity linking, Fig. 6b
reveals more information about the three entities.
For instance, tweets for Washington (state) are mostly
concentrated in that state, and those for Washington
Redskins are mostly in their home location but also
have occurrences all over the US.
</bodyText>
<figureCaption confidence="0.9976025">
Figure 7: Intensity of “washington” over the US. The size
of dots indicates the normalized intensity.
</figureCaption>
<bodyText confidence="0.999419411764706">
Case study 2: “spurs.” We retrieve tweets with
keyword “spurs”, a popular anchor for both San An-
tonio Spurs and Tottenham Hotspur F.C. We also re-
trieve tweets for each of them with entity linking.
As illustrated in Fig. 8, the intensity based on
keyword “spurs” roughly matchestthat of San Anto-
nio Spurs based on entity linking, but significantly
differs from Tottenham Hotspur F.C. In other words,
“spurs” only accounts for the former entity, even
though it is also a standard anchor for the latter. The
reason is that an overwhelming majority of the men-
tions of “spurs” in the US refer to San Antonio Spurs.
That means, while keywords may work for the domi-
nating entity, they are particularly weak for the other
entity. In contrast, with entity linking, both entities
can be identified, where the major peaks correspond
to the game days of each team.
</bodyText>
<figure confidence="0.9279558">
keywords (&amp;quot;spurs&amp;quot;)
entity lnking (San Antonio Spurs)
entity linking
entity linking
0.6
0.4
0.2
0
1 6 11 16 21 26 31
Day
</figure>
<figureCaption confidence="0.999868">
Figure 8: Intensity of “spurs” over December 2012.
</figureCaption>
<sectionHeader confidence="0.993925" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<subsectionHeader confidence="0.751801">
0.8 entity lnking (Tottenham Hotspur F.C.)
</subsectionHeader>
<bodyText confidence="0.999973684210526">
As microblogging and other social media services
become increasingly popular, the number of short
and noisy messages are growing at an unprecedented
rate. We demonstrate, for the first time, that spa-
tiotemporal signals are critical in advancing entity
linking. After all, it might not be a mere text-based
language processing problem.
There are some important future directions for
this work. First, updating the spatiotemporal model
on the fly is a useful extension to cater to the online
streaming setting. Second, we foresee a more gen-
eral framework to integrate various meta-data such
as authorship, surrounding conversation, attached
URL and hashtags, in addition to the spatiotempo-
ral signals. Finally, it would also pay off to analyze
the interactions between the meta-data and the use of
language. We believe that exploring the meta-data
for entity linking on microblogs will be an interest-
ing and active line of research.
</bodyText>
<figure confidence="0.937858">
m
(a) with keywords (“washington”) (b) with entity linking
Washington Redskins
Washington, D.C.
Washington (state)
1
iy
N
</figure>
<page confidence="0.994675">
269
</page>
<sectionHeader confidence="0.996038" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847339805825">
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. In Web Intelligence, pages 492–
499.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 389–398.
R. C Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of the European Chapter of the ACL (EACL),
pages 9–16.
M. Cataldi, L. Di Caro, and C. Schifanella. 2010.
Emerging topic detection on twitter based on tempo-
ral and social terms evaluation. In Proceedings of the
International Workshop on Multimedia Data Mining
(MDMKDD), pages 4:1–10.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP).
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 708–
716.
N. Dalvi, R. Kumar, and B. Pang. 2012. Object matching
in tweets with spatial models. In Proceedings of In-
ternational Conference on Web Search and Web Data
Mining (WSDM), pages 43–52.
J. Eisenstein, B. O’Connor, N.A. Smith, and E.P. Xing.
2010. A latent variable model for geographic lexi-
cal variation. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 1277–1287.
Y. Fang, B.-J. Hsu, and K. C.-C. Chang. 2012.
Confidence-aware graph regularization with heteroge-
neous pairwise features. In Proceedings of Interna-
tional Conference on Research and Development in In-
formation Retrieval (SIGIR), pages 951–960.
Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2013.
Roundtriprank: Graph-based proximity with impor-
tance and specificity. In Proceedings of International
Conference on Data Engineering (ICDE), pages 613–
624.
Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2014. Graph-
based semi-supervised learning: Realizing pointwise
smoothness probabilistically. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
P. Ferragina and U. Scaiella. 2010. TAGME: on-the-fly
annotation of short text fragments (by Wikipedia en-
tities). In Proceedings of ACM Conference on Infor-
mation and Knowledge Management (CIKM), pages
1625–1628.
S. Guo, M.-W. Chang, and E. Kıcıman. 2013. To link
or not to link? A study on end-to-end tweet entity
linking. In Proceedings of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 1020–1030.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
ceedings of International Conference on Research and
Development in Information Retrieval (SIGIR), pages
765–774.
J. Hoffart, M. Amir Yosef, I. Bordino, H. F¨urstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 782–792.
H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: Successful approaches and challenges. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1148–1158.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis.
2010. Overview of the TAC 2010 knowledge base
population track. In Text Analysis Conference (TAC).
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
Wikipedia entities in web text. In Proceedings of
International Conference on Knowledge Discovery
and Data Mining (KDD), pages 457–466.
J. Leskovec, L. Backstrom, and J. Kleinberg. 2009.
Meme-tracking and the dynamics of the news cycle.
In Proceedings of International Conference on Knowl-
edge Discovery and Data Mining (KDD), pages 497–
506.
C. Li, J. Weng, Q. He, Y Yao, A. Datta, A. Sun, and B.-S.
Lee. 2012a. TwiNER: named entity recognition in tar-
geted twitter stream. In Proceedings of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 721–730.
R. Li, S. Wang, H. Deng, R. Wang, and K. Chen-Chuan
Chang. 2012b. Towards social user profiling: unified
and discriminative influence model for inferring home
locations. In Proceedings of International Conference
on Knowledge Discovery and Data Mining (KDD),
pages 1023–1031.
R. Li, S. Wang, and K. Chen-Chuan Chang. 2013. To-
wards social data platform: Automatic topic-focused
monitor for twitter stream. Proceedings of the VLDB
Endowment (PVLDB), 6(14).
</reference>
<page confidence="0.946902">
270
</page>
<reference confidence="0.9998238375">
Q. Mei, C. Liu, H. Su, and C.-X. Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of the International
World Wide Web Conference (WWW), pages 533–542.
E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding
semantics to microblog posts. In Proceedings of In-
ternational Conference on Web Search and Web Data
Mining (WSDM), pages 563–572.
D. Milne and I. H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 509–518.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularize likelihood
methods. In Advances in Large Margin Classifiers,
pages 61–74.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to Wikipedia. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1375–1384.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 1524–1534.
A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open
domain event extraction from Twitter. In Proceedings
of International Conference on Knowledge Discovery
and Data Mining (KDD), pages 1104–1112.
A. Sadilek, H. Kautz, and V. Silenzio. 2012. Modeling
spread of disease from social interactions. In Proceed-
ings of the International Conference on Weblogs and
Social Media (ICWSM).
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes Twitter users: real-time event detection
by social sensors. In Proceedings of the International
World Wide Web Conference (WWW), pages 851–860.
D.W. Scott. 2009. Multivariate density estimation: the-
ory, practice, and visualization. John Wiley &amp; Sons.
W. Shen, J. Wang, P. Luo, and M. Wang. 2012. LIN-
DEN: linking named entities with knowledge base via
semantic knowledge. In Proceedings of the Inter-
national World Wide Web Conference (WWW), pages
449–458.
W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base via
user interest modeling. In Proceedings of Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), pages 68–76.
A. Sil and A. Yates. 2013. Re-ranking for joint named-
entity recognition and linking. In Proceedings ofACM
Conference on Information and Knowledge Manage-
ment (CIKM).
A. Tumasjan, T. O. Sprenger, P. G Sandner, and I. M
Welpe. 2010. Predicting elections with Twitter: What
140 characters reveal about political sentiment. In
Proceedings of the International Conference on We-
blogs and Social Media (ICWSM), pages 178–185.
V. Vapnik and S. Mukherjee. 1999. Support vector
method for multivariate density estimation. In Pro-
ceedings of the Conference on Advances in Neural In-
formation Processing Systems (NIPS), pages 659–665.
B.P. Wing and J. Baldridge. 2011. Simple supervised
document geolocation with geodesic grids. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 955–964.
J.-M. Xu, A. Bhargava, R. Nowak, and X. Zhu. 2012.
Socioscope: Spatio-temporal signal recovery from so-
cial media. In Proceedings of the European Confer-
ence on Machine Learning (ECML), pages 644–659.
Z. Yin, L. Cao, J. Han, C. Zhai, and T. Huang. 2011.
Geographical topic discovery and comparison. In Pro-
ceedings of the International World Wide Web Confer-
ence (WWW), pages 247–256.
X. Zhu, Z. Ghahramani, J. Lafferty, et al. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of the International
Conference on Machine Learning (ICML), pages 912–
919.
</reference>
<page confidence="0.9986685">
271
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768443">
<title confidence="0.998508">Entity Linking on Microblogs with Spatial and Temporal Signals</title>
<author confidence="0.990277">Fang</author>
<affiliation confidence="0.996804">University of Illinois at</affiliation>
<address confidence="0.9053855">201 N. Goodwin Urbana, IL 61801, USA</address>
<email confidence="0.998758">fang2@illinois.edu</email>
<author confidence="0.998293">Ming-Wei Chang</author>
<affiliation confidence="0.983541">Microsoft</affiliation>
<address confidence="0.978492">1 Microsoft Way Redmond, WA 98052, USA</address>
<email confidence="0.999886">minchang@microsoft.com</email>
<abstract confidence="0.999378842105263">Microblogs present an excellent opportunity for monitoring and analyzing world happenings. Given that words are often ambiguous, entity linking becomes a crucial step towards understanding microblogs. In this paper, we re-examine the problem of entity linking on microblogs. We first observe that spatiotemspatial and temporal) signals play a key role, but they are not utilized in existing approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly superprocess. Using entity on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Asur</author>
<author>B A Huberman</author>
</authors>
<title>Predicting the future with social media.</title>
<date>2010</date>
<booktitle>In Web Intelligence,</booktitle>
<pages>492--499</pages>
<contexts>
<context position="1218" citStr="Asur and Huberman, 2010" startWordPosition="176" endWordPosition="179">poral (i.e., spatial and temporal) signals play a key role, but they are not utilized in existing approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly supervised process. Using entity annotations1 on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach. 1 Introduction Microblogging services provide an immense platform for intelligence gathering, such as market research (Asur and Huberman, 2010), disaster monitoring (Sadilek et al., 2012) and political analysis (Tumasjan et al., 2010). Extracting entities from microblogs is an essential step for many such applications. Suppose that a marketing firm is interested in the sentiment about some product on Twitter. However, any sentiment analysis is potentially ∗Work done during an internship at Microsoft Research. †Also affiliated with Agency for Science, Technology and Research, 1 Fusionopolis Way, Singapore 138632. 1Can be downloaded at http://research.microsoft.com/enus/downloads/84ac9d88-c353-4059-97a4-87d129db0464/. misleading if we </context>
</contexts>
<marker>Asur, Huberman, 2010</marker>
<rawString>S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In Web Intelligence, pages 492– 499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Benson</author>
<author>A Haghighi</author>
<author>R Barzilay</author>
</authors>
<title>Event discovery in social media feeds.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>389--398</pages>
<contexts>
<context position="9202" citStr="Benson et al., 2011" startWordPosition="1469" endWordPosition="1472">rns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013) only identify coarse-grained topics (e.g., crime or sports), falling short of recognizing specific entities. Lastly, there is a line of research on record extraction from social media (Benson et al., 2011; Ritter et al., 2012). Although the problem is different from entity linking, they present an interesting insight into social media. They observe that the same record is often referenced by multiple messages, and exploit this redundancy to help with extraction. The redundant nature of social media can be potentially leveraged to improve entity linking as well. 3 Spatiotemporal Entity Linking Our spatiotemporal framework for entity linking requires an input tweet m, as well as its associated timestamp t and location l. For each tweet m, the goal is to predict an output set {e1, e2, ...I of ent</context>
</contexts>
<marker>Benson, Haghighi, Barzilay, 2011</marker>
<rawString>E. Benson, A. Haghighi, and R. Barzilay. 2011. Event discovery in social media feeds. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 389–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the ACL (EACL),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2528" citStr="Bunescu and Pasca, 2006" startWordPosition="373" endWordPosition="376"> for a given entity (e.g., a product or an organization), a straightforward approach is to formulate a keyword query. However, simple keyword matching is largely ineffective, since keywords are often ambiguous. For instance, “spurs” can refer to two distinct sports teams (San Antonio Spurs, which is a basketball team in the US, and Tottenham Hotspur F.C., which is a soccer team in the UK), besides being a non-entity verb or noun. Thus, retrieving tweets via keyword queries inevitably mixes different entities. Given the ambiguity of keywords, in this paper, we study the task of entity linking (Bunescu and Pasca, 2006) on microblogs. As input, we are given a short message (e.g., a tweet) and an entity database (e.g., Wikipedia where each article is an entity). As output, we map every surface form (e.g., “spurs”) in the message to an entity (e.g., San Antonio Spurs) or to ∅ (i.e., a non-entity). This task is particularly challenging for microblogs due to their short, noisy and colloquial nature. Fortunately, we also observe two new opportunities, which are often missing in traditional data. First, microblogs usually embed rich meta-data, most notably spatiotemporal (i.e., spatial and temporal) signals. Speci</context>
<context position="6503" citStr="Bunescu and Pasca, 2006" startWordPosition="1035" endWordPosition="1038">st work to model spatiotemporal signals for entity linking. • We demonstrate the effectiveness of our framework through extensive quantitative experiments. In particular, we improve F1 by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility an</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. C Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the European Chapter of the ACL (EACL), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cataldi</author>
<author>L Di Caro</author>
<author>C Schifanella</author>
</authors>
<title>Emerging topic detection on twitter based on temporal and social terms evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Multimedia Data Mining (MDMKDD),</booktitle>
<pages>4--1</pages>
<marker>Cataldi, Di Caro, Schifanella, 2010</marker>
<rawString>M. Cataldi, L. Di Caro, and C. Schifanella. 2010. Emerging topic detection on twitter based on temporal and social terms evaluation. In Proceedings of the International Workshop on Multimedia Data Mining (MDMKDD), pages 4:1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="11241" citStr="Collins, 2002" startWordPosition="1818" endWordPosition="1819">ormation from disambiguation pages, redirect pages and anchor texts in Wikipedia3. To handle the case where a is not an entity, we add a special symbol 0 to every £(a). We have 6 million anchors in total. Given a tweet, the system generates a set of candidate anchors based on the lexicon. Specifically, each tweet is tokenized into individual words, keeping each punctuation as a separate token. To identify if any sequence of tokens matches an anchor in the lexicon, we use exact matching, but allow for case-insensitivity. Furthermore, we employ an NER system trained using structural perceptron (Collins, 2002) to filter the anchors of type person, loc or org, such that only the anchors recognized by the NER are retained. 3.2 Incorporating Spatiotemporal Signals Consider a tweet m with timestamp t and location l. Given anchor a in tweet m, our system links a to the candidate entity e* as follows: e* = arg maxeES(a) P(e|m, a, t, l) = arg maxeES(a) P(e, m, a, t, l) (1) We further adopt the following conditional independence assumption in our model. ASSUMPTION: Given an entity e, how e is expressed (m, a), and when or where e is published (t, l), are conditionally independent. In other words, we have P</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>708--716</pages>
<contexts>
<context position="6519" citStr="Cucerzan, 2007" startWordPosition="1039" endWordPosition="1040">mporal signals for entity linking. • We demonstrate the effectiveness of our framework through extensive quantitative experiments. In particular, we improve F1 by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coheren</context>
<context position="10581" citStr="Cucerzan, 2007" startWordPosition="1707" endWordPosition="1708">can link to. Following earlier work (Sil and Yates, 2013), our database consists of the intersection of Freebase and Wikipedia. We then select the entities belonging to the core types (Guo et al., 2013)2 based on the Free2Their core types include person, organization (org), location (loc), book, tvshow and movie. We deal with two additional types, namely, event and product. base type information. In total, there are 2.7 million entities in our database. A lexicon is a dictionary that maps a candidate anchor a (i.e., a surface form) to its possible entity set £(a). Similar to existing studies (Cucerzan, 2007; Guo et al., 2013), our lexicon comprises information from disambiguation pages, redirect pages and anchor texts in Wikipedia3. To handle the case where a is not an entity, we add a special symbol 0 to every £(a). We have 6 million anchors in total. Given a tweet, the system generates a set of candidate anchors based on the lexicon. Specifically, each tweet is tokenized into individual words, keeping each punctuation as a separate token. To identify if any sequence of tokens matches an anchor in the lexicon, we use exact matching, but allow for case-insensitivity. Furthermore, we employ an NE</context>
<context position="17009" citStr="Cucerzan, 2007" startWordPosition="2841" endWordPosition="2842"> ground-truth distribution, and we want P(e|m, a) to be as close to G(e|m, a) as possible. To this end, we optimize b1 and b2 by minimizing the cross entropy between G and P: minb1,b2 — Em,a G(e|m, a) log P(e|m, a). Alternative base system. We also consider LinkProbability (LP) as an additional base system. As pointed out earlier (Guo et al., 2013), mention detection is an important step for end-to-end entity linking, and its design is crucial to the ultimate performance. Hence, to detect candidate anchors, LP uses the same design of database and lexicon discussed in this paper and elsewhere (Cucerzan, 2007; Guo et al., 2013), which is believed to be effective. Given a potential anchor a in message m, P(e|m, a) is simply modeled as P(e|a), which can be estimated from Wikipedia anchor statistics. In fact, anchor statistics constitute one of the most useful features in more sophisticated systems (Shen et al., 2012; Guo et al., 2013). Given its robust mention detection mechanism and the utility of anchor statistics, the simple LP turns out surprisingly well. 5 Estimating Spatiotemporal Signals There are two critical challenges for successfully estimating the spatiotemporal signals in the form of P(</context>
<context position="32693" citStr="Cucerzan, 2007" startWordPosition="5574" endWordPosition="5575">seful? Q3: Does the graph-based smoothing help? Q4: What causes the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. org loc event movie 266 E2E LP IE IR IE IR base 57.0 – 58.4 – 48.3 – 48.5 – +T 64.9 *** 71.4 *** 52.4 * 59.7 *** +L 65.0 ** 76.1 *** 50.3 * 61.8 *** +T+L 68.6 *** 79.0 *** 49.0 53.3 *** +T×L 66.2 *** 74.1 *** 50.6 61.2 *** (a) F1 scores (b) Precision (c) Recall E2E +T +L +T+L Precision 90 80 70 60 85.5 86.9 82.4 85.6 E2E +T +L +T+L Recall 40 60 50 30 42.8 51.8 53.6 57.2 Figure 3: Effect of using spatiotemporal signals. (a) F1 sco</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708– 716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dalvi</author>
<author>R Kumar</author>
<author>B Pang</author>
</authors>
<title>Object matching in tweets with spatial models.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>43--52</pages>
<contexts>
<context position="8149" citStr="Dalvi et al., 2012" startWordPosition="1296" endWordPosition="1299">es, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on me</context>
<context position="22984" citStr="Dalvi et al., 2012" startWordPosition="3900" endWordPosition="3903">tuition can be captured by the following optimization: 2(1 − E) E 1 i,j∈I Wijkθi − θjk2 + E Ei∈I Diikθi − ˆθik2, (5) where c ∈ (0, 1) is a regularization parameter, W is an affinity matrix such that Wij measures the “closeness” of δi and δj, and D is a diagonal matrix such that Dii = Ej∈I Wij. In particular, we design the affinity matrix as follows: � (d0 + d(δi, δj))γ i =6 j Wij = (6) 0 i = j, where d0 &gt; 0 and γ &lt; 0 are parameters, and d(·) is a symmetric distance function for bins. Given that γ &lt; 0, Wij follows a polynomial decay when δi and δj become farther apart, as previously suggested (Dalvi et al., 2012). It can be shown that the optimization problem (5) is equivalent to finding θi such that ∀i ∈ I, θi=(1−E) Ej∈I Dii θj + A. (7) Interestingly, we can consider (7) as a generalization of previous observations on social media. It is proposed that the temporal model on social media needs to account for two factors: imitation and recency (Leskovec et al., 2009). For imitation, users often imitate one another, so that a past story can be picked up and propagated by other users by writing new articles about the same story. For recency, more recent stories are more likely to be imitated. We generaliz</context>
<context position="26472" citStr="Dalvi et al., 2012" startWordPosition="4512" endWordPosition="4515"> use 6The identities of verified accounts are validated by Twitter, and thus their tweets contain little spam. Moreover, we ignore retweets here as their spatiotemporal behavior might significantly differ from that of the original tweets. the location in the user profile and map it to coordinates based on a lookup table containing major cities in the US7. Such mapping exists for about 25% of the tweets. For the remaining tweets, their user profiles are either uninformative (e.g., “home”) or report a non-US location. In the end, 1.8 million tweets remain in our dataset. Note that some studies (Dalvi et al., 2012; Li et al., 2012b) enable the inference of missing locations based on user generated content or user network. We do not apply these methods, which are beyond our focus. Lastly, we collect the Wikipedia pageviews in the year 2012 in order to estimate P(e) in (3). Development set. We randomly sample 250 tweets as the development set and label their core entities (see Sect. 3.1). There are two human annotators. Each annotator labels half of the tweets, which are then counter-checked by the other to reach an agreement. To label a tweet, the annotators are given all available information to unders</context>
</contexts>
<marker>Dalvi, Kumar, Pang, 2012</marker>
<rawString>N. Dalvi, R. Kumar, and B. Pang. 2012. Object matching in tweets with spatial models. In Proceedings of International Conference on Web Search and Web Data Mining (WSDM), pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>B O’Connor</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>1277--1287</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>J. Eisenstein, B. O’Connor, N.A. Smith, and E.P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 1277–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fang</author>
<author>B-J Hsu</author>
<author>K C-C Chang</author>
</authors>
<title>Confidence-aware graph regularization with heterogeneous pairwise features.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>951--960</pages>
<contexts>
<context position="22198" citStr="Fang et al., 2012" startWordPosition="3740" endWordPosition="3743">n a bin, and thus θi cannot be well estimated. On the contrary, if the bins are too large, θi cannot reliably model θtl. In this paper, we tune the bin size using development data. Graph-based smoothing. Our algorithm contains two steps: first, we estimate ˆθi for each bin δi as in the above binning approach; next, we smooth the initial estimate to obtain the final estimate θi using graph-based regularization. 5The location grid is actually defined in longitude and latitude. Here we show the equivalent distance on Earth’s surface. As a common insight in graph-based learning (Zhu et al., 2003; Fang et al., 2012; Fang et al., 2014), if δi and δj are close to each other, θi and θj should be similar. Moreover, θi should not deviate too much from the initial estimate ˆθi. The intuition can be captured by the following optimization: 2(1 − E) E 1 i,j∈I Wijkθi − θjk2 + E Ei∈I Diikθi − ˆθik2, (5) where c ∈ (0, 1) is a regularization parameter, W is an affinity matrix such that Wij measures the “closeness” of δi and δj, and D is a diagonal matrix such that Dii = Ej∈I Wij. In particular, we design the affinity matrix as follows: � (d0 + d(δi, δj))γ i =6 j Wij = (6) 0 i = j, where d0 &gt; 0 and γ &lt; 0 are paramete</context>
</contexts>
<marker>Fang, Hsu, Chang, 2012</marker>
<rawString>Y. Fang, B.-J. Hsu, and K. C.-C. Chang. 2012. Confidence-aware graph regularization with heterogeneous pairwise features. In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR), pages 951–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fang</author>
<author>K C-C Chang</author>
<author>H W Lauw</author>
</authors>
<title>Roundtriprank: Graph-based proximity with importance and specificity.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference on Data Engineering (ICDE),</booktitle>
<pages>613--624</pages>
<contexts>
<context position="24032" citStr="Fang et al., 2013" startWordPosition="4089" endWordPosition="4092">can be picked up and propagated by other users by writing new articles about the same story. For recency, more recent stories are more likely to be imitated. We generalize their idea to both temporal and spatial signals, and extend it to entity linking. Specifically, (7) can be interpreted as a result of imitation: tweets mentioning entity e in δi are imitated from those in δj. In addition, a closer δj has a higher chance to be imitated, which captures recency. To solve (7), let Q = [θ1, ... , θn]T and Qˆ = ˆθn]T, treating θi and ˆθi as column vectors. Through the following iterative updates (Fang et al., 2013), Q(t) converges to Q as t → ∞, starting from min (θ1,...,θn) [ ˆθ1,..., 264 an arbitrary Q(0): Q(t+1) = (1 − c)(D−1W)Q(t) + c ˆQ. (8) The time complexity is O(tnk), where k is the number of neighboring bins on the graph. Such cost is reasonable, given that the update generally converges for t &lt; 50, and k is a constant if we only consider k nearest neighbors on the graph. Joint vs. separate modeling. Finally, while modeling time and location jointly is more expressive than modeling each signal separately, the joint approach also creates much more bins (and hence more multinomial parameters), m</context>
</contexts>
<marker>Fang, Chang, Lauw, 2013</marker>
<rawString>Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2013. Roundtriprank: Graph-based proximity with importance and specificity. In Proceedings of International Conference on Data Engineering (ICDE), pages 613– 624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fang</author>
<author>K C-C Chang</author>
<author>H W Lauw</author>
</authors>
<title>Graphbased semi-supervised learning: Realizing pointwise smoothness probabilistically.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="22218" citStr="Fang et al., 2014" startWordPosition="3744" endWordPosition="3747">i cannot be well estimated. On the contrary, if the bins are too large, θi cannot reliably model θtl. In this paper, we tune the bin size using development data. Graph-based smoothing. Our algorithm contains two steps: first, we estimate ˆθi for each bin δi as in the above binning approach; next, we smooth the initial estimate to obtain the final estimate θi using graph-based regularization. 5The location grid is actually defined in longitude and latitude. Here we show the equivalent distance on Earth’s surface. As a common insight in graph-based learning (Zhu et al., 2003; Fang et al., 2012; Fang et al., 2014), if δi and δj are close to each other, θi and θj should be similar. Moreover, θi should not deviate too much from the initial estimate ˆθi. The intuition can be captured by the following optimization: 2(1 − E) E 1 i,j∈I Wijkθi − θjk2 + E Ei∈I Diikθi − ˆθik2, (5) where c ∈ (0, 1) is a regularization parameter, W is an affinity matrix such that Wij measures the “closeness” of δi and δj, and D is a diagonal matrix such that Dii = Ej∈I Wij. In particular, we design the affinity matrix as follows: � (d0 + d(δi, δj))γ i =6 j Wij = (6) 0 i = j, where d0 &gt; 0 and γ &lt; 0 are parameters, and d(·) is a sy</context>
</contexts>
<marker>Fang, Chang, Lauw, 2014</marker>
<rawString>Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2014. Graphbased semi-supervised learning: Realizing pointwise smoothness probabilistically. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>U Scaiella</author>
</authors>
<title>TAGME: on-the-fly annotation of short text fragments (by Wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>1625--1628</pages>
<contexts>
<context position="7272" citStr="Ferragina and Scaiella, 2010" startWordPosition="1159" endWordPosition="1162">k (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the u</context>
<context position="32663" citStr="Ferragina and Scaiella, 2010" startWordPosition="5568" endWordPosition="5571">orm? Q2: Are spatiotemporal signals indeed useful? Q3: Does the graph-based smoothing help? Q4: What causes the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. org loc event movie 266 E2E LP IE IR IE IR base 57.0 – 58.4 – 48.3 – 48.5 – +T 64.9 *** 71.4 *** 52.4 * 59.7 *** +L 65.0 ** 76.1 *** 50.3 * 61.8 *** +T+L 68.6 *** 79.0 *** 49.0 53.3 *** +T×L 66.2 *** 74.1 *** 50.6 61.2 *** (a) F1 scores (b) Precision (c) Recall E2E +T +L +T+L Precision 90 80 70 60 85.5 86.9 82.4 85.6 E2E +T +L +T+L Recall 40 60 50 30 42.8 51.8 53.6 57.2 Figure 3: Effect of using spat</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>P. Ferragina and U. Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by Wikipedia entities). In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 1625–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guo</author>
<author>M-W Chang</author>
<author>E Kıcıman</author>
</authors>
<title>To link or not to link? A study on end-to-end tweet entity linking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),</booktitle>
<pages>1020--1030</pages>
<contexts>
<context position="7310" citStr="Guo et al., 2013" startWordPosition="1167" endWordPosition="1170">ncludes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore</context>
<context position="10169" citStr="Guo et al., 2013" startWordPosition="1636" endWordPosition="1639">king as well. 3 Spatiotemporal Entity Linking Our spatiotemporal framework for entity linking requires an input tweet m, as well as its associated timestamp t and location l. For each tweet m, the goal is to predict an output set {e1, e2, ...I of entities that are mentioned in m. 3.1 Background To build an entity linking system, we need both a database and a lexicon. A database is a set of entities that a tweet can link to. Following earlier work (Sil and Yates, 2013), our database consists of the intersection of Freebase and Wikipedia. We then select the entities belonging to the core types (Guo et al., 2013)2 based on the Free2Their core types include person, organization (org), location (loc), book, tvshow and movie. We deal with two additional types, namely, event and product. base type information. In total, there are 2.7 million entities in our database. A lexicon is a dictionary that maps a candidate anchor a (i.e., a surface form) to its possible entity set £(a). Similar to existing studies (Cucerzan, 2007; Guo et al., 2013), our lexicon comprises information from disambiguation pages, redirect pages and anchor texts in Wikipedia3. To handle the case where a is not an entity, we add a speci</context>
<context position="12480" citStr="Guo et al., 2013" startWordPosition="2039" endWordPosition="2042">a|e). Intuitively, the expression (m, a) of a given entity e is stable across most times and locations (t, l), which could be attributed to the imitation nature (Leskovec et al., 2009) of social media. That is, as stories of e propagate on the social media, users over different t and l often imitate each other (disregarding cross-lingual scenarios). While there could be a “burn-in” period for recent events of e, the distribution of m and a would eventually “stabilize” over most t and l. 3We use a snapshot of Wikipedia taken on 04/03/2013. 261 Existing Work Non-spatiotemporal (Sec 4) Example: (Guo et al., 2013) Popularity of Entity Example: estimate using Wikipedia pageviews Spatio Figure 2: Overall framework of spatiotemporal entity linking. Subsequently, we decompose the model below, to enable the reuse of existing non-spatiotemporal models for P(e|m, a), as we shall see later. P(e, m, a, t, l) = P(m, a|t, l, e)P(t,l, e) = P(m, a|e)P(t,l, e) = P(e|m, a)P(m, a)P(e|t, l)P(t, l)/P(e) (2) Note that in (2), we choose to rewrite P(t,l, e) as P(e|t,l)P(t, l) instead of P(t, l|e)P(e). The choice is due to computational issues. In the latter, we need to estimate one distribution for every e; in the former,</context>
<context position="14823" citStr="Guo et al., 2013" startWordPosition="2445" endWordPosition="2448">s not estimated based on the joint optimization technique for P(e|t, l) in Sect. 5. The reason is that our tweets used in the experiments only cover a onemonth period, which does not necessarily reflect the general popularity of the entities. 4 End-to-End Entity Linking The goal of this section is to describe our base system that does not consider spatiotemporal signals, i.e., to model P(e|m, a). Specifically, we adopt an end-to-end entity linking system (E2E), which is designed to jointly detect mentions and disambiguate entities. E2E is a supervised method largely based on a previous study (Guo et al., 2013). For efficiency, we only adopt the first order model. Therefore, the prediction function can be decomposed for each anchor a independently, e* = arg maxeEg(a) wTΦ(m, a, e). (4) where w is a linear model trained using structural SVM, and Φ is a feature function over message m, anchor a, and candidate output e. We use all the basic features and the cohesiveness score feature (Guo et al., 2013)4. Additional features are also included. First, for each mention and candidate entity pair, we add a feature to capture the number of highly correlated candidates carried by other mentions in the same twe</context>
<context position="16745" citStr="Guo et al., 2013" startWordPosition="2795" endWordPosition="2798">beled data. Given a labeled development set, let G(e|m, a) = 1 if and only if anchor a in tweet m is labeled to link to entity e, and let G(0|m, a) = 1 if and only if a in m is not labeled to link to any entity. Note that EeES(a) G(e|m, a) = 1. Thus, G represents the ground-truth distribution, and we want P(e|m, a) to be as close to G(e|m, a) as possible. To this end, we optimize b1 and b2 by minimizing the cross entropy between G and P: minb1,b2 — Em,a G(e|m, a) log P(e|m, a). Alternative base system. We also consider LinkProbability (LP) as an additional base system. As pointed out earlier (Guo et al., 2013), mention detection is an important step for end-to-end entity linking, and its design is crucial to the ultimate performance. Hence, to detect candidate anchors, LP uses the same design of database and lexicon discussed in this paper and elsewhere (Cucerzan, 2007; Guo et al., 2013), which is believed to be effective. Given a potential anchor a in message m, P(e|m, a) is simply modeled as P(e|a), which can be estimated from Wikipedia anchor statistics. In fact, anchor statistics constitute one of the most useful features in more sophisticated systems (Shen et al., 2012; Guo et al., 2013). Give</context>
<context position="29495" citStr="Guo et al., 2013" startWordPosition="5034" endWordPosition="5037">count confident ones such that their inferred probability is at least 0.5. 6.2 Evaluation Methodology We adopt two different evaluation policies, which are driven by information extraction (IE) and retrieval (IR) needs, respectively. IE-driven evaluation. The IE-driven evaluation is similar to the standard evaluation for an end-to-end entity linking system, which evaluates the entities “extracted” by linking. We randomly sample 250 tweets as the test set, which are labeled in the same manner as the development set. For evaluation, we compute the F1 score over the test set, as defined earlier (Guo et al., 2013). According to the labels, 40% of the tweets in the test set contain at least one entity. A total of 179 entity instances are identified, or an average of 0.72 per tweet. These entities belong to different types, including person (24%), org (36%), loc (16%), event (9%), and others (15%). IR-driven evaluation. One key application of entity linking is to enable intelligence gathering for a query entity, where the first step is to “retrieve” the tweets mentioning the query entity. As listed in Table 1, we sample ten query entities of different types, where each entity is known to be influenced by</context>
<context position="32754" citStr="Guo et al., 2013" startWordPosition="5583" endWordPosition="5586">uses the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. org loc event movie 266 E2E LP IE IR IE IR base 57.0 – 58.4 – 48.3 – 48.5 – +T 64.9 *** 71.4 *** 52.4 * 59.7 *** +L 65.0 ** 76.1 *** 50.3 * 61.8 *** +T+L 68.6 *** 79.0 *** 49.0 53.3 *** +T×L 66.2 *** 74.1 *** 50.6 61.2 *** (a) F1 scores (b) Precision (c) Recall E2E +T +L +T+L Precision 90 80 70 60 85.5 86.9 82.4 85.6 E2E +T +L +T+L Recall 40 60 50 30 42.8 51.8 53.6 57.2 Figure 3: Effect of using spatiotemporal signals. (a) F1 scores. ***, **, *: Significantly different from the base system</context>
</contexts>
<marker>Guo, Chang, Kıcıman, 2013</marker>
<rawString>S. Guo, M.-W. Chang, and E. Kıcıman. 2013. To link or not to link? A study on end-to-end tweet entity linking. In Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL), pages 1020–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>L Sun</author>
<author>J Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>765--774</pages>
<contexts>
<context position="7000" citStr="Han et al., 2011" startWordPosition="1114" endWordPosition="1117">tive study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additiona</context>
</contexts>
<marker>Han, Sun, Zhao, 2011</marker>
<rawString>X. Han, L. Sun, and J. Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR), pages 765–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>M Amir Yosef</author>
<author>I Bordino</author>
<author>H F¨urstenau</author>
<author>M Pinkal</author>
<author>M Spaniol</author>
<author>B Taneva</author>
<author>S Thater</author>
<author>G Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>782--792</pages>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>J. Hoffart, M. Amir Yosef, I. Bordino, H. F¨urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="6686" citStr="Ji and Grishman, 2011" startWordPosition="1065" endWordPosition="1068"> by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al.,</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>H. Ji and R. Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
<author>H T Dang</author>
<author>K Griffitt</author>
<author>J Ellis</author>
</authors>
<title>knowledge base population track.</title>
<date>2010</date>
<journal>Overview of the TAC</journal>
<booktitle>In Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="6662" citStr="Ji et al., 2010" startWordPosition="1061" endWordPosition="1064">ar, we improve F1 by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scai</context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis. 2010. Overview of the TAC 2010 knowledge base population track. In Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulkarni</author>
<author>A Singh</author>
<author>G Ramakrishnan</author>
<author>S Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>457--466</pages>
<contexts>
<context position="7023" citStr="Kulkarni et al., 2009" startWordPosition="1118" endWordPosition="1121"> presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. 2009. Collective annotation of Wikipedia entities in web text. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Leskovec</author>
<author>L Backstrom</author>
<author>J Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>497--506</pages>
<contexts>
<context position="8103" citStr="Leskovec et al., 2009" startWordPosition="1289" endWordPosition="1292">pproaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existin</context>
<context position="12047" citStr="Leskovec et al., 2009" startWordPosition="1964" endWordPosition="1967">tamp t and location l. Given anchor a in tweet m, our system links a to the candidate entity e* as follows: e* = arg maxeES(a) P(e|m, a, t, l) = arg maxeES(a) P(e, m, a, t, l) (1) We further adopt the following conditional independence assumption in our model. ASSUMPTION: Given an entity e, how e is expressed (m, a), and when or where e is published (t, l), are conditionally independent. In other words, we have P(m, a|t, l, e) = P(m, a|e). Intuitively, the expression (m, a) of a given entity e is stable across most times and locations (t, l), which could be attributed to the imitation nature (Leskovec et al., 2009) of social media. That is, as stories of e propagate on the social media, users over different t and l often imitate each other (disregarding cross-lingual scenarios). While there could be a “burn-in” period for recent events of e, the distribution of m and a would eventually “stabilize” over most t and l. 3We use a snapshot of Wikipedia taken on 04/03/2013. 261 Existing Work Non-spatiotemporal (Sec 4) Example: (Guo et al., 2013) Popularity of Entity Example: estimate using Wikipedia pageviews Spatio Figure 2: Overall framework of spatiotemporal entity linking. Subsequently, we decompose the m</context>
<context position="23343" citStr="Leskovec et al., 2009" startWordPosition="3965" endWordPosition="3968">0 + d(δi, δj))γ i =6 j Wij = (6) 0 i = j, where d0 &gt; 0 and γ &lt; 0 are parameters, and d(·) is a symmetric distance function for bins. Given that γ &lt; 0, Wij follows a polynomial decay when δi and δj become farther apart, as previously suggested (Dalvi et al., 2012). It can be shown that the optimization problem (5) is equivalent to finding θi such that ∀i ∈ I, θi=(1−E) Ej∈I Dii θj + A. (7) Interestingly, we can consider (7) as a generalization of previous observations on social media. It is proposed that the temporal model on social media needs to account for two factors: imitation and recency (Leskovec et al., 2009). For imitation, users often imitate one another, so that a past story can be picked up and propagated by other users by writing new articles about the same story. For recency, more recent stories are more likely to be imitated. We generalize their idea to both temporal and spatial signals, and extend it to entity linking. Specifically, (7) can be interpreted as a result of imitation: tweets mentioning entity e in δi are imitated from those in δj. In addition, a closer δj has a higher chance to be imitated, which captures recency. To solve (7), let Q = [θ1, ... , θn]T and Qˆ = ˆθn]T, treating </context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>J. Leskovec, L. Backstrom, and J. Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 497– 506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>J Weng</author>
<author>Q He</author>
<author>Y Yao</author>
<author>A Datta</author>
<author>A Sun</author>
<author>B-S Lee</author>
</authors>
<title>TwiNER: named entity recognition in targeted twitter stream.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>721--730</pages>
<contexts>
<context position="7413" citStr="Li et al., 2012" startWordPosition="1186" endWordPosition="1189">round document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media </context>
<context position="26489" citStr="Li et al., 2012" startWordPosition="4516" endWordPosition="4519"> of verified accounts are validated by Twitter, and thus their tweets contain little spam. Moreover, we ignore retweets here as their spatiotemporal behavior might significantly differ from that of the original tweets. the location in the user profile and map it to coordinates based on a lookup table containing major cities in the US7. Such mapping exists for about 25% of the tweets. For the remaining tweets, their user profiles are either uninformative (e.g., “home”) or report a non-US location. In the end, 1.8 million tweets remain in our dataset. Note that some studies (Dalvi et al., 2012; Li et al., 2012b) enable the inference of missing locations based on user generated content or user network. We do not apply these methods, which are beyond our focus. Lastly, we collect the Wikipedia pageviews in the year 2012 in order to estimate P(e) in (3). Development set. We randomly sample 250 tweets as the development set and label their core entities (see Sect. 3.1). There are two human annotators. Each annotator labels half of the tweets, which are then counter-checked by the other to reach an agreement. To label a tweet, the annotators are given all available information to understand its content,</context>
</contexts>
<marker>Li, Weng, He, Yao, Datta, Sun, Lee, 2012</marker>
<rawString>C. Li, J. Weng, Q. He, Y Yao, A. Datta, A. Sun, and B.-S. Lee. 2012a. TwiNER: named entity recognition in targeted twitter stream. In Proceedings of International Conference on Research and Development in Information Retrieval (SIGIR), pages 721–730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Li</author>
<author>S Wang</author>
<author>H Deng</author>
<author>R Wang</author>
<author>K Chen-Chuan Chang</author>
</authors>
<title>Towards social user profiling: unified and discriminative influence model for inferring home locations.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>1023--1031</pages>
<contexts>
<context position="7413" citStr="Li et al., 2012" startWordPosition="1186" endWordPosition="1189">round document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media </context>
<context position="26489" citStr="Li et al., 2012" startWordPosition="4516" endWordPosition="4519"> of verified accounts are validated by Twitter, and thus their tweets contain little spam. Moreover, we ignore retweets here as their spatiotemporal behavior might significantly differ from that of the original tweets. the location in the user profile and map it to coordinates based on a lookup table containing major cities in the US7. Such mapping exists for about 25% of the tweets. For the remaining tweets, their user profiles are either uninformative (e.g., “home”) or report a non-US location. In the end, 1.8 million tweets remain in our dataset. Note that some studies (Dalvi et al., 2012; Li et al., 2012b) enable the inference of missing locations based on user generated content or user network. We do not apply these methods, which are beyond our focus. Lastly, we collect the Wikipedia pageviews in the year 2012 in order to estimate P(e) in (3). Development set. We randomly sample 250 tweets as the development set and label their core entities (see Sect. 3.1). There are two human annotators. Each annotator labels half of the tweets, which are then counter-checked by the other to reach an agreement. To label a tweet, the annotators are given all available information to understand its content,</context>
</contexts>
<marker>Li, Wang, Deng, Wang, Chang, 2012</marker>
<rawString>R. Li, S. Wang, H. Deng, R. Wang, and K. Chen-Chuan Chang. 2012b. Towards social user profiling: unified and discriminative influence model for inferring home locations. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 1023–1031.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Li</author>
<author>S Wang</author>
<author>K Chen-Chuan Chang</author>
</authors>
<title>Towards social data platform: Automatic topic-focused monitor for twitter stream.</title>
<date>2013</date>
<booktitle>Proceedings of the VLDB Endowment (PVLDB),</booktitle>
<pages>6--14</pages>
<contexts>
<context position="8997" citStr="Li et al., 2013" startWordPosition="1437" endWordPosition="1440">Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013) only identify coarse-grained topics (e.g., crime or sports), falling short of recognizing specific entities. Lastly, there is a line of research on record extraction from social media (Benson et al., 2011; Ritter et al., 2012). Although the problem is different from entity linking, they present an interesting insight into social media. They observe that the same record is often referenced by multiple messages, and exploit this redundancy to help with extraction. The redundant nature of social media can be potentially leveraged to improve entity linking as well. 3 Spatiotemporal Entity Linking</context>
</contexts>
<marker>Li, Wang, Chang, 2013</marker>
<rawString>R. Li, S. Wang, and K. Chen-Chuan Chang. 2013. Towards social data platform: Automatic topic-focused monitor for twitter stream. Proceedings of the VLDB Endowment (PVLDB), 6(14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>C Liu</author>
<author>H Su</author>
<author>C-X Zhai</author>
</authors>
<title>A probabilistic approach to spatiotemporal theme pattern mining on weblogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the International World Wide Web Conference (WWW),</booktitle>
<pages>533--542</pages>
<contexts>
<context position="8195" citStr="Mei et al., 2006" startWordPosition="1305" endWordPosition="1308">ortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selecti</context>
</contexts>
<marker>Mei, Liu, Su, Zhai, 2006</marker>
<rawString>Q. Mei, C. Liu, H. Su, and C.-X. Zhai. 2006. A probabilistic approach to spatiotemporal theme pattern mining on weblogs. In Proceedings of the International World Wide Web Conference (WWW), pages 533–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Meij</author>
<author>W Weerkamp</author>
<author>M de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>563--572</pages>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of International Conference on Web Search and Web Data Mining (WSDM), pages 563–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>509--518</pages>
<contexts>
<context position="6544" citStr="Milne and Witten, 2008" startWordPosition="1041" endWordPosition="1044">or entity linking. • We demonstrate the effectiveness of our framework through extensive quantitative experiments. In particular, we improve F1 by more than 10 points over the existing state of the art. • We point out that entity linking should be evaluated for both information extraction and retrieval needs. In the former, we evaluate the extraction of entities from tweets, while in the latter we evaluate the retrieval of tweets for a query entity. A qualitative study is also presented for the latter. 2 Related Work Earlier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have</context>
<context position="32338" citStr="Milne and Witten, 2008" startWordPosition="5511" endWordPosition="5514">mpute the F1 score on each split for each algorithm. Two-tail paired t-test is then applied to determine if the F1 scores of two algorithms over the 10 splits are significantly different. 6.3 Results and Discussion We present the empirical findings for the following research questions. Q1: How do our base systems perform? Q2: Are spatiotemporal signals indeed useful? Q3: Does the graph-based smoothing help? Q4: What causes the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. org loc event movie 266 E2E LP IE IR IE IR base 57.0 – 58.4 – 48.3 – 48.5 – +T</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>D. Milne and I. H. Witten. 2008. Learning to link with Wikipedia. In Proceedings of ACM Conference on Information and Knowledge Management (CIKM), pages 509–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparison to regularize likelihood methods.</title>
<date>2000</date>
<booktitle>In Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<contexts>
<context position="15960" citStr="Platt, 2000" startWordPosition="2645" endWordPosition="2646">r of highly correlated candidates carried by other mentions in the same tweet with respect to the current candidate. Second, we include a binary feature that will be active if the type of the mention (when recognized by our NER system) and the type of the candidate entity (according to the Freebase type information) agree with each other. Probability conversion. It is crucial to have a well calibrated probability distribution for the predictions. In order to convert the output of the structural SVM model, we adapt an existing approach 4Described in Table 4 and Sect. 4.3 of the reference. 262 (Platt, 2000) to our case. We define exp(b1 + b2WTΦ(m, a, e)) P(e|m, a)= Ee,ES(a) exp(b1+b2WTΦ(m, a, e&apos;)), where b1 and b2 are the calibration parameters that will be tuned using labeled data. Given a labeled development set, let G(e|m, a) = 1 if and only if anchor a in tweet m is labeled to link to entity e, and let G(0|m, a) = 1 if and only if a in m is not labeled to link to any entity. Note that EeES(a) G(e|m, a) = 1. Thus, G represents the ground-truth distribution, and we want P(e|m, a) to be as close to G(e|m, a) as possible. To this end, we optimize b1 and b2 by minimizing the cross entropy between</context>
</contexts>
<marker>Platt, 2000</marker>
<rawString>J. Platt. 2000. Probabilistic outputs for support vector machines and comparison to regularize likelihood methods. In Advances in Large Margin Classifiers, pages 61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1375--1384</pages>
<contexts>
<context position="32374" citStr="Ratinov et al., 2011" startWordPosition="5517" endWordPosition="5520">ach algorithm. Two-tail paired t-test is then applied to determine if the F1 scores of two algorithms over the 10 splits are significantly different. 6.3 Results and Discussion We present the empirical findings for the following research questions. Q1: How do our base systems perform? Q2: Are spatiotemporal signals indeed useful? Q3: Does the graph-based smoothing help? Q4: What causes the errors? How to recover them? Base system comparison (Q1). To show that our base systems, in particular E2E, already outperform other systems, we compare with Wikiminer (Milne and Witten, 2008) and Illinois (Ratinov et al., 2011) systems.8 As existing systems are more geared for the IE scenario, we report in Table 2 the IE-drive F1 on the test set. 8We use the authors’ implementations. AIDA (Hoffart et al., 2011) is not compared, as it mostly links to person, org and loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan (Cucerzan, 2007) were already compared in an earlier paper (Guo et al., 2013) which E2E is largely based on. To be fair, we discard non-core entities linked by Wikiminer or Illinois. org loc event movie 266 E2E LP IE IR IE IR base 57.0 – 58.4 – 48.3 – 48.5 – +T 64.9 *** 71.4 *** 52.4 * 59.7 *** +</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1375–1384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="7396" citStr="Ritter et al., 2011" startWordPosition="1182" endWordPosition="1185">tion based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>O Etzioni Mausam</author>
<author>S Clark</author>
</authors>
<title>Open domain event extraction from Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>1104--1112</pages>
<contexts>
<context position="9224" citStr="Ritter et al., 2012" startWordPosition="1473" endWordPosition="1477">While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013) only identify coarse-grained topics (e.g., crime or sports), falling short of recognizing specific entities. Lastly, there is a line of research on record extraction from social media (Benson et al., 2011; Ritter et al., 2012). Although the problem is different from entity linking, they present an interesting insight into social media. They observe that the same record is often referenced by multiple messages, and exploit this redundancy to help with extraction. The redundant nature of social media can be potentially leveraged to improve entity linking as well. 3 Spatiotemporal Entity Linking Our spatiotemporal framework for entity linking requires an input tweet m, as well as its associated timestamp t and location l. For each tweet m, the goal is to predict an output set {e1, e2, ...I of entities that are mention</context>
</contexts>
<marker>Ritter, Mausam, Clark, 2012</marker>
<rawString>A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open domain event extraction from Twitter. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 1104–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sadilek</author>
<author>H Kautz</author>
<author>V Silenzio</author>
</authors>
<title>Modeling spread of disease from social interactions.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="1262" citStr="Sadilek et al., 2012" startWordPosition="183" endWordPosition="186"> a key role, but they are not utilized in existing approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly supervised process. Using entity annotations1 on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach. 1 Introduction Microblogging services provide an immense platform for intelligence gathering, such as market research (Asur and Huberman, 2010), disaster monitoring (Sadilek et al., 2012) and political analysis (Tumasjan et al., 2010). Extracting entities from microblogs is an essential step for many such applications. Suppose that a marketing firm is interested in the sentiment about some product on Twitter. However, any sentiment analysis is potentially ∗Work done during an internship at Microsoft Research. †Also affiliated with Agency for Science, Technology and Research, 1 Fusionopolis Way, Singapore 138632. 1Can be downloaded at http://research.microsoft.com/enus/downloads/84ac9d88-c353-4059-97a4-87d129db0464/. misleading if we cannot correctly retrieve the tweets mention</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>A. Sadilek, H. Kautz, and V. Silenzio. 2012. Modeling spread of disease from social interactions. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sakaki</author>
<author>M Okazaki</author>
<author>Y Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the International World Wide Web Conference (WWW),</booktitle>
<pages>851--860</pages>
<contexts>
<context position="8275" citStr="Sakaki et al., 2010" startWordPosition="1319" endWordPosition="1322">recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earthquake shakes Twitter users: real-time event detection by social sensors. In Proceedings of the International World Wide Web Conference (WWW), pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Scott</author>
</authors>
<title>Multivariate density estimation: theory, practice, and visualization.</title>
<date>2009</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="20570" citStr="Scott, 2009" startWordPosition="3453" endWordPosition="3454"> objective function Ω({e}, θtl) sums over all messages at t,l. These messages themselves are unlabeled, but some supervision is provided by the base system indirectly. In other words, even though we do not know the ground truth entity assignments in the messages, we can leverage the predictions from the base system to update the entity assignments. 5.2 Handling Continuous Time and Location Given the continuous space of time t and location l, we propose two methods to estimate θtl. While it could be cast as an instance of the well-studied density estimation problem (Vapnik and Mukherjee, 1999; Scott, 2009), we resort to a simple binning method, which has also been applied to other spatiotemporal contexts (Wing and Baldridge, 2011; Xu et al., 2012). The binning approach can already demonstrate the significance of spatiotemporal signals in entity linking. Binning. Our first approach segments the continuous space into discrete bins. Time is divided into a set of equal intervals ΔT (e.g., every one hour), and location is divided into a set of equal squares ΔL (e.g., each 100×100 sqkm area5). Let Δ = ΔT×ΔL denote the set of bins over time and location. We further index the bins by I = {1, ... , n} w</context>
</contexts>
<marker>Scott, 2009</marker>
<rawString>D.W. Scott. 2009. Multivariate density estimation: theory, practice, and visualization. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shen</author>
<author>J Wang</author>
<author>P Luo</author>
<author>M Wang</author>
</authors>
<title>LINDEN: linking named entities with knowledge base via semantic knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the International World Wide Web Conference (WWW),</booktitle>
<pages>449--458</pages>
<contexts>
<context position="7065" citStr="Shen et al., 2012" startWordPosition="1126" endWordPosition="1129">lier research on entity linking (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008) has been focused on well-written documents such as news and encyclopedia articles. The TAC KBP track (Ji et al., 2010; Ji and Grishman, 2011) also includes an entity linking task with a slightly different setting—to link a given mention based on a background document. These efforts exploit the statistical power aggregated by a semantic knowledge bases, most notably Wikipedia. Various features in the target document are also leveraged (Han et al., 2011; Kulkarni et al., 2009; Hoffart et al., 2011; Shen et al., 2012) to assess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent st</context>
<context position="17320" citStr="Shen et al., 2012" startWordPosition="2892" endWordPosition="2895"> As pointed out earlier (Guo et al., 2013), mention detection is an important step for end-to-end entity linking, and its design is crucial to the ultimate performance. Hence, to detect candidate anchors, LP uses the same design of database and lexicon discussed in this paper and elsewhere (Cucerzan, 2007; Guo et al., 2013), which is believed to be effective. Given a potential anchor a in message m, P(e|m, a) is simply modeled as P(e|a), which can be estimated from Wikipedia anchor statistics. In fact, anchor statistics constitute one of the most useful features in more sophisticated systems (Shen et al., 2012; Guo et al., 2013). Given its robust mention detection mechanism and the utility of anchor statistics, the simple LP turns out surprisingly well. 5 Estimating Spatiotemporal Signals There are two critical challenges for successfully estimating the spatiotemporal signals in the form of P(e|t,l). First, it is impractical to collect sufficient labeled data to directly estimate it. Second, we need to properly handle the continuous space of the spatiotemporal signals. In the following, we detail the overall model for learning spatiotemporal signals in a weakly supervised fashion (Sect. 5.1), and t</context>
</contexts>
<marker>Shen, Wang, Luo, Wang, 2012</marker>
<rawString>W. Shen, J. Wang, P. Luo, and M. Wang. 2012. LINDEN: linking named entities with knowledge base via semantic knowledge. In Proceedings of the International World Wide Web Conference (WWW), pages 449–458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shen</author>
<author>J Wang</author>
<author>P Luo</author>
<author>M Wang</author>
</authors>
<title>Linking named entities in tweets with knowledge base via user interest modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>68--76</pages>
<contexts>
<context position="7688" citStr="Shen et al., 2013" startWordPosition="1226" endWordPosition="1229">ssess both local compatibility and global coherence. These techniques have also been adapted and tailored to short texts including tweets, for the problem of entity linking (Ferragina and Scaiella, 2010; Meij et al., 2012; Guo et al., 2013) as well as the related problem of named entity recognition (NER) (Ritter et al., 2011; Li et al., 2012a). However, given the short and noisy nature of microblogs, these approaches, which largely depend on textual features, often result in unsatisfactory performance. Fortunately, additional non-textual meta-data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., </context>
</contexts>
<marker>Shen, Wang, Luo, Wang, 2013</marker>
<rawString>W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Linking named entities in tweets with knowledge base via user interest modeling. In Proceedings of International Conference on Knowledge Discovery and Data Mining (KDD), pages 68–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sil</author>
<author>A Yates</author>
</authors>
<title>Re-ranking for joint namedentity recognition and linking.</title>
<date>2013</date>
<booktitle>In Proceedings ofACM Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="10024" citStr="Sil and Yates, 2013" startWordPosition="1612" endWordPosition="1615">ssages, and exploit this redundancy to help with extraction. The redundant nature of social media can be potentially leveraged to improve entity linking as well. 3 Spatiotemporal Entity Linking Our spatiotemporal framework for entity linking requires an input tweet m, as well as its associated timestamp t and location l. For each tweet m, the goal is to predict an output set {e1, e2, ...I of entities that are mentioned in m. 3.1 Background To build an entity linking system, we need both a database and a lexicon. A database is a set of entities that a tweet can link to. Following earlier work (Sil and Yates, 2013), our database consists of the intersection of Freebase and Wikipedia. We then select the entities belonging to the core types (Guo et al., 2013)2 based on the Free2Their core types include person, organization (org), location (loc), book, tvshow and movie. We deal with two additional types, namely, event and product. base type information. In total, there are 2.7 million entities in our database. A lexicon is a dictionary that maps a candidate anchor a (i.e., a surface form) to its possible entity set £(a). Similar to existing studies (Cucerzan, 2007; Guo et al., 2013), our lexicon comprises </context>
</contexts>
<marker>Sil, Yates, 2013</marker>
<rawString>A. Sil and A. Yates. 2013. Re-ranking for joint namedentity recognition and linking. In Proceedings ofACM Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting elections with Twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>178--185</pages>
<contexts>
<context position="1309" citStr="Tumasjan et al., 2010" startWordPosition="191" endWordPosition="194">ting approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly supervised process. Using entity annotations1 on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach. 1 Introduction Microblogging services provide an immense platform for intelligence gathering, such as market research (Asur and Huberman, 2010), disaster monitoring (Sadilek et al., 2012) and political analysis (Tumasjan et al., 2010). Extracting entities from microblogs is an essential step for many such applications. Suppose that a marketing firm is interested in the sentiment about some product on Twitter. However, any sentiment analysis is potentially ∗Work done during an internship at Microsoft Research. †Also affiliated with Agency for Science, Technology and Research, 1 Fusionopolis Way, Singapore 138632. 1Can be downloaded at http://research.microsoft.com/enus/downloads/84ac9d88-c353-4059-97a4-87d129db0464/. misleading if we cannot correctly retrieve the tweets mentioning the target product. To retrieve tweets for </context>
<context position="8854" citStr="Tumasjan et al., 2010" startWordPosition="1413" endWordPosition="1416">011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013) only identify coarse-grained topics (e.g., crime or sports), falling short of recognizing specific entities. Lastly, there is a line of research on record extraction from social media (Benson et al., 2011; Ritter et al., 2012). Although the problem is different from entity linking, they present an interesting insight into social media. They observe that the same record is often referenced by multiple messages, and exploit this redundancy to help with e</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T. O. Sprenger, P. G Sandner, and I. M Welpe. 2010. Predicting elections with Twitter: What 140 characters reveal about political sentiment. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM), pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
<author>S Mukherjee</author>
</authors>
<title>Support vector method for multivariate density estimation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>659--665</pages>
<contexts>
<context position="20556" citStr="Vapnik and Mukherjee, 1999" startWordPosition="3449" endWordPosition="3452">e time t and location l, the objective function Ω({e}, θtl) sums over all messages at t,l. These messages themselves are unlabeled, but some supervision is provided by the base system indirectly. In other words, even though we do not know the ground truth entity assignments in the messages, we can leverage the predictions from the base system to update the entity assignments. 5.2 Handling Continuous Time and Location Given the continuous space of time t and location l, we propose two methods to estimate θtl. While it could be cast as an instance of the well-studied density estimation problem (Vapnik and Mukherjee, 1999; Scott, 2009), we resort to a simple binning method, which has also been applied to other spatiotemporal contexts (Wing and Baldridge, 2011; Xu et al., 2012). The binning approach can already demonstrate the significance of spatiotemporal signals in entity linking. Binning. Our first approach segments the continuous space into discrete bins. Time is divided into a set of equal intervals ΔT (e.g., every one hour), and location is divided into a set of equal squares ΔL (e.g., each 100×100 sqkm area5). Let Δ = ΔT×ΔL denote the set of bins over time and location. We further index the bins by I = </context>
</contexts>
<marker>Vapnik, Mukherjee, 1999</marker>
<rawString>V. Vapnik and S. Mukherjee. 1999. Support vector method for multivariate density estimation. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS), pages 659–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B P Wing</author>
<author>J Baldridge</author>
</authors>
<title>Simple supervised document geolocation with geodesic grids.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>955--964</pages>
<contexts>
<context position="8397" citStr="Wing and Baldridge, 2011" startWordPosition="1337" endWordPosition="1340">on that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus is not scalable to the vast number of entities. On the other hand, automatic approaches (Li et al., 2013)</context>
<context position="20696" citStr="Wing and Baldridge, 2011" startWordPosition="3472" endWordPosition="3475">upervision is provided by the base system indirectly. In other words, even though we do not know the ground truth entity assignments in the messages, we can leverage the predictions from the base system to update the entity assignments. 5.2 Handling Continuous Time and Location Given the continuous space of time t and location l, we propose two methods to estimate θtl. While it could be cast as an instance of the well-studied density estimation problem (Vapnik and Mukherjee, 1999; Scott, 2009), we resort to a simple binning method, which has also been applied to other spatiotemporal contexts (Wing and Baldridge, 2011; Xu et al., 2012). The binning approach can already demonstrate the significance of spatiotemporal signals in entity linking. Binning. Our first approach segments the continuous space into discrete bins. Time is divided into a set of equal intervals ΔT (e.g., every one hour), and location is divided into a set of equal squares ΔL (e.g., each 100×100 sqkm area5). Let Δ = ΔT×ΔL denote the set of bins over time and location. We further index the bins by I = {1, ... , n} where n = |Δ|, and refer to each bin δi through its index i ∈ I. Correspondingly, we denote the multinomial parameter at bin δi</context>
</contexts>
<marker>Wing, Baldridge, 2011</marker>
<rawString>B.P. Wing and J. Baldridge. 2011. Simple supervised document geolocation with geodesic grids. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 955–964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-M Xu</author>
<author>A Bhargava</author>
<author>R Nowak</author>
<author>X Zhu</author>
</authors>
<title>Socioscope: Spatio-temporal signal recovery from social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>644--659</pages>
<contexts>
<context position="8293" citStr="Xu et al., 2012" startWordPosition="1323" endWordPosition="1326"> al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tumasjan et al., 2010) requires significant labor, and thus i</context>
<context position="20714" citStr="Xu et al., 2012" startWordPosition="3476" endWordPosition="3479">the base system indirectly. In other words, even though we do not know the ground truth entity assignments in the messages, we can leverage the predictions from the base system to update the entity assignments. 5.2 Handling Continuous Time and Location Given the continuous space of time t and location l, we propose two methods to estimate θtl. While it could be cast as an instance of the well-studied density estimation problem (Vapnik and Mukherjee, 1999; Scott, 2009), we resort to a simple binning method, which has also been applied to other spatiotemporal contexts (Wing and Baldridge, 2011; Xu et al., 2012). The binning approach can already demonstrate the significance of spatiotemporal signals in entity linking. Binning. Our first approach segments the continuous space into discrete bins. Time is divided into a set of equal intervals ΔT (e.g., every one hour), and location is divided into a set of equal squares ΔL (e.g., each 100×100 sqkm area5). Let Δ = ΔT×ΔL denote the set of bins over time and location. We further index the bins by I = {1, ... , n} where n = |Δ|, and refer to each bin δi through its index i ∈ I. Correspondingly, we denote the multinomial parameter at bin δi by θi, which can </context>
</contexts>
<marker>Xu, Bhargava, Nowak, Zhu, 2012</marker>
<rawString>J.-M. Xu, A. Bhargava, R. Nowak, and X. Zhu. 2012. Socioscope: Spatio-temporal signal recovery from social media. In Proceedings of the European Conference on Machine Learning (ECML), pages 644–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Yin</author>
<author>L Cao</author>
<author>J Han</author>
<author>C Zhai</author>
<author>T Huang</author>
</authors>
<title>Geographical topic discovery and comparison.</title>
<date>2011</date>
<booktitle>In Proceedings of the International World Wide Web Conference (WWW),</booktitle>
<pages>247--256</pages>
<contexts>
<context position="8236" citStr="Yin et al., 2011" startWordPosition="1313" endWordPosition="1316">data in microblogs can often help. A recent study (Shen et al., 2013) improves entity linking by utilizing user account information, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Inspired by the use of non-textual features, we explore the spatiotemporal signals associated with tweets. Although the spatiotemporal aspect of social media has been studied in many papers, including temporal cycle tracking (Leskovec et al., 2009), spatial object matching (Dalvi et al., 2012), min260 ing emerging topics (Mei et al., 2006; Cataldi et al., 2010; Yin et al., 2011), event monitoring (Sakaki et al., 2010; Xu et al., 2012), and identifying geographical linguistic variations (Eisenstein et al., 2010; Wing and Baldridge, 2011), none of them addresses the problem of entity linking. In this paper, we propose a novel spatiotemporal framework for entity linking, building upon some of the previously observed patterns in social media. While we believe that entity linking is the first step towards intelligence gathering, many existing studies filter or cluster tweets based on merely keywords. On the one hand, manual selection of keywords (Sakaki et al., 2010; Tuma</context>
</contexts>
<marker>Yin, Cao, Han, Zhai, Huang, 2011</marker>
<rawString>Z. Yin, L. Cao, J. Han, C. Zhai, and T. Huang. 2011. Geographical topic discovery and comparison. In Proceedings of the International World Wide Web Conference (WWW), pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>912--919</pages>
<contexts>
<context position="22179" citStr="Zhu et al., 2003" startWordPosition="3736" endWordPosition="3739">nadequate tweets in a bin, and thus θi cannot be well estimated. On the contrary, if the bins are too large, θi cannot reliably model θtl. In this paper, we tune the bin size using development data. Graph-based smoothing. Our algorithm contains two steps: first, we estimate ˆθi for each bin δi as in the above binning approach; next, we smooth the initial estimate to obtain the final estimate θi using graph-based regularization. 5The location grid is actually defined in longitude and latitude. Here we show the equivalent distance on Earth’s surface. As a common insight in graph-based learning (Zhu et al., 2003; Fang et al., 2012; Fang et al., 2014), if δi and δj are close to each other, θi and θj should be similar. Moreover, θi should not deviate too much from the initial estimate ˆθi. The intuition can be captured by the following optimization: 2(1 − E) E 1 i,j∈I Wijkθi − θjk2 + E Ei∈I Diikθi − ˆθik2, (5) where c ∈ (0, 1) is a regularization parameter, W is an affinity matrix such that Wij measures the “closeness” of δi and δj, and D is a diagonal matrix such that Dii = Ej∈I Wij. In particular, we design the affinity matrix as follows: � (d0 + d(δi, δj))γ i =6 j Wij = (6) 0 i = j, where d0 &gt; 0 and</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, J. Lafferty, et al. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In Proceedings of the International Conference on Machine Learning (ICML), pages 912– 919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>